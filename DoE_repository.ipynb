{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from psutil import virtual_memory\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "def detect_spark_memory_limit():\n",
    "    \"\"\"Spark does not automatically use all available memory on a machine. When working on large datasets, this may\n",
    "    cause Java heap space errors, even though there is plenty of RAM available. To fix this, we detect the total amount\n",
    "    of physical memory and allow Spark to use (almost) all of it.\"\"\"\n",
    "    mem_gib = virtual_memory().total >> 30\n",
    "    return int(mem_gib * 0.9)\n",
    "\n",
    "\n",
    "spark_mem_limit = detect_spark_memory_limit()\n",
    "spark_conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.driver.memory\", f\"{spark_mem_limit}g\")\n",
    "    .set(\"spark.executor.memory\", f\"{spark_mem_limit}g\")\n",
    "    .set(\"spark.driver.maxResultSize\", \"0\")\n",
    "    .set(\"spark.debug.maxToStringFields\", \"2000000000\")\n",
    "    .set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"500000\")\n",
    "    ###.set(\"spark.executor.heartbeatInterval\", \"3600s\")\n",
    "    .set(\n",
    "        \"spark.sql.execution.arrow.pyspark.enabled\", \"true\"\n",
    "    )  ## esto lo pongo por esto: https://stackoverflow.com/questions/69973790/pyspark-spark-sparkexception-job-aborted-due-to-stage-failure-task-0-in-stage\n",
    "    .set(\"spark.ui.showConsoleProgress\", \"false\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.config(conf=spark_conf)\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\",\"localhost\") ### Run locally \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1# Make a list of variant of interest (Sequence ontology terms) to subset data of interest. \n",
    "\n",
    "### Bear in mind that SO works with ontology structure as: SO:XXXXXX, but databases has the SO as: SO_XXXXXX\n",
    "\n",
    "var_filter_lof = [\n",
    "    ### High impact variants https://www.ensembl.org/info/genome/variation/prediction/predicted_data.html\n",
    "    \"SO_0001589\",## frameshit_variant\n",
    "    \"SO_0001587\",## stop_gained\n",
    "    \"SO_0001574\",## splice_acceptor_variant\n",
    "    \"SO_0001575\",## splice_donor_variant\n",
    "    \"SO_0002012\",## start_lost\n",
    "    \"SO_0001578\",## stop_lost\n",
    "    \"SO_0001893\",## transcript_ablation\n",
    "    # \"SO:0001889\", ## transcript_amplification ## the Only HIGH impact that increase protein.\n",
    "]\n",
    "\n",
    "gof=['SO_0002053']\n",
    "lof=['SO_0002054']\n",
    "## Building Sequence Ontology \n",
    "so_path=\"/Users/juanr/Desktop/Target_Engine/data_download/sequenceOntology_20221118.csv\"\n",
    "so_ontology=spark.read.csv(so_path, header=True)\n",
    "building=(so_ontology\n",
    ".select(F.col('Accession'), F.col('Parents'))\n",
    ".withColumn('Parentalind',\n",
    "    F.split(F.col('Parents'), \",\"))\n",
    ".withColumn('Parentalind', F.explode_outer('Parentalind'))\n",
    ".groupBy('Parentalind')\n",
    ".agg(F.collect_list(F.col('Accession')).alias('childrens'))\n",
    ".join(so_ontology, F.col('Parentalind')==so_ontology.Accession, 'right')\n",
    ")\n",
    "\n",
    "\n",
    "### Load evidence datasources downloaded in January 2023: \n",
    "\n",
    "otgenetics_evidence_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=ot_genetics_portal\"\n",
    "otgenetics=spark.read.parquet(otgenetics_evidence_path)\n",
    "gene_burden_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=gene_burden\"\n",
    "gene_burden=spark.read.parquet(gene_burden_path)\n",
    "eva_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=eva\"\n",
    "eva_germline=spark.read.parquet(eva_path)\n",
    "eva_somatic_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=eva_somatic\"\n",
    "eva_somatic=spark.read.parquet(eva_somatic_path)\n",
    "gel_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=genomics_england\"\n",
    "gel=spark.read.parquet(gel_path)\n",
    "g2p_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=gene2phenotype\"\n",
    "g2p=spark.read.parquet(g2p_path)\n",
    "uniprot_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=uniprot_literature\"\n",
    "uniprot=spark.read.parquet(uniprot_path)\n",
    "uniprotvar_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=uniprot_variants\"\n",
    "uniprotvar=spark.read.parquet(uniprotvar_path)\n",
    "orphanet_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=orphanet\"\n",
    "orphanet=spark.read.parquet(orphanet_path)\n",
    "clingen_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=clingen\"\n",
    "clingen=spark.read.parquet(clingen_path)\n",
    "cgc_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=cancer_gene_census\"\n",
    "cgc=spark.read.parquet(cgc_path)\n",
    "intogen_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=intogen\"\n",
    "intogen=spark.read.parquet(intogen_path)\n",
    "impc_path=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=impc\"\n",
    "impc=spark.read.parquet(impc_path)\n",
    "chembl_evidences=\"/Users/juanr/Desktop/Target_Engine/DownloadFebruary_Release23.02/evidence/sourceId=chembl/\"\n",
    "chembl=spark.read.parquet(chembl_evidences)\n",
    "\n",
    "\n",
    "## others\n",
    "target_path=\"/Users/juanr/Desktop/Target_Engine/downloadedEvidencesJanuary/targets/\"\n",
    "target=spark.read.parquet(target_path)\n",
    "disease_path=\"/Users/juanr/Desktop/Target_Engine/data_download/Parquet/diseases/\"\n",
    "diseases=spark.read.parquet(disease_path)\n",
    "dis_name=diseases.select('id','name')\n",
    "indication_path=\"/Users/juanr/Desktop/Target_Engine/downloadedEvidencesJanuary/indication/\"\n",
    "indication=spark.read.parquet(indication_path)\n",
    "drug_path=\"/Users/juanr/Desktop/Target_Engine/downloadedEvidencesJanuary/molecule/\"\n",
    "drug=spark.read.parquet(drug_path)\n",
    "mecact_path=\"/Users/juanr/Desktop/Target_Engine/downloadedEvidencesJanuary/mechanismOfAction/\"\n",
    "mecact=spark.read.parquet(mecact_path)\n",
    "\n",
    "#### GENE BURDEN\n",
    "\n",
    "### We manually annotated those studies using LoF or PTV variants\n",
    "\n",
    "burden_lof_path=\"/Users/juanr/Desktop/Target_Engine/Conteo_estudios_geneBurden_20230117.csv\"\n",
    "burden_lof=spark.read.csv(burden_lof_path, header=True)\n",
    "burden_lof=burden_lof.withColumnRenamed('statisticalMethodOverview','stMethod')\n",
    "\n",
    "#### Para gene burden la funcion no tiene que hacer un filtrado de variantes\n",
    "\n",
    "### EVA/ClinVar \n",
    "\n",
    "##- Manually annotate which are the clinicalSignificances meaningfull: pathogenic, risk factor, protective\n",
    "\n",
    "clinSign_germline_path=\"/Users/juanr/Desktop/Target_Engine/eva_clinSig_20230117.csv\"\n",
    "clinSign_somatic_path=\"/Users/juanr/Desktop/Target_Engine/eva_somatic_clinSig_20230117.csv\"\n",
    "\n",
    "clinSign_germline=spark.read.csv(clinSign_germline_path, header=True)\n",
    "clinSign_germline=clinSign_germline.withColumnRenamed('clinicalSignificances','significances')\n",
    "clinSign_somatic=spark.read.csv(clinSign_somatic_path, header=True)\n",
    "clinSign_somatic=clinSign_somatic.withColumnRenamed('clinicalSignificances','significances')\n",
    "\n",
    "##-  Transform array of clinicalSignificances into Strings to check them.\n",
    "\n",
    "eva_somatic_toAsses=(eva_somatic\n",
    ".withColumn('clinicalSignificances',F.concat_ws(\",\",F.col(\"clinicalSignificances\"))))\n",
    "\n",
    "eva_germline_toAsses=(eva_germline\n",
    ".withColumn('clinicalSignificances',F.concat_ws(\",\",F.col(\"clinicalSignificances\"))))\n",
    "\n",
    "\n",
    "## annotate TSG/oncogene/bivalent using 'hallmarks.attributes'\n",
    "\n",
    "oncotsg_list = ['TSG','oncogene','Oncogene','oncogene','oncogene,TSG','TSG,oncogene','fusion,oncogene','oncogene,fusion']\n",
    "\n",
    "oncolabel=(target\n",
    ".select('id','approvedSymbol',F.explode_outer(F.col('hallmarks.attributes')))\n",
    ".select('id','approvedSymbol','col.description')\n",
    ".filter(F.col('description').isin(oncotsg_list))\n",
    ".groupBy('id','approvedSymbol')\n",
    ".agg(F.collect_set('description').alias('description'))\n",
    ".withColumn('description_splited',\n",
    "    F.concat_ws(\",\", F.col('description')))\n",
    ".withColumn('TSorOncogene', \n",
    "    F.when(\n",
    "        (F.col('description_splited').rlike('ncogene') &\n",
    "        F.col('description_splited').rlike('TSG'))\n",
    "        , F.lit('bivalent'))\n",
    "    .when(\n",
    "        F.col('description_splited').rlike('ncogene(\\s|$)')\n",
    "        , F.lit('oncogene'))\n",
    "    .when(\n",
    "        F.col('description_splited').rlike('TSG(\\s|$)')\n",
    "        , F.lit('TSG'))       \n",
    "    .otherwise(F.lit('noEvaluable')))\n",
    ".withColumnRenamed('id','target_id')\n",
    "\n",
    ")\n",
    "\n",
    "#### rlike('('+Keywords+')(\\s|$)'\n",
    "\n",
    "\n",
    "\n",
    "### Hacer el join del actionType con el chembl para sacar los mecanismos de accion. \n",
    "inhibitors = [\n",
    "'RNAI INHIBITOR',\n",
    "'NEGATIVE MODULATOR',\n",
    "'NEGATIVE ALLOSTERIC MODULATOR',\n",
    "'ANTAGONIST',\n",
    "'ANTISENSE INHIBITOR',\n",
    "'BLOCKER',\n",
    "'INHIBITOR',\n",
    "'DEGRADER',\n",
    "'INVERSE AGONIST',\n",
    "'ALLOSTERIC ANTAGONIST']\n",
    "\n",
    "activators=[\n",
    "'PARTIAL AGONIST',\n",
    "'ACTIVATOR',\n",
    "'POSITIVE ALLOSTERIC MODULATOR',\n",
    "'POSITIVE MODULATOR',\n",
    "'AGONIST',\n",
    "'SEQUESTERING AGENT']\n",
    "\n",
    "columnas= ['activator','inhibitor']\n",
    "both=activators+inhibitors\n",
    "\n",
    "actiontype2=(mecact\n",
    ".select('chemblIds','actionType','mechanismOfAction','targets')\n",
    ".select(F.explode_outer('chemblIds').alias('drugId2'),'actionType','mechanismOfAction','targets')\n",
    ".select(F.explode_outer('targets').alias('targetId2'), 'drugId2','actionType','mechanismOfAction')\n",
    ".dropDuplicates())\n",
    "\n",
    "\n",
    "chembl1=chembl.select('targetId','drugId','diseaseId','clinicalPhase','diseaseFromSourceId')\n",
    "chembl2=(chembl1\n",
    ".join(actiontype2,\n",
    "    (actiontype2.drugId2==F.col('drugId')) & \n",
    "    (actiontype2.targetId2==F.col('targetId')),\n",
    " 'left')\n",
    ".drop('targetId2','drugId2')\n",
    "###.dropDuplicates()\n",
    ".withColumn('twoCategories_new',\n",
    "    F.when(F.col('actionType').isin(inhibitors), F.lit('inhibitor'))\n",
    "    .when(F.col('actionType').isin(activators), F.lit('activator'))\n",
    "    .otherwise(F.lit('noEvaluable'))))\n",
    "\n",
    "chembl3=(chembl2\n",
    ".filter(F.col('twoCategories_new')!='noEvaluable')\n",
    ".groupBy('targetId','diseaseId')\n",
    ".pivot('twoCategories_new')\n",
    ".agg(F.count('targetId')))\n",
    "\n",
    "chembl4=(chembl3\n",
    ".select(\n",
    "    'targetId',\n",
    "    'diseaseId',\n",
    "    ##'clinicalPhase',\n",
    "    *(F.col(c).cast(\"int\").alias(c) for c in columnas))\n",
    ".withColumn('coherency',\n",
    "    F.when(\n",
    "        (\n",
    "        (F.col('activator').isNotNull()) &\n",
    "        (F.col('inhibitor').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('activator'))-(F.col('inhibitor'))!=(F.col('activator'))\n",
    "                ,F.lit('dispar'))\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Join all datasets\n",
    "\n",
    "dfs=[otgenetics,gene_burden,eva_germline,eva_somatic,g2p,orphanet,cgc,intogen,impc,chembl]\n",
    "\n",
    "all = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    all = all.unionByName(df, allowMissingColumns=True)\n",
    "all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 20230203 ### \n",
    "prueba_assessment=(all\n",
    ".withColumn(\"beta\",F.col(\"beta\").cast('float')) ## from ot genetics & gene burden\n",
    ".withColumn(\"OddsRatio\",F.col(\"OddsRatio\").cast('float')) ## from ot genetics & gene burden\n",
    ".withColumn('clinicalSignificances',F.concat_ws(\",\",F.col(\"clinicalSignificances\"))) ### from eva\n",
    ".withColumn('exploded',F.explode_outer(F.col('mutatedSamples'))) ### para cgc e intogen\n",
    ".withColumn('variantConsequence',F.col('exploded.functionalConsequenceId'))### para cgc e intogen\n",
    "### .withColumn('numberSamplesSameMutationType',F.col('exploded.numberSamplesWithMutationType'))### para cgc e intogen\n",
    ".withColumn(\"mutatedSamplesVariantInfo\", F.coalesce(F.col('mutatedSamples.functionalConsequenceId'), F.array()))### para cgc e intogen\n",
    "\n",
    ".join(oncolabel, \n",
    "oncolabel.target_id==F.col('targetId'), 'left')### para cgc \n",
    ".join(burden_lof, \n",
    "burden_lof.stMethod == F.col('statisticalMethodOverview'), 'left') ### para gene burden\n",
    ".join(actiontype2, ## para chembl\n",
    "    (actiontype2.drugId2==F.col('drugId')) & \n",
    "    (actiontype2.targetId2==F.col('targetId')),\n",
    " 'left')\n",
    "##.drop('targetId2','drugId2')\n",
    "###.dropDuplicates()\n",
    "\n",
    ".withColumn('Assessment',\n",
    "\n",
    "#### Ot_genetics Portal ### updated to include the coloc+gwas analysis \n",
    "        F.when(F.col('datasourceId')=='ot_genetics_portal',\n",
    "\n",
    "                F.when(  ### label 14 evidences that are contradictory\n",
    "                        (                       \n",
    "                        (F.col('variantFunctionalConsequenceFromQtlId')=='SO_0002315') &\n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof)) \n",
    "                        )\n",
    "                        ,\n",
    "                        F.lit('dispar'))\n",
    "                ### evidences with gwas+coloc increased expression without +var_lof\n",
    "                .when(  \n",
    "                        (\n",
    "                        (F.col('beta').isNull()) &\n",
    "                        (F.col('variantFunctionalConsequenceFromQtlId')=='SO_0002315') &\n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof) == False)\n",
    "                        ),\n",
    "                                F.when((F.col('OddsRatio') >1), F.lit('GoF_risk'))\n",
    "                                 .when((F.col('OddsRatio') <1) , F.lit('GoF_protect'))                 \n",
    "                        )\n",
    "                .when(\n",
    "                        (\n",
    "                        (F.col('oddsRatio').isNull()) &\n",
    "                        (F.col('variantFunctionalConsequenceFromQtlId')=='SO_0002315') &\n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof) == False)\n",
    "                        ),\n",
    "                                F.when((F.col('beta') <0), F.lit('GoF_protect'))\n",
    "                                .when((F.col('beta') >0), F.lit('GoF_risk'))\n",
    "                        ) \n",
    "\n",
    "\n",
    "                ### evidences with coherent Gwas-coloc + var_lof\n",
    "                .when(  \n",
    "                        (\n",
    "                        (F.col('beta').isNull()) &\n",
    "                        (F.col('variantFunctionalConsequenceFromQtlId')=='SO_0002316') &\n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof))\n",
    "                        ),\n",
    "                                F.when((F.col('OddsRatio') >1), F.lit('LoF_risk'))\n",
    "                                 .when((F.col('OddsRatio') <1) , F.lit('LoF_protect'))                 \n",
    "                        )\n",
    "                .when(\n",
    "                        (\n",
    "                        (F.col('oddsRatio').isNull()) &\n",
    "                        (F.col('variantFunctionalConsequenceFromQtlId')=='SO_0002316') &\n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof))\n",
    "                        ),\n",
    "                                F.when((F.col('beta') <0), F.lit('LoF_protect'))\n",
    "                                .when((F.col('beta') >0), F.lit('LoF_risk'))\n",
    "                        ) \n",
    "                ### evidences with colo+Gwas data but not variants\n",
    "                .when(  \n",
    "                        (\n",
    "                        (F.col('beta').isNull()) &\n",
    "                        (F.col('variantFunctionalConsequenceFromQtlId')=='SO_0002316') &\n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof) == False)\n",
    "                        ),\n",
    "                                F.when((F.col('OddsRatio') >1), F.lit('LoF_risk'))\n",
    "                                 .when((F.col('OddsRatio') <1) , F.lit('LoF_protect'))                 \n",
    "                        )\n",
    "                .when(\n",
    "                        (\n",
    "                        (F.col('oddsRatio').isNull()) &\n",
    "                        (F.col('variantFunctionalConsequenceFromQtlId')=='SO_0002316') &\n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof) == False)\n",
    "                        ),\n",
    "                                F.when((F.col('beta') <0), F.lit('LoF_protect'))\n",
    "                                .when((F.col('beta') >0), F.lit('LoF_risk'))\n",
    "                        ) \n",
    "                ### evidences with coherent non/inconclusive gwas+coloc + var_lof\n",
    "                .when(  \n",
    "                        (\n",
    "                        (F.col('beta').isNull()) &\n",
    "                                (\n",
    "                                (F.col('variantFunctionalConsequenceFromQtlId')=='SO_0002314') |\n",
    "                                (F.col('variantFunctionalConsequenceFromQtlId').isNull())\n",
    "                                ) &\n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof))\n",
    "                        ),\n",
    "                                F.when((F.col('OddsRatio') >1), F.lit('LoF_risk'))\n",
    "                                 .when((F.col('OddsRatio') <1) , F.lit('LoF_protect'))                 \n",
    "                        )\n",
    "                .when(  \n",
    "                        (\n",
    "                        (F.col('oddsRatio').isNull()) &\n",
    "                                (\n",
    "                                (F.col('variantFunctionalConsequenceFromQtlId')=='SO_0002314') |\n",
    "                                (F.col('variantFunctionalConsequenceFromQtlId').isNull())\n",
    "                                ) &\n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof))\n",
    "                        ),\n",
    "                                F.when((F.col('beta') <0), F.lit('LoF_protect'))\n",
    "                                .when((F.col('beta') >0), F.lit('LoF_risk'))\n",
    "                        ) \n",
    "                \n",
    "                .otherwise(F.lit('noEvaluable')))  ### son tambien no data las que tiene riesgo pero no tienen LoF\n",
    "\n",
    "#### Gene burden                            \n",
    "        .when(F.col('datasourceId')=='gene_burden',\n",
    "        ### .filter(F.col('variantType').isin(var_filter))\n",
    "                F.when(\n",
    "                        (\n",
    "                        (F.col('whatToDo')=='get')&\n",
    "                        (F.col('beta').isNull()) &  \n",
    "                        (F.col('OddsRatio') > 1)\n",
    "                        ), \n",
    "                        F.lit('LoF_risk')\n",
    "                        )\n",
    "                .when(\n",
    "                        (\n",
    "                        (F.col('whatToDo')=='get')&\n",
    "                        (F.col('beta').isNull()) & \n",
    "                        (F.col('OddsRatio') <1) \n",
    "                        ),\n",
    "                        F.lit('LoF_protect')\n",
    "                )\n",
    "                .when(\n",
    "                        (\n",
    "                        (F.col('whatToDo')=='get')&\n",
    "                        (F.col('OddsRatio').isNull()) & \n",
    "                        (F.col('beta') >0)\n",
    "                        ), \n",
    "                        F.lit('LoF_risk')\n",
    "                )\n",
    "                .when(\n",
    "                        (\n",
    "                        (F.col('whatToDo')=='get')&\n",
    "                        (F.col('OddsRatio').isNull()) & \n",
    "                        (F.col('beta') <0) \n",
    "                        ),\n",
    "                        F.lit('LoF_protect')\n",
    "                )\n",
    "                .otherwise(F.lit('noEvaluable')) ### son tambien no data las que tiene riesgo pero no se ensayan LoF o PT \n",
    "                        )\n",
    "#### Eva_germline                        \n",
    "        .when(F.col('datasourceId')=='eva',              \n",
    "        #### .filter(F.col('variantFunctionalConsequenceId').isin(var_filter_lof))\n",
    "                F.when(\n",
    "                        (\n",
    "                        ## (F.col('clinicalSignificances')!='likely pathogenic') &  \n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof)) &\n",
    "                        F.col('clinicalSignificances').rlike('(pathogenic)$')\n",
    "                        ),\n",
    "                        F.lit('LoF_risk'))\n",
    "                .when(\n",
    "                        (\n",
    "                        F.col('clinicalSignificances').contains('protective')&  \n",
    "                        F.col('variantFunctionalConsequenceId').isin(var_filter_lof)),\n",
    "                        F.lit('LoF_protect'))\n",
    "                .otherwise(F.lit('noEvaluable')) ### Son todas aquellas que tenen info pero no son patogenicas/protective  + LoF\n",
    "                        )\n",
    "        #### Eva_somatic  \n",
    "        .when(F.col('datasourceId')=='eva_somatic',              \n",
    "        #### .filter(F.col('variantFunctionalConsequenceId').isin(var_filter_lof))\n",
    "                F.when(\n",
    "                        (\n",
    "                        ##(F.col('clinicalSignificances')!='likely pathogenic') &  \n",
    "                        (F.col('variantFunctionalConsequenceId').isin(var_filter_lof)) &\n",
    "                        F.col('clinicalSignificances').rlike('(pathogenic)$')\n",
    "                        ),\n",
    "                        F.lit('LoF_risk'))\n",
    "                .when(\n",
    "                        (\n",
    "                        F.col('clinicalSignificances').contains('protective')&  \n",
    "                        F.col('variantFunctionalConsequenceId').isin(var_filter_lof)),\n",
    "                        F.lit('LoF_protect'))\n",
    "                .otherwise(F.lit('noEvaluable')) ### Son todas aquellas que tenen info pero no son patogenicas/protective  + LoF\n",
    "                        )\n",
    "#### G2P                     \n",
    "        .when(F.col('datasourceId')=='gene2phenotype',  \n",
    "                F.when(\n",
    "                        F.col('variantFunctionalConsequenceId')=='SO_0002317', F.lit('LoF_risk')\n",
    "                        ) ### absent gene product\n",
    "                .when(\n",
    "                        F.col('variantFunctionalConsequenceId')=='SO_0002315', F.lit('GoF_risk')\n",
    "                        ) ### increased gene product level\n",
    "                .otherwise(F.lit('noEvaluable')))\n",
    "#### Orphanet \n",
    "        .when(F.col('datasourceId')=='orphanet',  \n",
    "                F.when(\n",
    "                        F.col('variantFunctionalConsequenceId')=='SO_0002054', F.lit('LoF_risk')\n",
    "                        ) ### Loss of Function Variant\n",
    "                .when(\n",
    "                        F.col('variantFunctionalConsequenceId')=='SO_0002053', F.lit('GoF_risk')\n",
    "                        ) ### Gain_of_Function Variant\n",
    "                .otherwise(F.lit('noEvaluable')))\n",
    " #### CGC               \n",
    "        .when(F.col('datasourceId')=='cancer_gene_census',          \n",
    "                F.when(\n",
    "                        F.col('TSorOncogene')=='oncogene', F.lit('GoF_risk')\n",
    "                )\n",
    "                .when(\n",
    "                        F.col('TSorOncogene')=='TSG', F.lit('LoF_risk')\n",
    "                        )\n",
    "                .when(\n",
    "                        F.col('TSorOncogene')=='bivalent', F.lit('bivalent_risk'))\n",
    "\n",
    "                .otherwise(\n",
    "                        F.when(\n",
    "                                F.arrays_overlap(\n",
    "                                F.col('mutatedSamples.functionalConsequenceId'),\n",
    "                                F.array([F.lit(i) for i in (var_filter_lof)])\n",
    "                                ), F.lit('LoF_risk')\n",
    "                                )\n",
    "                        .otherwise(F.lit('noEvaluable'))\n",
    "                        )\n",
    "                ) #### Aqui asumimos que todo lo que esta incluido da riesgo, pero solo podemos dar LoF porque ya no tienen dato de TSG/oncogen\n",
    "#### intogen\n",
    "        .when(F.col('datasourceId')=='intogen',     \n",
    "                F.when(\n",
    "                        F.arrays_overlap(\n",
    "                                F.col('mutatedSamples.functionalConsequenceId'),\n",
    "                                F.array([F.lit(i) for i in (gof)])\n",
    "                                ), F.lit('GoF_risk'))\n",
    "                .when(\n",
    "                        F.arrays_overlap(\n",
    "                                F.col('mutatedSamples.functionalConsequenceId'),\n",
    "                                F.array([F.lit(i) for i in (lof)])\n",
    "                                ), F.lit('LoF_risk'))\n",
    "\n",
    "                .otherwise(F.lit('noEvaluable'))\n",
    "        )\n",
    "#### impc        \n",
    "        .when(F.col('datasourceId')=='impc', \n",
    "                F.when(\n",
    "                        F.col('diseaseId').isNotNull(), F.lit('KO_risk')\n",
    "                        )\n",
    "                .otherwise(F.lit('noEvaluable')))\n",
    "### chembl        \n",
    "        .when(F.col('datasourceId')=='chembl',\n",
    "                F.when(\n",
    "                        F.col('actionType').isin(inhibitors), F.lit('LoF_protect')\n",
    "                        )\n",
    "                .when(\n",
    "                        F.col('actionType').isin(activators), F.lit('GoF_protect')\n",
    "                )\n",
    "                .otherwise(F.lit('noEvaluable'))\n",
    "                )\n",
    ")\n",
    "\n",
    "### Homogenizar para contar todos los datos juntos:\n",
    ".withColumn('homogenized',\n",
    "    F.when(F.col('Assessment')=='KO_risk', F.lit('LoF_risk'))\n",
    "    .otherwise(F.col('Assessment')))\n",
    ".withColumn('tendency',\n",
    "    F.when(F.col('homogenized').contains('risk'), F.lit('Risk'))\n",
    "    .when(F.col('homogenized').contains('protect'), F.lit('Protect'))\n",
    "    .otherwise(F.lit('noEvaluable')))\n",
    ".withColumn('variation',\n",
    "    F.when(F.col('homogenized').contains('LoF'), F.lit('LoF'))\n",
    "    .when(F.col('homogenized').contains('GoF'), F.lit('GoF'))\n",
    "    .otherwise(F.lit('noEvaluable')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### checking contradictions intra datasources 03.02.2023 #### \n",
    "\n",
    "terms=['noEvaluable','bivalent_risk','null','dispar']\n",
    "\n",
    "\n",
    "coherency_toAssess=(prueba_assessment\n",
    ".filter((F.col('Assessment')).isin(terms)==False)\n",
    ".groupBy('targetId','diseaseId','datasourceId')\n",
    ".pivot('homogenized')\n",
    ".agg(F.count('targetId')))\n",
    "\n",
    "columns=[\n",
    " 'GoF_risk',\n",
    " 'LoF_protect',\n",
    " 'LoF_risk',\n",
    " 'GoF_protect']\n",
    "\n",
    "coherency_assessed=(coherency_toAssess\n",
    ".select(\n",
    "    F.col('targetId').alias('targetId3'),\n",
    "    F.col('datasourceId').alias('datasourceId3'),\n",
    "    F.col('diseaseId').alias('diseaseId3'),\n",
    "    *(F.col(c).cast(\"int\").alias(c) for c in columns))\n",
    "\n",
    ".withColumn('coherency',\n",
    "    F.when(\n",
    "        (\n",
    "        (F.col('GoF_risk').isNotNull()) &\n",
    "        (F.col('LoF_risk').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('GoF_risk'))-(F.col('LoF_risk'))!=(F.col('GoF_risk'))\n",
    "                ,F.lit('dispar'))\n",
    "        )\n",
    "    .when(\n",
    "        (\n",
    "        (F.col('LoF_protect').isNotNull()) &\n",
    "        (F.col('LoF_risk').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('LoF_protect'))-(F.col('LoF_risk'))!=(F.col('LoF_protect'))\n",
    "                ,F.lit('dispar'))\n",
    "        )   \n",
    "    .when(\n",
    "        (\n",
    "        (F.col('GoF_protect').isNotNull()) &\n",
    "        (F.col('GoF_risk').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('GoF_protect'))-(F.col('GoF_risk'))!=(F.col('GoF_protect'))\n",
    "                ,F.lit('dispar'))\n",
    "        )   \n",
    "    .when(\n",
    "        (\n",
    "        (F.col('GoF_protect').isNotNull()) &\n",
    "        (F.col('LoF_protect').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('GoF_protect'))-(F.col('LoF_protect'))!=(F.col('GoF_protect'))\n",
    "                ,F.lit('dispar'))\n",
    "        )      \n",
    "        \n",
    "        ))\n",
    "\n",
    "\n",
    "#### Build dataset for checking intradatasource disparities \n",
    "\n",
    "columnstoassess=prueba_assessment.drop('targetId','diseaseId','datasourceId').columns\n",
    "terms=['noEvaluable','bivalent_risk','null','dispar']\n",
    "### Make the incoherencies dataset: join disease name, approved symbol and collect how many evidences are supporting every column\n",
    "\n",
    "intradatasource_disparities=(coherency_assessed\n",
    ".filter(F.col('coherency')=='dispar')\n",
    ".groupBy('datasourceId3','targetId3','diseaseId3')\n",
    ".agg(F.count('targetId3').alias('targetI'))\n",
    ".withColumnRenamed('targetId3','targetIdU')\n",
    ".withColumnRenamed('datasourceId3','datasourceI')\n",
    ".withColumnRenamed('diseaseId3','diseaseI')\n",
    "\n",
    ".join((prueba_assessment.filter(F.col('Assessment').isin(terms)==False)), \n",
    "    (F.col('targetIdU')==prueba_assessment.targetId)&\n",
    "    (F.col('datasourceI')==prueba_assessment.datasourceId) &\n",
    "    (F.col('diseaseI')==prueba_assessment.diseaseId),'left')\n",
    ".join(diseases.select('id','name',), F.col('diseaseId')==diseases.id,'left')\n",
    ".groupBy('targetId','diseaseI','name','datasourceI')\n",
    ".agg(F.collect_list('clinicalSignificances').alias('clinicalSignificances'),\n",
    "    F.collect_list('beta').alias('betaValues'),\n",
    "    F.collect_list('OddsRatio').alias('oddsRatio'),\n",
    "    F.collect_list('variantFunctionalConsequenceId').alias('varFunctConsId'),\n",
    "    F.collect_list('drugId').alias('drugId'),\n",
    "    F.collect_set('actionType').alias('actionTypeDif'),\n",
    "    F.collect_list('diseaseFromSource'))\n",
    ".withColumnRenamed('targetId','targetiddd')\n",
    ".withColumnRenamed('name','diseaseName')\n",
    ".join(coherency_assessed, \n",
    "    (F.col('targetIddd')==coherency_assessed.targetId3) &\n",
    "    (F.col('datasourceI')==coherency_assessed.datasourceId3) &\n",
    "    (F.col('diseaseI')==coherency_assessed.diseaseId3)\n",
    ")\n",
    "### anadir el approved symbol:\n",
    ".join(target.select('id','approvedSymbol'), F.col('targetiddd')==target.id, 'left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Include the calculation of overlapping\n",
    "terms=['noEvaluable','bivalent_risk','null','dispar']\n",
    "#### reminder of 'toprocess': \n",
    "toprocess=(prueba_assessment\n",
    ".filter(\n",
    "    (F.col('Assessment').isin(terms)==False))\n",
    ".groupBy('targetId','diseaseId')\n",
    ".pivot('homogenized')\n",
    ".agg(F.count('targetId'))\n",
    ".join(contradictIntra.select('targetId3','diseaseId3','filterOut'),\n",
    "    (F.col('targetId')==contradictIntra.targetId3) & \n",
    "    (F.col('diseaseId')==contradictIntra.diseaseId3)\n",
    "    ,'left')\n",
    ".withColumn('filterOut',\n",
    "    F.when(F.col('filterOut')=='out', F.lit(F.col('filterOut')))\n",
    "    .otherwise(F.lit('keep'))))\n",
    "#####\n",
    "\n",
    "\n",
    "columns=[\n",
    " 'GoF_risk',\n",
    " 'LoF_protect',\n",
    " 'LoF_risk',\n",
    " 'GoF_protect']\n",
    "\n",
    "coherencyInter_assessed_wOut=(toprocess\n",
    "##.filter(F.col('filterOut')!='out')\n",
    ".select(\n",
    "    F.col('targetId'),#.alias('targetId3'),\n",
    "    F.col('diseaseId'),#.alias('diseaseId3'),\n",
    "    F.col('filterOut'),\n",
    "    *(F.col(c).cast(\"int\").alias(c) for c in columns))\n",
    "\n",
    ".withColumn('coherency',\n",
    "    F.when(\n",
    "        (\n",
    "        (F.col('GoF_risk').isNotNull()) &\n",
    "        (F.col('LoF_risk').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('GoF_risk'))-(F.col('LoF_risk'))!=(F.col('GoF_risk'))\n",
    "                ,F.lit('dispar'))\n",
    "        )\n",
    "    .when(\n",
    "        (\n",
    "        (F.col('LoF_protect').isNotNull()) &\n",
    "        (F.col('LoF_risk').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('LoF_protect'))-(F.col('LoF_risk'))!=(F.col('LoF_protect'))\n",
    "                ,F.lit('dispar'))\n",
    "        )   \n",
    "    .when(\n",
    "        (\n",
    "        (F.col('GoF_protect').isNotNull()) &\n",
    "        (F.col('GoF_risk').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('GoF_protect'))-(F.col('GoF_risk'))!=(F.col('GoF_protect'))\n",
    "                ,F.lit('dispar'))\n",
    "        )   \n",
    "    .when(\n",
    "        (\n",
    "        (F.col('GoF_protect').isNotNull()) &\n",
    "        (F.col('LoF_protect').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('GoF_protect'))-(F.col('LoF_protect'))!=(F.col('GoF_protect'))\n",
    "                ,F.lit('dispar'))\n",
    "        )      \n",
    "    .when(\n",
    "        (\n",
    "        (F.col('GoF_protect').isNotNull()) &\n",
    "        (F.col('LoF_risk').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('GoF_protect'))-(F.col('LoF_risk'))!=(F.col('GoF_protect'))\n",
    "                ,F.lit('coherent'))\n",
    "    )      \n",
    "    .when(\n",
    "        (\n",
    "        (F.col('LoF_protect').isNotNull()) &\n",
    "        (F.col('GoF_risk').isNotNull())\n",
    "        ),\n",
    "            F.when(\n",
    "                (F.col('LoF_protect'))-(F.col('GoF_risk'))!=(F.col('LoF_protect'))\n",
    "                ,F.lit('coherent'))\n",
    "        )   \n",
    "    .otherwise(F.lit('take')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build the dataset for Disparities inter datasource: \n",
    "\n",
    "\n",
    "terms=['noEvaluable','bivalent_risk','null','dispar']\n",
    "\n",
    "### Make the incoherencies dataset: join disease name, approved symbol and collect how many evidences are supporting every column\n",
    "\n",
    "interdatasource=(coherencyInter_assessed_wOut\n",
    ".filter(F.col('coherency')=='dispar')\n",
    ".groupBy('targetId','diseaseId')\n",
    ".agg(F.count('targetId').alias('targetI'))\n",
    ".withColumnRenamed('targetId','targetIdU')\n",
    ".withColumnRenamed('diseaseId','diseaseI')\n",
    "\n",
    ".join((prueba_assessment.filter(F.col('Assessment').isin(terms)==False)), \n",
    "    (F.col('targetIdU')==prueba_assessment.targetId)&\n",
    "###    (F.col('datasourceI')==prueba_assessment.datasourceId) &\n",
    "    (F.col('diseaseI')==prueba_assessment.diseaseId),'left')\n",
    ".join(diseases.select('id','name',), F.col('diseaseId')==diseases.id,'left')\n",
    ".groupBy('targetId','diseaseI','name')\n",
    ".agg(F.collect_list('clinicalSignificances').alias('clinicalSignificances'),\n",
    "    F.collect_list('beta').alias('betaValues'),\n",
    "    F.collect_list('OddsRatio').alias('oddsRatio'),\n",
    "    F.collect_list('variantFunctionalConsequenceId').alias('varFunctConsId'),\n",
    "    F.collect_list('drugId').alias('drugId'),\n",
    "    F.collect_set('actionType').alias('actionTypeDif'),\n",
    "    F.collect_set('datasourceId'),\n",
    "    F.collect_list('diseaseFromSource'))\n",
    ".withColumnRenamed('targetId','targetiddd')\n",
    ".withColumnRenamed('name','diseaseName')\n",
    ".join(coherencyInter_assessed_wOut, \n",
    "    (F.col('targetIddd')==coherencyInter_assessed_wOut.targetId) &\n",
    "##    (F.col('datasourceI')==coherency_assessed.datasourceId3) &\n",
    "    (F.col('diseaseI')==coherencyInter_assessed_wOut.diseaseId)\n",
    ")\n",
    "### anadir el approved symbol:\n",
    ".join(target.select('id','approvedSymbol'), F.col('targetiddd')==target.id, 'left')\n",
    ")\n",
    "\n",
    "##interdatasource.count()\n",
    "### 13.548 counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### las columnas con null no nos valen para las operaciones, necesitamos cambiarlas por 0: \n",
    "\n",
    "##interdatasource.fillna(value=0, subset=[\"GoF_risk\",\"LoF_protect\",\"LoF_risk\",\"GoF_protect\",'totalEvidences'])\n",
    "\n",
    "interdatasourcePercentage=(interdatasource\n",
    "\n",
    ".withColumn('minoritaryPercentage',\n",
    "    F.when(\n",
    "            F.col('LoF_protect')>= F.col('LoF_risk'), F.lit((F.col('LoF_risk')/(F.col('LoF_risk')+F.col('LoF_protect')))*100))\n",
    "    .when(\n",
    "            F.col('LoF_protect') < F.col('LoF_risk'), F.lit((F.col('LoF_protect')/(F.col('LoF_risk')+F.col('LoF_protect')))*100))\n",
    "\n",
    "    .when(\n",
    "            F.col('GoF_protect') < F.col('GoF_risk'), F.lit((F.col('GoF_protect')/(F.col('GoF_protect')+F.col('GoF_risk')))*100))\n",
    "    .when(\n",
    "            F.col('GoF_protect') >= F.col('GoF_risk'), F.lit((F.col('GoF_risk')/(F.col('GoF_protect')+F.col('GoF_risk')))*100))\n",
    "            \n",
    "    .when(\n",
    "            F.col('GoF_risk') < F.col('LoF_risk'), F.lit((F.col('GoF_risk')/(F.col('GoF_risk')+F.col('LoF_risk')))*100))\n",
    "    .when(\n",
    "            F.col('GoF_risk') >= F.col('LoF_risk'), F.lit((F.col('LoF_risk')/(F.col('GoF_risk')+F.col('LoF_risk')))*100))    \n",
    "\n",
    "    .when(\n",
    "            F.col('GoF_protect') < F.col('LoF_protect'), F.lit((F.col('GoF_protect')/(F.col('GoF_protect')+F.col('LoF_protect')))*100))\n",
    "    .when(\n",
    "            F.col('GoF_protect') >= F.col('LoF_protect'), F.lit((F.col('LoF_protect')/(F.col('GoF_protect')+F.col('LoF_protect')))*100))   \n",
    "    \n",
    "    )\n",
    ".fillna(value=0, subset=[\"GoF_risk\",\"LoF_protect\",\"LoF_risk\",\"GoF_protect\"])\n",
    ".withColumn('totalEvidences',\n",
    "        F.expr(\"GoF_risk + LoF_protect + LoF_risk + GoF_protect\")\n",
    "))\n",
    "### .toPandas().to_csv('interdatasourcetest_TODAY.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Filter by 'take' and study the distribution of evidences\n",
    "\n",
    "countPairsEvidences=(coherencyInter_assessed_wOut\n",
    ".filter(\n",
    "    (F.col('coherency')=='take'))\n",
    ".fillna(value=0, subset=[\"GoF_risk\",\"LoF_protect\",\"LoF_risk\",\"GoF_protect\"])\n",
    ".withColumn('totalEvidences',\n",
    "        F.expr(\"GoF_risk + LoF_protect + LoF_risk + GoF_protect\"))\n",
    ".withColumn('totalEvidences_woLR',\n",
    "        F.expr(\"GoF_risk + LoF_protect + GoF_protect\")\n",
    "))\n",
    "countPairsEvidences2=countPairsEvidences.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### make the matrix of coincidences between datasources:\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "\n",
    "terms=['noEvaluable','bivalent_risk','null','dispar']\n",
    "\n",
    "tdds=(prueba_assessment ### order target-traits pairs by datasource\n",
    ".filter(\n",
    "    (F.col('Assessment').isin(terms)==False))\n",
    ".groupBy('targetId','diseaseId','datasourceId')\n",
    ".agg(F.count('targetId'))\n",
    ".select(F.col('targetId').alias('targetIddd'),\n",
    "    F.col('diseaseId').alias('diseaseIdddd'),\n",
    "    F.col('datasourceId')))\n",
    "\n",
    " ### multiply every target-trait by every datasource they appear on. \n",
    " # Add an unique ID to identify and count distinctly\n",
    "\n",
    "analysis=(prueba_assessment\n",
    ".filter(\n",
    "    (F.col('Assessment').isin(terms)==False))\n",
    ".groupBy('targetId','diseaseId')\n",
    ".agg(F.count('targetId'))\n",
    ".withColumn('nr',monotonically_increasing_id())\n",
    ".join(tdds, (F.col('targetId')==tdds.targetIddd) & (F.col('diseaseId')==tdds.diseaseIdddd),'left')\n",
    ")\n",
    "analysisdo=analysis.select('targetId','diseaseId','nr','datasourceId')\n",
    "\n",
    "df_collect = (analysisdo\n",
    "## user_id = nr\n",
    "## item_id = datasourceId\n",
    "        .select(\"nr\", \"datasourceId\")\n",
    "        .groupBy(\"datasourceId\")\n",
    "        .agg(F.collect_set(\"nr\").alias(\"nrs\")))\n",
    "### Step 2. Cross join df_collect with itself to get all item-item combinations\n",
    "\n",
    "df_crossjoin = (df_collect\n",
    "                    .join(df_collect\n",
    "                            .withColumnRenamed(\"datasourceId\", \"datasourceId_y\")\n",
    "                            .withColumnRenamed(\"nrs\", \"nrs_y\")))\n",
    "### Step 2. Find user union and intersection and the count\n",
    "\n",
    "df_ui = (df_crossjoin\n",
    "                 .withColumn(\"nrs_union\", \n",
    "                         F.size((F.array_union(\"nrs\", \"nrs_y\"))))\n",
    "                 .withColumn(\"nrs_intersect\", \n",
    "                             F.size(F.array_intersect(\"nrs\", \"nrs_y\"))))\n",
    "    \n",
    "### Step 3. Pivot to get item-item matrix\n",
    "\n",
    "df_matrix_union = (df_ui\n",
    "                   .groupBy(\"datasourceId\")\n",
    "                   .pivot(\"datasourceId_y\")\n",
    "                   .agg(F.first(\"nrs_union\"))\n",
    "                   .orderBy(\"datasourceId\"))\n",
    "\n",
    "df_matrix_intrsct = (df_ui\n",
    "                   .groupBy(\"datasourceId\")\n",
    "                   .pivot(\"datasourceId_y\")\n",
    "                   .agg(F.first(\"nrs_intersect\"))\n",
    "                   .orderBy(\"datasourceId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matrix of contradictions per datasource and intradatasource \n",
    "###\n",
    "######\n",
    "## to construct the matrix of coincidences between datasources we need to get: coherent and dispar. \n",
    "\n",
    "## 1) Get pairs target-diseases with label dispar/coherent\n",
    "toget=['dispar','coherent']\n",
    "pairs_analysis=(coherencyInter_assessed_wOut\n",
    ".filter(F.col('coherency').isin(toget))\n",
    ".withColumnRenamed('targetId','targetId2')\n",
    ".withColumnRenamed('diseaseId','diseaseId2')\n",
    ")\n",
    "testeado=toprocess.join(pairs_analysis, (pairs_analysis.targetId2==F.col('targetId')) & (pairs_analysis.diseaseId2==F.col('diseaseId')), 'right')\n",
    "\n",
    "#######\n",
    "\n",
    "toprocess=(prueba_assessment\n",
    ".filter(\n",
    "    (F.col('Assessment').isin(terms)==False))\n",
    ".groupBy('targetId','diseaseId')\n",
    ".pivot('homogenized')\n",
    ".agg(F.count('targetId'))\n",
    ".join(contradictIntra.select('targetId3','diseaseId3','filterOut'),\n",
    "    (F.col('targetId')==contradictIntra.targetId3) & \n",
    "    (F.col('diseaseId')==contradictIntra.diseaseId3)\n",
    "    ,'left')\n",
    ".withColumn('filterOut2',\n",
    "    F.when(F.col('filterOut')=='out', F.lit(F.col('filterOut')))\n",
    "    .otherwise(F.lit('keep')))\n",
    ".drop('filterOut')\n",
    "\n",
    ")\n",
    "\n",
    "pairs_analysis=(coherencyInter_assessed_wOut\n",
    ".filter(F.col('coherency').isin(toget))\n",
    ".withColumnRenamed('targetId','targetId2')\n",
    ".withColumnRenamed('diseaseId','diseaseId2')\n",
    ")\n",
    "testeado=toprocess.join(pairs_analysis, (pairs_analysis.targetId2==F.col('targetId')) & (pairs_analysis.diseaseId2==F.col('diseaseId')), 'right')\n",
    "\n",
    "allcontradict_coherent=(testeado\n",
    ".filter( \n",
    "    (F.col('coherency')=='dispar') & (F.col('filterOut')=='out') | \n",
    "    (F.col('coherency')=='dispar') & (F.col('filterOut')=='keep') | \n",
    "    (F.col('coherency')=='coherent') & (F.col('filterOut')=='keep'))\n",
    ".select('targetId','diseaseId')\n",
    ".withColumn('nr',monotonically_increasing_id())\n",
    ".join(tdds, (F.col('targetId')==tdds.targetIddd) & (F.col('diseaseId')==tdds.diseaseIdddd),'left')\n",
    ".select('targetId','diseaseId','nr','datasourceId'))\n",
    "\n",
    "\n",
    "#### make the pipeline \n",
    "\n",
    "\n",
    "\n",
    "df_collect = (allcontradict_coherent\n",
    "## user_id = nr\n",
    "## item_id = datasourceId\n",
    "        .select(\"nr\", \"datasourceId\")\n",
    "        .groupBy(\"datasourceId\")\n",
    "        .agg(F.collect_set(\"nr\").alias(\"nrs\")))\n",
    "### Step 2. Cross join df_collect with itself to get all item-item combinations\n",
    "\n",
    "df_crossjoin = (df_collect\n",
    "                    .join(df_collect\n",
    "                            .withColumnRenamed(\"datasourceId\", \"datasourceId_y\")\n",
    "                            .withColumnRenamed(\"nrs\", \"nrs_y\")))\n",
    "### Step 2. Find user union and intersection and the count\n",
    "\n",
    "df_ui = (df_crossjoin\n",
    "                 .withColumn(\"nrs_union\", \n",
    "                         F.size((F.array_union(\"nrs\", \"nrs_y\"))))\n",
    "                 .withColumn(\"nrs_intersect\", \n",
    "                             F.size(F.array_intersect(\"nrs\", \"nrs_y\"))))\n",
    "    \n",
    "### Step 3. Pivot to get item-item matrix\n",
    "\n",
    "df_matrix_union = (df_ui\n",
    "                   .groupBy(\"datasourceId\")\n",
    "                   .pivot(\"datasourceId_y\")\n",
    "                   .agg(F.first(\"nrs_union\"))\n",
    "                   .orderBy(\"datasourceId\"))\n",
    "\n",
    "df_matrix_intrsct = (df_ui\n",
    "                   .groupBy(\"datasourceId\")\n",
    "                   .pivot(\"datasourceId_y\")\n",
    "                   .agg(F.first(\"nrs_intersect\"))\n",
    "                   .orderBy(\"datasourceId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matrix of contradictions per datasource and intradatasource <<removing COHERENT>>\n",
    "\n",
    "allcontradict_WOcoherent=(testeado\n",
    ".filter( \n",
    "    (F.col('coherency')=='dispar') & (F.col('filterOut')=='out') | \n",
    "    (F.col('coherency')=='dispar') & (F.col('filterOut')=='keep')\n",
    "    ## | (F.col('coherency')=='coherent') & (F.col('filterOut')=='keep')\n",
    "    )\n",
    ".select('targetId','diseaseId')\n",
    ".withColumn('nr',monotonically_increasing_id())\n",
    ".join(tdds, (F.col('targetId')==tdds.targetIddd) & (F.col('diseaseId')==tdds.diseaseIdddd),'left')\n",
    ".select('targetId','diseaseId','nr','datasourceId'))\n",
    "\n",
    "\n",
    "#### make the pipeline \n",
    "\n",
    "df_collect = (allcontradict_WOcoherent\n",
    "## user_id = nr\n",
    "## item_id = datasourceId\n",
    "        .select(\"nr\", \"datasourceId\")\n",
    "        .groupBy(\"datasourceId\")\n",
    "        .agg(F.collect_set(\"nr\").alias(\"nrs\")))\n",
    "### Step 2. Cross join df_collect with itself to get all item-item combinations\n",
    "\n",
    "df_crossjoin = (df_collect\n",
    "                    .join(df_collect\n",
    "                            .withColumnRenamed(\"datasourceId\", \"datasourceId_y\")\n",
    "                            .withColumnRenamed(\"nrs\", \"nrs_y\")))\n",
    "### Step 2. Find user union and intersection and the count\n",
    "\n",
    "df_ui = (df_crossjoin\n",
    "                 .withColumn(\"nrs_union\", \n",
    "                         F.size((F.array_union(\"nrs\", \"nrs_y\"))))\n",
    "                 .withColumn(\"nrs_intersect\", \n",
    "                             F.size(F.array_intersect(\"nrs\", \"nrs_y\"))))\n",
    "    \n",
    "### Step 3. Pivot to get item-item matrix\n",
    "\n",
    "df_matrix_union = (df_ui\n",
    "                   .groupBy(\"datasourceId\")\n",
    "                   .pivot(\"datasourceId_y\")\n",
    "                   .agg(F.first(\"nrs_union\"))\n",
    "                   .orderBy(\"datasourceId\"))\n",
    "\n",
    "df_matrix_intrsct_alldispar = (df_ui\n",
    "                   .groupBy(\"datasourceId\")\n",
    "                   .pivot(\"datasourceId_y\")\n",
    "                   .agg(F.first(\"nrs_intersect\"))\n",
    "                   .orderBy(\"datasourceId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matrix of ONLY <contradictions per datasource>> and intradatasource <<removing COHERENT and INTRAdatasource. \n",
    "\n",
    "allcontradict_WOcoherentWOintradata=(testeado\n",
    ".filter( \n",
    "    (F.col('coherency')=='dispar') & (F.col('filterOut')=='keep') \n",
    "    ## | (F.col('coherency')=='dispar') & (F.col('filterOut')=='keep')\n",
    "    ## | (F.col('coherency')=='coherent') & (F.col('filterOut')=='keep')\n",
    "    )\n",
    ".select('targetId','diseaseId')\n",
    ".withColumn('nr',monotonically_increasing_id())\n",
    ".join(tdds, (F.col('targetId')==tdds.targetIddd) & (F.col('diseaseId')==tdds.diseaseIdddd),'left')\n",
    ".select('targetId','diseaseId','nr','datasourceId'))\n",
    "\n",
    "\n",
    "#### make the pipeline \n",
    "\n",
    "\n",
    "\n",
    "df_collect = (allcontradict_WOcoherentWOintradata\n",
    "## user_id = nr\n",
    "## item_id = datasourceId\n",
    "        .select(\"nr\", \"datasourceId\")\n",
    "        .groupBy(\"datasourceId\")\n",
    "        .agg(F.collect_set(\"nr\").alias(\"nrs\")))\n",
    "### Step 2. Cross join df_collect with itself to get all item-item combinations\n",
    "\n",
    "df_crossjoin = (df_collect\n",
    "                    .join(df_collect\n",
    "                            .withColumnRenamed(\"datasourceId\", \"datasourceId_y\")\n",
    "                            .withColumnRenamed(\"nrs\", \"nrs_y\")))\n",
    "### Step 2. Find user union and intersection and the count\n",
    "\n",
    "df_ui = (df_crossjoin\n",
    "                 .withColumn(\"nrs_union\", \n",
    "                         F.size((F.array_union(\"nrs\", \"nrs_y\"))))\n",
    "                 .withColumn(\"nrs_intersect\", \n",
    "                             F.size(F.array_intersect(\"nrs\", \"nrs_y\"))))\n",
    "    \n",
    "### Step 3. Pivot to get item-item matrix\n",
    "\n",
    "df_matrix_union = (df_ui\n",
    "                   .groupBy(\"datasourceId\")\n",
    "                   .pivot(\"datasourceId_y\")\n",
    "                   .agg(F.first(\"nrs_union\"))\n",
    "                   .orderBy(\"datasourceId\"))\n",
    "\n",
    "df_matrix_intrsct_alldispar_WOintradata = (df_ui\n",
    "                   .groupBy(\"datasourceId\")\n",
    "                   .pivot(\"datasourceId_y\")\n",
    "                   .agg(F.first(\"nrs_intersect\"))\n",
    "                   .orderBy(\"datasourceId\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
