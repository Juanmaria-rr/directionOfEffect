{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark session created at 2025-09-10 14:59:00.115158\n",
      "Analysis started on 2025-09-10 at  2025-09-10 14:59:00.115158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 14:59:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/09/10 14:59:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created with:\n",
      "  spark.driver.memory: 12g\n",
      "  spark.executor.memory: 40g\n",
      "  spark.executor.cores: 10\n",
      "  spark.executor.instances: 1\n",
      "  spark.yarn.executor.memoryOverhead: 6g\n",
      "  spark.sql.shuffle.partitions: 128\n",
      "  spark.default.parallelism: 128\n",
      "  spark.sql.adaptive.enabled: true\n",
      "  spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "Spark UI: http://jr-temp-doe-m.c.open-targets-eu-dev.internal:43343\n",
      "Loaded all base tables.\n",
      "Built newColoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded gwasComplete\n",
      "Built gwasComplete\n",
      "Built resolvedColoc\n",
      "Built temporary DoE datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built analysis_chembl_indication\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Single-script, loop-free PySpark job (tall/unpivot + single aggregation)\n",
    "\n",
    "import os\n",
    "from datetime import date\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    ")\n",
    "\n",
    "# Your helpers\n",
    "from functions import (\n",
    "    relative_success,\n",
    "    spreadSheetFormatter,\n",
    "    discrepancifier,\n",
    "    temporary_directionOfEffect,\n",
    "    buildColocData,\n",
    "    gwasDataset,\n",
    ")\n",
    "from DoEAssessment import directionOfEffect  # noqa: F401  (kept if you need it later)\n",
    "\n",
    "# -------------------------------\n",
    "# Spark / YARN resource settings (Single-Node Option A)\n",
    "# -------------------------------\n",
    "driver_memory = \"12g\"                 # string with unit\n",
    "executor_memory = \"40g\"               # string with unit (heap)\n",
    "executor_cores = 10                   # int\n",
    "num_executors = 1                     # int (one fat executor on single node)\n",
    "executor_memory_overhead = \"6g\"       # string with unit (PySpark/Arrow/off-heap)\n",
    "shuffle_partitions = 128              # int (~2–3x cores)\n",
    "default_parallelism = 128             # int (match shuffle_partitions)\n",
    "\n",
    "# If you later move to a multi-worker cluster, replace the values above.\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MyOptimizedPySparkApp\")\n",
    "    .config(\"spark.master\", \"yarn\")\n",
    "    # core resources\n",
    "    .config(\"spark.driver.memory\", driver_memory)\n",
    "    .config(\"spark.executor.memory\", executor_memory)\n",
    "    .config(\"spark.executor.cores\", executor_cores)\n",
    "    .config(\"spark.executor.instances\", num_executors)\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead)\n",
    "    # shuffle & parallelism\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions)\n",
    "    .config(\"spark.default.parallelism\", default_parallelism)\n",
    "    # adaptive query execution for better skew/partition sizing\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession created with:\")\n",
    "for k in [\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.executor.cores\",\n",
    "    \"spark.executor.instances\",\n",
    "    \"spark.yarn.executor.memoryOverhead\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.default.parallelism\",\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "]:\n",
    "    print(f\"  {k}: {spark.conf.get(k)}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# -------------------------------\n",
    "# Spark / YARN resource settings\n",
    "# -------------------------------\n",
    "driver_memory = \"16g\"\n",
    "executor_memory = \"32g\"\n",
    "executor_cores = \"8\"\n",
    "num_executors = \"16\"\n",
    "executor_memory_overhead = \"8g\"\n",
    "shuffle_partitions = \"150\"\n",
    "default_parallelism = str(int(executor_cores) * int(num_executors) * 2)  # 80\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MyOptimizedPySparkApp\")\n",
    "    .config(\"spark.master\", \"yarn\")\n",
    "    .config(\"spark.driver.memory\", driver_memory)\n",
    "    .config(\"spark.executor.memory\", executor_memory)\n",
    "    .config(\"spark.executor.cores\", executor_cores)\n",
    "    .config(\"spark.executor.instances\", num_executors)\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead)\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions)\n",
    "    .config(\"spark.default.parallelism\", default_parallelism)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession created with:\")\n",
    "for k in [\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.executor.cores\",\n",
    "    \"spark.executor.instances\",\n",
    "    \"spark.yarn.executor.memoryOverhead\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.default.parallelism\",\n",
    "]:\n",
    "    print(f\"  {k}: {spark.conf.get(k)}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "'''\n",
    "# --------------------------------\n",
    "# 0) Load inputs\n",
    "# --------------------------------\n",
    "path_n = \"gs://open-targets-data-releases/25.06/output/\"\n",
    "\n",
    "target = spark.read.parquet(f\"{path_n}target/\")\n",
    "diseases = spark.read.parquet(f\"{path_n}disease/\")\n",
    "evidences = spark.read.parquet(f\"{path_n}evidence\")\n",
    "credible = spark.read.parquet(f\"{path_n}credible_set\")\n",
    "new = spark.read.parquet(f\"{path_n}colocalisation_coloc\")\n",
    "index = spark.read.parquet(f\"{path_n}study/\")\n",
    "variantIndex = spark.read.parquet(f\"{path_n}variant\")\n",
    "biosample = spark.read.parquet(f\"{path_n}biosample\")\n",
    "ecaviar = spark.read.parquet(f\"{path_n}colocalisation_ecaviar\")\n",
    "all_coloc = ecaviar.unionByName(new, allowMissingColumns=True)\n",
    "print(\"Loaded all base tables.\")\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build coloc + GWAS dataset\n",
    "# --------------------------------\n",
    "newColoc = buildColocData(all_coloc, credible, index)\n",
    "print(\"Built newColoc\")\n",
    "\n",
    "gwasComplete = gwasDataset(evidences, credible)\n",
    "print(\"Built gwasComplete\")\n",
    "\n",
    "resolvedColoc = (\n",
    "    newColoc.withColumnRenamed(\"geneId\", \"targetId\")\n",
    "    .join(\n",
    "        gwasComplete.withColumnRenamed(\"studyLocusId\", \"leftStudyLocusId\"),\n",
    "        on=[\"leftStudyLocusId\", \"targetId\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .join(\n",
    "        diseases.selectExpr(\"id as diseaseId\", \"name\", \"parents\", \"therapeuticAreas\"),\n",
    "        on=\"diseaseId\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"diseaseId\",\n",
    "        F.explode_outer(F.concat(F.array(F.col(\"diseaseId\")), F.col(\"parents\"))),\n",
    "    )\n",
    "    .drop(\"parents\", \"oldDiseaseId\")\n",
    "    .withColumn(\n",
    "        \"colocDoE\",\n",
    "        F.when(\n",
    "            F.col(\"rightStudyType\").isin([\"eqtl\", \"pqtl\", \"tuqtl\", \"sceqtl\", \"sctuqtl\"]),\n",
    "            F.when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"GoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"LoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"LoF_protect\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"GoF_protect\"))\n",
    "        ).when(\n",
    "            F.col(\"rightStudyType\").isin([\"sqtl\", \"scsqtl\"]),\n",
    "            F.when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"LoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"GoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"GoF_protect\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"LoF_protect\"))\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "print(\"Built resolvedColoc\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Direction of Effect & ChEMBL indication\n",
    "# --------------------------------\n",
    "datasource_filter = [\n",
    "    \"gwas_credible_sets\",\n",
    "    \"gene_burden\",\n",
    "    \"eva\",\n",
    "    \"eva_somatic\",\n",
    "    \"gene2phenotype\",\n",
    "    \"orphanet\",\n",
    "    \"cancer_gene_census\",\n",
    "    \"intogen\",\n",
    "    \"impc\",\n",
    "    \"chembl\",\n",
    "]\n",
    "assessment, evidences, actionType_unused, oncolabel_unused = temporary_directionOfEffect(path_n, datasource_filter)\n",
    "print(\"Built temporary DoE datasets\")\n",
    "\n",
    "# (Optional) Add MoA to ChEMBL paths as in your later code\n",
    "mecact_path = f\"{path_n}drug_mechanism_of_action/\"\n",
    "mecact = spark.read.parquet(mecact_path)\n",
    "actionType = (\n",
    "    mecact.select(\n",
    "        F.explode_outer(\"chemblIds\").alias(\"drugId\"),\n",
    "        \"actionType\",\n",
    "        \"mechanismOfAction\",\n",
    "        \"targets\",\n",
    "    )\n",
    "    .select(\n",
    "        F.explode_outer(\"targets\").alias(\"targetId\"),\n",
    "        \"drugId\",\n",
    "        \"actionType\",\n",
    "        \"mechanismOfAction\",\n",
    "    )\n",
    "    .groupBy(\"targetId\", \"drugId\")\n",
    "    .agg(F.collect_set(\"actionType\").alias(\"actionType2\"))\n",
    "    .withColumn(\"nMoA\", F.size(F.col(\"actionType2\")))\n",
    ")\n",
    "\n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "        .join(actionType, on=[\"targetId\", \"drugId\"], how=\"left\")\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(\"clinicalPhase\").over(Window.partitionBy(\"targetId\", \"diseaseId\")),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\", \"actionType2\")\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    .drop(\"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\")\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    ")\n",
    "print(\"Built analysis_chembl_indication\")\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Benchmark (filtered coloc) + clinical phase flags\n",
    "# --------------------------------\n",
    "resolvedColocFiltered = resolvedColoc.filter((F.col(\"clpp\") >= 0.01) | (F.col(\"h4\") >= 0.8))\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\").count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\")).drop(\"count\")\n",
    ")\n",
    "benchmark = (\n",
    "    resolvedColocFiltered.filter(F.col(\"name\") != \"COVID-19\")\n",
    "    .join(analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\")\n",
    "    .withColumn(\n",
    "        \"AgreeDrug\",\n",
    "        F.when((F.col(\"drugGoF_protect\").isNotNull()) & (F.col(\"colocDoE\") == \"GoF_protect\"), \"yes\")\n",
    "        .when((F.col(\"drugLoF_protect\").isNotNull()) & (F.col(\"colocDoE\") == \"LoF_protect\"), \"yes\")\n",
    "        .otherwise(\"no\"),\n",
    "    )\n",
    "    .join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    ")\n",
    "\n",
    "benchmark = (\n",
    "    benchmark.join(F.broadcast(negativeTD), on=[\"targetId\", \"diseaseId\"], how=\"left\")\n",
    "    .withColumn(\"PhaseT\", F.when(F.col(\"stopReason\") == \"Negative\", \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=4\", F.when((F.col(\"maxClinPhase\") == 4) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=3\", F.when((F.col(\"maxClinPhase\") >= 3) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=2\", F.when((F.col(\"maxClinPhase\") >= 2) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=1\", F.when((F.col(\"maxClinPhase\") >= 1) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Replace nested loops:\n",
    "#     compute DoE counts once → derive flags → unpivot → single aggregation\n",
    "# --------------------------------\n",
    "doe_cols = [\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "\n",
    "# counts per colocDoE over the grouping you previously used in the loop\n",
    "group_keys = [\n",
    "    \"targetId\", \"diseaseId\", \"maxClinPhase\",\n",
    "    \"actionType2\", \"biosampleName\", \"projectId\", \"rightStudyType\", \"colocalisationMethod\"\n",
    "]\n",
    "\n",
    "doe_counts = (\n",
    "    benchmark.groupBy(*group_keys)\n",
    "    .agg(*[F.sum(F.when(F.col(\"colocDoE\") == c, 1).otherwise(0)).alias(c) for c in doe_cols])\n",
    ")\n",
    "\n",
    "# max name(s) (in case of ties) without arrays of structs\n",
    "greatest_count = F.greatest(*[F.col(c) for c in doe_cols])\n",
    "max_names = F.filter(\n",
    "    F.array(*[F.when(F.col(c) == greatest_count, F.lit(c)) for c in doe_cols]),\n",
    "    lambda x: x.isNotNull()\n",
    ")\n",
    "\n",
    "# presence of drug-side signals (equivalent to *_ch presence in your loop path)\n",
    "has_lof_ch = F.col(\"drugLoF_protect\").isNotNull()\n",
    "has_gof_ch = F.col(\"drugGoF_protect\").isNotNull()\n",
    "\n",
    "\n",
    "test2.unpersist()\n",
    "test2 = (\n",
    "    benchmark.select(*group_keys, \"drugLoF_protect\", \"drugGoF_protect\")\n",
    "    .join(doe_counts, on=group_keys, how=\"left\")\n",
    "    .withColumn(\"NoneCellYes\",\n",
    "        F.when(has_lof_ch & (~has_gof_ch) & F.array_contains(max_names, F.lit(\"LoF_protect\")), \"yes\")\n",
    "         .when(has_gof_ch & (~has_lof_ch) & F.array_contains(max_names, F.lit(\"GoF_protect\")), \"yes\")\n",
    "         .otherwise(\"no\")\n",
    "    )\n",
    "    .withColumn(\"NdiagonalYes\",\n",
    "        F.when(has_lof_ch & (~has_gof_ch) & (F.array_contains(max_names, F.lit(\"LoF_protect\")) | F.array_contains(max_names, F.lit(\"GoF_risk\"))), \"yes\")\n",
    "         .when(has_gof_ch & (~has_lof_ch) & (F.array_contains(max_names, F.lit(\"GoF_protect\")) | F.array_contains(max_names, F.lit(\"LoF_risk\"))), \"yes\")\n",
    "         .otherwise(\"no\")\n",
    "    )\n",
    "    .withColumn(\"drugCoherency\",\n",
    "        F.when(has_lof_ch & ~has_gof_ch, \"coherent\")\n",
    "         .when(~has_lof_ch & has_gof_ch, \"coherent\")\n",
    "         .when(has_lof_ch & has_gof_ch, \"dispar\")\n",
    "         .otherwise(\"other\")\n",
    "    ).withColumn(\n",
    "    \"hasGenetics2\",\n",
    "    F.when(\n",
    "        reduce(lambda acc, c: acc & F.col(c).isNull(), doe_cols[1:], F.col(doe_cols[0]).isNull()),\n",
    "        F.lit(\"no\")\n",
    "    ).otherwise(F.lit(\"yes\"))\n",
    ")\n",
    "    .withColumn(\"hasGenetics\", F.when(F.col(\"NdiagonalYes\").isNotNull(), \"yes\").otherwise(\"no\")) #### we have to change it\n",
    ")\n",
    "test2.persist()\n",
    "\n",
    "common_cols = [\n",
    "    \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "    \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\",\n",
    "    \"NoneCellYes\",\"NdiagonalYes\",\"hasGenetics2\"\n",
    "]\n",
    "\n",
    "# 1) actionType2 is ARRAY<STRING> → explode to one value per row\n",
    "long_action = (\n",
    "    test2.join(\n",
    "        benchmark.select(\n",
    "            \"targetId\",\"diseaseId\",\"maxClinPhase\",\"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\"\n",
    "        ).dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"]),\n",
    "        on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"left\"\n",
    "    )\n",
    "    .select(*common_cols, F.explode_outer(\"actionType2\").alias(\"value\"))\n",
    "    .withColumn(\"feature\", F.lit(\"actionType2\"))\n",
    "    .select(*common_cols, \"feature\", \"value\")\n",
    ")\n",
    "\n",
    "# 2–5) the scalar columns\n",
    "def longify_scalar(colname: str):\n",
    "    return (\n",
    "        test2.join(\n",
    "            benchmark.select(\n",
    "                \"targetId\",\"diseaseId\",\"maxClinPhase\",\"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\"\n",
    "            ).dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"]),\n",
    "            on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"left\"\n",
    "        )\n",
    "        .select(*common_cols, F.col(colname).alias(\"value\"))\n",
    "        .withColumn(\"feature\", F.lit(colname))\n",
    "        .select(*common_cols, \"feature\", \"value\")\n",
    "    )\n",
    "\n",
    "long_biosample = longify_scalar(\"biosampleName\")\n",
    "long_project   = longify_scalar(\"projectId\")\n",
    "long_rstype    = longify_scalar(\"rightStudyType\")\n",
    "long_colocm    = longify_scalar(\"colocalisationMethod\")\n",
    "\n",
    "# Union them (schema-aligned)\n",
    "long_features = (\n",
    "    long_action\n",
    "    .unionByName(long_biosample)\n",
    "    .unionByName(long_project)\n",
    "    .unionByName(long_rstype)\n",
    "    .unionByName(long_colocm)\n",
    ").filter(F.col(\"value\").isNotNull())\n",
    "\n",
    "\n",
    "# Single aggregation for ALL features and ALL metrics (booleans as max over yes/no)\n",
    "agg_once = (\n",
    "    long_features\n",
    "    .groupBy(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\",\"feature\",\"value\")\n",
    "    .agg(\n",
    "        F.max(F.when(F.col(\"NoneCellYes\")==\"yes\", 1).otherwise(0)).alias(\"NoneCellYes\"),\n",
    "        F.max(F.when(F.col(\"NdiagonalYes\")==\"yes\", 1).otherwise(0)).alias(\"NdiagonalYes\"),\n",
    "        F.max(F.when(F.col(\"hasGenetics2\")==\"yes\", 1).otherwise(0)).alias(\"hasGenetics\"),\n",
    "    )\n",
    "    .selectExpr(\n",
    "        \"*\",\n",
    "        \"CASE WHEN NoneCellYes=1 THEN 'yes' ELSE 'no' END as NoneCellYes_flag\",\n",
    "        \"CASE WHEN NdiagonalYes=1 THEN 'yes' ELSE 'no' END as NdiagonalYes_flag\",\n",
    "        \"CASE WHEN hasGenetics=1 THEN 'yes' ELSE 'no' END as hasGenetics_flag\"\n",
    "    )\n",
    ")\n",
    "# ---- make sure 'yes'/'no' columns exist as ints (we already did fillna(0), but be explicit)\n",
    "mat_counts = (\n",
    "    mat_counts\n",
    "    .withColumn(\"yes\", F.coalesce(F.col(\"yes\"), F.lit(0)).cast(\"int\"))\n",
    "    .withColumn(\"no\",  F.coalesce(F.col(\"no\"),  F.lit(0)).cast(\"int\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions:\n",
    "##### Find how the matrix 2x2 is done\n",
    "##### substitute rel_success and odds ratio with our formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing functions\n",
      "imported functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_167 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_125 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_101 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_112 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_115 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_34 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_98 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_49 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_128 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_59 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_99 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_59 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_106 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_75 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_83 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_21 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_157 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_54 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_88 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_106 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_31 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_52 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_145 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_91 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_29 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_40 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_11 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_33 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_40 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_30 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_13 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_0 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_84 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_111 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_54 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_38 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_46 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_86 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_5 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_94 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_32 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_19 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_19 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_94 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_75 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_38 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_15 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_66 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_66 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_70 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_39 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_57 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_116 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_91 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_5 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_107 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_58 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_35 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_23 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_28 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_138 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_7 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_106 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_15 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_121 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_18 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_98 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_114 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_166 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_121 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_45 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_75 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_30 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_84 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_9 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_26 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_2 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_25 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_148 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_120 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_33 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_66 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_95 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_92 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_151 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_10 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_111 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_54 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_111 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_126 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_75 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_81 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_57 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_4 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_52 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_46 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_107 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_87 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_18 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_25 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_55 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_52 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_111 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_27 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_79 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_107 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_5 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_74 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_63 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_48 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_120 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_49 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_80 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_36 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_7 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_19 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_33 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_102 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_92 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_68 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_107 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_17 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_20 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_108 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_33 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_116 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_64 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_87 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_113 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_16 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_130 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_50 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_92 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_114 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_71 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_71 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_124 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_125 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_116 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_64 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_83 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_53 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_98 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_120 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_87 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_74 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_48 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_48 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_57 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_24 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_39 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_40 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_15 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_23 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_115 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_94 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_30 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_67 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_121 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_53 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_33 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_34 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_86 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_113 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_40 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_27 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_112 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_65 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_32 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_121 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_15 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_81 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_64 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_61 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_103 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_120 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_13 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_24 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_74 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_83 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_79 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_66 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_159 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_114 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_101 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_34 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_115 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_6 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_1 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_150 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_58 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_59 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_18 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_106 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_11 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_52 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_85 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_17 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_64 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_48 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_45 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_95 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_81 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_37 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_29 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_24 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_13 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_72 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_7 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_89 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_144 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_84 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_120 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_109 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_96 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_104 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_3 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_95 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_105 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_3 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_59 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_70 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_71 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_103 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_173 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_121 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_9 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_79 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_46 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_5 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_24 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_29 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_1 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_82 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_111 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_39 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_67 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_17 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_25 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_65 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_127 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_107 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_169 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_39 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_20 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_50 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_47 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_141 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_14 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_161 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_86 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_20 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_11 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_153 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_119 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_100 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_70 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_127 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_50 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_123 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_116 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_9 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_13 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_87 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_84 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_67 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_114 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_91 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_99 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_70 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_127 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_164 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_3 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_41 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_75 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_89 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_46 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_37 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_30 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_117 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_72 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_50 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_73 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_3 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_7 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_32 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_78 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_83 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_142 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_54 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_1 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_99 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_103 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_101 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_87 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_113 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_94 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_59 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_133 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_80 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_29 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_12 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_95 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_69 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_86 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_93 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_37 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_37 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_8 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_101 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_57 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_137 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_91 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_77 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_51 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_99 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_11 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_103 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_18 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_9 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_94 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_62 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_64 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_1 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_65 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_158 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_98 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_73 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_38 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_74 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_50 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_112 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_32 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_92 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_71 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_105 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_58 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_218_122 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_127 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_61 !\n",
      "25/09/10 10:23:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_49 !\n",
      "25/09/10 10:23:11 WARN YarnAllocator: Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:23:11 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:23:11 ERROR YarnScheduler: Lost executor 1 on jr-temp-doe-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:23:11 WARN TaskSetManager: Lost task 9.0 in stage 252.0 (TID 12799) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:23:11 WARN TaskSetManager: Lost task 3.0 in stage 252.0 (TID 12793) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:23:11 WARN TaskSetManager: Lost task 15.0 in stage 252.0 (TID 12805) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:23:11 WARN TaskSetManager: Lost task 5.0 in stage 252.0 (TID 12795) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:23:11 WARN TaskSetManager: Lost task 11.0 in stage 252.0 (TID 12801) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:23:11 WARN TaskSetManager: Lost task 13.0 in stage 252.0 (TID 12803) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:23:11 WARN TaskSetManager: Lost task 7.0 in stage 252.0 (TID 12797) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:23:11 WARN TaskSetManager: Lost task 1.0 in stage 252.0 (TID 12791) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757496295740_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 10:23:11.803]Container killed on request. Exit code is 137\n",
      "[2025-09-10 10:23:11.803]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 10:23:11.803]Killed by external signal\n",
      ".\n",
      "25/09/10 10:27:02 WARN DAGScheduler: Broadcasting large task binary with size 1944.6 KiB\n",
      "25/09/10 10:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1895.7 KiB\n",
      "25/09/10 10:27:10 WARN DAGScheduler: Broadcasting large task binary with size 1881.1 KiB\n",
      "25/09/10 10:27:13 WARN DAGScheduler: Broadcasting large task binary with size 1934.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis written: gs://ot-team/jroldan/analysis/2025-09-10_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "\n",
    "# Same schema you had:\n",
    "result_schema = StructType([\n",
    "    StructField(\"group\",        StringType(),  True),\n",
    "    StructField(\"comparison\",   StringType(),  True),\n",
    "    StructField(\"phase\",        StringType(),  True),\n",
    "    StructField(\"oddsRatio\",    DoubleType(),  True),\n",
    "    StructField(\"pValue\",       DoubleType(),  True),\n",
    "    StructField(\"lowerInterval\",DoubleType(),  True),\n",
    "    StructField(\"upperInterval\",DoubleType(),  True),\n",
    "    StructField(\"total\",        StringType(),  True),\n",
    "    StructField(\"values\",       ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"relSuccess\",   DoubleType(),  True),\n",
    "    StructField(\"rsLower\",      DoubleType(),  True),\n",
    "    StructField(\"rsUpper\",      DoubleType(),  True),\n",
    "    StructField(\"path\",         StringType(),  True),\n",
    "])\n",
    "\n",
    "def _relative_success(matrix_2x2: np.ndarray):\n",
    "    a, b = matrix_2x2[0,0], matrix_2x2[0,1]\n",
    "    c, d = matrix_2x2[1,0], matrix_2x2[1,1]\n",
    "    rate_yes = a / (a + b) if (a + b) > 0 else 0.0\n",
    "    rate_no  = c / (c + d) if (c + d) > 0 else 0.0\n",
    "    rs = rate_yes - rate_no\n",
    "    import math\n",
    "    se = 0.0\n",
    "    if (a+b) > 0:\n",
    "        se += rate_yes * (1 - rate_yes) / (a + b)\n",
    "    if (c+d) > 0:\n",
    "        se += rate_no  * (1 - rate_no)  / (c + d)\n",
    "    se = math.sqrt(se)\n",
    "    lo, hi = rs - 1.96*se, rs + 1.96*se\n",
    "    return float(rs), float(lo), float(hi)\n",
    "\n",
    "def fisher_by_group(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # pdf has rows for both comparison=='yes' and comparison=='no' (may be missing one)\n",
    "    sub = pdf[[\"comparison\",\"yes\",\"no\"]].copy()\n",
    "    sub = sub.set_index(\"comparison\").reindex([\"yes\",\"no\"]).fillna(0)\n",
    "    mat = sub[[\"yes\",\"no\"]].to_numpy(dtype=int)\n",
    "\n",
    "    total = int(mat.sum())\n",
    "    or_val, p_val = fisher_exact(mat, alternative=\"two-sided\")\n",
    "    ci = odds_ratio(mat).confidence_interval(0.95)\n",
    "    rs, rs_lo, rs_hi = _relative_success(mat)\n",
    "\n",
    "    out = pd.DataFrame([{\n",
    "        \"group\":        pdf[\"metric\"].iloc[0],\n",
    "        \"comparison\":   f\"{pdf['value'].iloc[0]}_only\",\n",
    "        \"phase\":        pdf[\"phase_name\"].iloc[0],\n",
    "        \"oddsRatio\":    round(float(or_val), 2),\n",
    "        \"pValue\":       float(p_val),\n",
    "        \"lowerInterval\":round(float(ci[0]), 2),\n",
    "        \"upperInterval\":round(float(ci[1]), 2),\n",
    "        \"total\":        str(total),\n",
    "        \"values\":       mat.tolist(),\n",
    "        \"relSuccess\":   round(float(rs), 2),\n",
    "        \"rsLower\":      round(float(rs_lo), 2),\n",
    "        \"rsUpper\":      round(float(rs_hi), 2),\n",
    "        \"path\":         \"\",\n",
    "    }])\n",
    "    return out\n",
    "\n",
    "# (optional) Arrow for speed\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# ✅ Use applyInPandas with the explicit schema\n",
    "results_df = (\n",
    "    mat_counts\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .applyInPandas(fisher_by_group, schema=result_schema)\n",
    ")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Fisher + reporting (applyInPandas version)\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    ")\n",
    "from datetime import date\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "\n",
    "# 0) Ensure 'yes'/'no' numeric columns exist and are ints (no NaN)\n",
    "mat_counts = (\n",
    "    mat_counts\n",
    "    .fillna(0)\n",
    "    .withColumn(\"yes\", F.coalesce(F.col(\"yes\"), F.lit(0)).cast(\"int\"))\n",
    "    .withColumn(\"no\",  F.coalesce(F.col(\"no\"),  F.lit(0)).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# 1) Output schema (mirrors your previous one)\n",
    "result_schema = StructType([\n",
    "    StructField(\"group\",        StringType(),  True),  # metric\n",
    "    StructField(\"comparison\",   StringType(),  True),  # e.g. \"<value>_only\"\n",
    "    StructField(\"phase\",        StringType(),  True),  # phase_name\n",
    "    StructField(\"oddsRatio\",    DoubleType(),  True),\n",
    "    StructField(\"pValue\",       DoubleType(),  True),\n",
    "    StructField(\"lowerInterval\",DoubleType(),  True),\n",
    "    StructField(\"upperInterval\",DoubleType(),  True),\n",
    "    StructField(\"total\",        StringType(),  True),\n",
    "    StructField(\"values\",       ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"relSuccess\",   DoubleType(),  True),\n",
    "    StructField(\"rsLower\",      DoubleType(),  True),\n",
    "    StructField(\"rsUpper\",      DoubleType(),  True),\n",
    "    StructField(\"path\",         StringType(),  True),\n",
    "])\n",
    "\n",
    "# 2) Relative success helper (vectorized inside pandas func)\n",
    "def _relative_success(matrix_2x2: np.ndarray):\n",
    "    # rows: comparison yes/no ; cols: prediction yes/no\n",
    "    a, b = matrix_2x2[0,0], matrix_2x2[0,1]\n",
    "    c, d = matrix_2x2[1,0], matrix_2x2[1,1]\n",
    "    rate_yes = a / (a + b) if (a + b) > 0 else 0.0\n",
    "    rate_no  = c / (c + d) if (c + d) > 0 else 0.0\n",
    "    rs = rate_yes - rate_no\n",
    "    import math\n",
    "    se = 0.0\n",
    "    if (a+b) > 0:\n",
    "        se += rate_yes * (1 - rate_yes) / (a + b)\n",
    "    if (c+d) > 0:\n",
    "        se += rate_no  * (1 - rate_no)  / (c + d)\n",
    "    se = math.sqrt(se)\n",
    "    lo, hi = rs - 1.96*se, rs + 1.96*se\n",
    "    return float(rs), float(lo), float(hi)\n",
    "\n",
    "# 3) Plain Python function for applyInPandas (no decorator)\n",
    "def fisher_by_group(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # pdf columns: metric, feature, value, phase_name, comparison, yes, no\n",
    "    sub = pdf[[\"comparison\",\"yes\",\"no\"]].copy()\n",
    "    # enforce both rows 'yes' and 'no'\n",
    "    sub = sub.set_index(\"comparison\").reindex([\"yes\",\"no\"]).fillna(0)\n",
    "    mat = sub[[\"yes\",\"no\"]].to_numpy(dtype=int)\n",
    "\n",
    "    total = int(mat.sum())\n",
    "    or_val, p_val = fisher_exact(mat, alternative=\"two-sided\")\n",
    "    ci = odds_ratio(mat).confidence_interval(0.95)\n",
    "    rs, rs_lo, rs_hi = _relative_success(mat)\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"group\":        pdf[\"metric\"].iloc[0],\n",
    "        \"comparison\":   f\"{pdf['value'].iloc[0]}_only\",\n",
    "        \"phase\":        pdf[\"phase_name\"].iloc[0],\n",
    "        \"oddsRatio\":    round(float(or_val), 2),\n",
    "        \"pValue\":       float(p_val),\n",
    "        \"lowerInterval\":round(float(ci[0]), 2),\n",
    "        \"upperInterval\":round(float(ci[1]), 2),\n",
    "        \"total\":        str(total),\n",
    "        \"values\":       mat.tolist(),\n",
    "        \"relSuccess\":   round(float(rs), 2),\n",
    "        \"rsLower\":      round(float(rs_lo), 2),\n",
    "        \"rsUpper\":      round(float(rs_hi), 2),\n",
    "        \"path\":         \"\",\n",
    "    }])\n",
    "\n",
    "# (optional) Arrow for speed\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# 4) Apply per (metric, feature, value, phase_name)\n",
    "results_df = (\n",
    "    mat_counts\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .applyInPandas(fisher_by_group, schema=result_schema)\n",
    ")\n",
    "\n",
    "# 5) Formatting + annotation + CSV export (unchanged logic)\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# disdic from agg_once, if not already present\n",
    "disdic = {r[\"value\"]: r[\"feature\"] for r in agg_once.select(\"feature\",\"value\").distinct().collect()}\n",
    "\n",
    "patterns = [\"_only\", \"_isRightTissueSignalAgreed\"]\n",
    "regex_pattern = \"(\" + \"|\".join(patterns) + \")\"\n",
    "\n",
    "df_fmt = (\n",
    "    spreadSheetFormatter(results_df)\n",
    "    .withColumn(\"prefix\", F.regexp_replace(F.col(\"comparison\"), regex_pattern + \".*\", \"\"))\n",
    "    .withColumn(\"suffix\", F.regexp_extract(F.col(\"comparison\"), regex_pattern, 0))\n",
    ")\n",
    "\n",
    "mapping_expr = create_map([F.lit(x) for x in chain(*disdic.items())])\n",
    "df_annot = df_fmt.withColumn(\"annotation\", mapping_expr.getItem(F.col(\"prefix\")))\n",
    "\n",
    "today_date = date.today().isoformat()\n",
    "out_csv = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue.csv\"\n",
    "df_annot.toPandas().to_csv(out_csv, index=False)\n",
    "print(f\"Analysis written: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 145\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame([row])\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Apply the grouped map per (metric, feature, value, phase)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m results_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    143\u001b[0m     \u001b[43mmat_counts\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetric\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphase_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfisher_by_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# ---- 4) Optional spreadsheet formatting + annotate and export ----\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chain\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/pandas/group_ops.py:100\u001b[0m, in \u001b[0;36mPandasGroupedOpsMixin.apply\u001b[0;34m(self, udf)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Columns are special because hasattr always return True\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(udf, Column)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(udf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m     99\u001b[0m ):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid udf: the udf argument must be a pandas_udf of type \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROUPED_MAP.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    104\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is preferred to use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplyInPandas\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m over this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI. This API will be deprecated in the future releases. See SPARK-28264 for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore details.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapplyInPandas(udf\u001b[38;5;241m.\u001b[39mfunc, schema\u001b[38;5;241m=\u001b[39mudf\u001b[38;5;241m.\u001b[39mreturnType)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP."
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Spark-friendly ANALYSIS\n",
    "# ============================\n",
    "from datetime import date\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    ")\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# ---- 1) Build the comparison × prediction long table in one go ----\n",
    "# metrics we want to analyze (these replace columns_to_aggregate)\n",
    "metric_flags = [\"NoneCellYes_flag\", \"NdiagonalYes_flag\", \"hasGenetics_flag\"]\n",
    "\n",
    "# Melt the phase flags into (phase_name, prediction) rows\n",
    "phases_long = (\n",
    "    agg_once.select(\n",
    "        \"targetId\",\"diseaseId\",\"maxClinPhase\",\"feature\",\"value\",\n",
    "        F.expr(\"stack(5, \"\n",
    "               \"'Phase>=4', `Phase>=4`, \"\n",
    "               \"'Phase>=3', `Phase>=3`, \"\n",
    "               \"'Phase>=2', `Phase>=2`, \"\n",
    "               \"'Phase>=1', `Phase>=1`, \"\n",
    "               \"'PhaseT',  `PhaseT`\"\n",
    "               \")\").alias(\"phase_name\", \"prediction\")\n",
    "    )\n",
    "    .filter(F.col(\"prediction\").isNotNull())\n",
    ")\n",
    "\n",
    "# For each metric, attach its comparison flag (\"yes\"/\"no\") and union them all\n",
    "def attach_metric(metric_col: str):\n",
    "    return (\n",
    "        agg_once.select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"feature\",\"value\", F.col(metric_col).alias(\"comparison\"))\n",
    "                .join(phases_long.select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"feature\",\"value\",\"phase_name\",\"prediction\"),\n",
    "                      on=[\"targetId\",\"diseaseId\",\"maxClinPhase\",\"feature\",\"value\"],\n",
    "                      how=\"inner\")\n",
    "                .withColumn(\"metric\", F.lit(metric_col.replace(\"_flag\", \"\")))  # pretty label\n",
    "    )\n",
    "\n",
    "analysis_long = attach_metric(metric_flags[0])\n",
    "for mc in metric_flags[1:]:\n",
    "    analysis_long = analysis_long.unionByName(attach_metric(mc))\n",
    "\n",
    "# Now analysis_long has rows like:\n",
    "# (targetId, diseaseId, feature, value, metric, phase_name, comparison='yes'/'no', prediction='yes'/'no')\n",
    "\n",
    "# ---- 2) Count ALL 2×2 cells in one aggregation ----\n",
    "cell_counts = (\n",
    "    analysis_long\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\",\"comparison\",\"prediction\")\n",
    "    .agg(F.count(\"*\").alias(\"a\"))\n",
    ")\n",
    "\n",
    "# ---- 3) Reshape to 2×2 matrices per (metric, feature, value, phase) ----\n",
    "# Create a compact 2-row frame with columns yes/no for prediction\n",
    "mat_counts = (\n",
    "    cell_counts\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\",\"comparison\")\n",
    "    .pivot(\"prediction\", [\"yes\",\"no\"])\n",
    "    .agg(F.first(\"a\"))\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# We'll compute Fisher per group using a grouped map Pandas UDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Output schema mirrors your previous 'schema'\n",
    "result_schema = StructType([\n",
    "    StructField(\"group\",        StringType(),  True),  # metric name\n",
    "    StructField(\"comparison\",   StringType(),  True),  # e.g., \"<value>_only\"\n",
    "    StructField(\"phase\",        StringType(),  True),  # phase_name\n",
    "    StructField(\"oddsRatio\",    DoubleType(),  True),\n",
    "    StructField(\"pValue\",       DoubleType(),  True),\n",
    "    StructField(\"lowerInterval\",DoubleType(),  True),\n",
    "    StructField(\"upperInterval\",DoubleType(),  True),\n",
    "    StructField(\"total\",        StringType(),  True),\n",
    "    StructField(\"values\",       ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"relSuccess\",   DoubleType(),  True),\n",
    "    StructField(\"rsLower\",      DoubleType(),  True),\n",
    "    StructField(\"rsUpper\",      DoubleType(),  True),\n",
    "    StructField(\"path\",         StringType(),  True),\n",
    "])\n",
    "\n",
    "# Relative success helper – reusing your function on driver won’t vectorize,\n",
    "# so we replicate a simple version here. If you must use your exact math,\n",
    "# import it and call it inside the pandas UDF.\n",
    "def _relative_success(matrix_2x2: np.ndarray):\n",
    "    # matrix rows: comparison yes/no; cols: prediction yes/no\n",
    "    # success rate in comparison==yes vs comparison==no\n",
    "    a, b = matrix_2x2[0,0], matrix_2x2[0,1]\n",
    "    c, d = matrix_2x2[1,0], matrix_2x2[1,1]\n",
    "    rate_yes = a / (a + b) if (a + b) > 0 else 0.0\n",
    "    rate_no  = c / (c + d) if (c + d) > 0 else 0.0\n",
    "    rs = rate_yes - rate_no\n",
    "    # crude Wald CI for difference in proportions (you can swap with your exact function)\n",
    "    import math\n",
    "    se = 0.0\n",
    "    if (a+b) > 0:\n",
    "        se += rate_yes * (1 - rate_yes) / (a + b)\n",
    "    if (c+d) > 0:\n",
    "        se += rate_no  * (1 - rate_no)  / (c + d)\n",
    "    se = math.sqrt(se)\n",
    "    lo, hi = rs - 1.96*se, rs + 1.96*se\n",
    "    return float(rs), float(lo), float(hi)\n",
    "\n",
    "@pandas_udf(result_schema)\n",
    "def fisher_by_group(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # pdf has columns: metric, feature, value, phase_name, comparison, yes, no\n",
    "    # build 2x2 with rows ordered comparison=['yes','no'] and cols ['yes','no']\n",
    "    # Ensure both rows exist; fill missing with zeros\n",
    "    sub = pdf[[\"comparison\",\"yes\",\"no\"]].copy()\n",
    "    sub = sub.set_index(\"comparison\").reindex([\"yes\",\"no\"]).fillna(0)\n",
    "    mat = sub[[\"yes\",\"no\"]].to_numpy(dtype=int)\n",
    "\n",
    "    total = int(mat.sum())\n",
    "    or_val, p_val = fisher_exact(mat, alternative=\"two-sided\")\n",
    "    ci = odds_ratio(mat).confidence_interval(0.95)\n",
    "    rs, rs_lo, rs_hi = _relative_success(mat)\n",
    "\n",
    "    row = {\n",
    "        \"group\":        pdf[\"metric\"].iloc[0],\n",
    "        \"comparison\":   f\"{pdf['value'].iloc[0]}_only\",  # to match your previous naming\n",
    "        \"phase\":        pdf[\"phase_name\"].iloc[0],\n",
    "        \"oddsRatio\":    float(round(or_val, 2)),\n",
    "        \"pValue\":       float(p_val),\n",
    "        \"lowerInterval\":float(round(ci[0], 2)),\n",
    "        \"upperInterval\":float(round(ci[1], 2)),\n",
    "        \"total\":        str(total),\n",
    "        \"values\":       mat.tolist(),\n",
    "        \"relSuccess\":   float(round(rs, 2)),\n",
    "        \"rsLower\":      float(round(rs_lo, 2)),\n",
    "        \"rsUpper\":      float(round(rs_hi, 2)),\n",
    "        \"path\":         \"\",   # you can fill a path pattern here if you still write per-combo parquet\n",
    "    }\n",
    "    return pd.DataFrame([row])\n",
    "\n",
    "# Apply the grouped map per (metric, feature, value, phase)\n",
    "results_df = (\n",
    "    mat_counts\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .apply(fisher_by_group)\n",
    ")\n",
    "\n",
    "# ---- 4) Optional spreadsheet formatting + annotate and export ----\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# Your disdic: {value -> feature}; rebuild safely from agg_once if not present\n",
    "disdic = {r[\"value\"]: r[\"feature\"] for r in agg_once.select(\"feature\",\"value\").distinct().collect()}\n",
    "\n",
    "# If you still want to keep the regex-based prefix/suffix (works with our \"value_only\" naming):\n",
    "patterns = [\"_only\", \"_isRightTissueSignalAgreed\"]\n",
    "regex_pattern = \"(\" + \"|\".join(patterns) + \")\"\n",
    "\n",
    "df_fmt = (\n",
    "    spreadSheetFormatter(results_df)  # reuse your helper\n",
    "    .withColumn(\"prefix\", F.regexp_replace(F.col(\"comparison\"), regex_pattern + \".*\", \"\"))\n",
    "    .withColumn(\"suffix\", F.regexp_extract(F.col(\"comparison\"), regex_pattern, 0))\n",
    ")\n",
    "\n",
    "mapping_expr = create_map([F.lit(x) for x in chain(*disdic.items())])\n",
    "df_annot = df_fmt.withColumn(\"annotation\", mapping_expr.getItem(F.col(\"prefix\")))\n",
    "\n",
    "today_date = date.today().isoformat()\n",
    "out_csv = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue.csv\"\n",
    "df_annot.toPandas().to_csv(out_csv, index=False)\n",
    "print(f\"Analysis written: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark session created at 2025-09-17 14:54:25.328103\n",
      "Analysis started on 2025-09-17 at  2025-09-17 14:54:25.328103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 14:54:27 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/09/17 14:54:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created with:\n",
      "  spark.driver.memory: 12g\n",
      "  spark.executor.memory: 40g\n",
      "  spark.executor.cores: 10\n",
      "  spark.executor.instances: 1\n",
      "  spark.yarn.executor.memoryOverhead: 6g\n",
      "  spark.sql.shuffle.partitions: 128\n",
      "  spark.default.parallelism: 128\n",
      "  spark.sql.adaptive.enabled: true\n",
      "  spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "Spark UI: http://jr-temp-doe-m.c.open-targets-eu-dev.internal:42205\n",
      "Loaded all base tables.\n",
      "Built newColoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded gwasComplete\n",
      "Built gwasComplete\n",
      "Built resolvedColoc\n",
      "Built temporary DoE datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built analysis_chembl_indication\n",
      "[info] agg_once not found — rebuilding it from test2/benchmark …\n",
      "[info] agg_once rebuilt.\n",
      "universe of pairs and phase flags built\n",
      "phase_universe_long built\n",
      "phase_universe_long built\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing functions\n",
      "imported functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'today_date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 668\u001b[0m\n\u001b[1;32m    662\u001b[0m df_annot \u001b[38;5;241m=\u001b[39m df_fmt\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m\"\u001b[39m, mapping_expr\u001b[38;5;241m.\u001b[39mgetItem(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m#today_date = date.today().isoformat()\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m#out_csv = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try.csv\"\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m#df_annot.toPandas().to_csv(out_csv, index=False)\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m#print(f\"Analysis written: {out_csv}\")\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m (out_path, coalesce_n) \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://ot-team/jroldan/analysis/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtoday_date\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreparing df_annot to write\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    670\u001b[0m (df_annot\n\u001b[1;32m    671\u001b[0m   \u001b[38;5;241m.\u001b[39mcoalesce(coalesce_n)                 \u001b[38;5;66;03m# 1 file if feasible; increase if OOM on shuffle\u001b[39;00m\n\u001b[1;32m    672\u001b[0m   \u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    673\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    674\u001b[0m   \u001b[38;5;241m.\u001b[39mcsv(out_path))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'today_date' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Single-script, loop-free PySpark job (tall/unpivot + single aggregation)\n",
    "\n",
    "import os\n",
    "from datetime import date\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    ")\n",
    "\n",
    "# Your helpers\n",
    "from functions import (\n",
    "    relative_success,\n",
    "    spreadSheetFormatter,\n",
    "    discrepancifier,\n",
    "    temporary_directionOfEffect,\n",
    "    buildColocData,\n",
    "    gwasDataset,\n",
    ")\n",
    "from DoEAssessment import directionOfEffect  # noqa: F401  (kept if you need it later)\n",
    "\n",
    "# -------------------------------\n",
    "# Spark / YARN resource settings (Single-Node Option A)\n",
    "# -------------------------------\n",
    "driver_memory = \"12g\"                 # string with unit\n",
    "executor_memory = \"40g\"               # string with unit (heap)\n",
    "executor_cores = 10                   # int\n",
    "num_executors = 1                     # int (one fat executor on single node)\n",
    "executor_memory_overhead = \"6g\"       # string with unit (PySpark/Arrow/off-heap)\n",
    "shuffle_partitions = 128              # int (~2–3x cores)\n",
    "default_parallelism = 128             # int (match shuffle_partitions)\n",
    "\n",
    "# If you later move to a multi-worker cluster, replace the values above.\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MyOptimizedPySparkApp\")\n",
    "    .config(\"spark.master\", \"yarn\")\n",
    "    # core resources\n",
    "    .config(\"spark.driver.memory\", driver_memory)\n",
    "    .config(\"spark.executor.memory\", executor_memory)\n",
    "    .config(\"spark.executor.cores\", executor_cores)\n",
    "    .config(\"spark.executor.instances\", num_executors)\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead)\n",
    "    # shuffle & parallelism\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions)\n",
    "    .config(\"spark.default.parallelism\", default_parallelism)\n",
    "    # adaptive query execution for better skew/partition sizing\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession created with:\")\n",
    "for k in [\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.executor.cores\",\n",
    "    \"spark.executor.instances\",\n",
    "    \"spark.yarn.executor.memoryOverhead\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.default.parallelism\",\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "]:\n",
    "    print(f\"  {k}: {spark.conf.get(k)}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# -------------------------------\n",
    "# Spark / YARN resource settings\n",
    "# -------------------------------\n",
    "driver_memory = \"16g\"\n",
    "executor_memory = \"32g\"\n",
    "executor_cores = \"8\"\n",
    "num_executors = \"16\"\n",
    "executor_memory_overhead = \"8g\"\n",
    "shuffle_partitions = \"150\"\n",
    "default_parallelism = str(int(executor_cores) * int(num_executors) * 2)  # 80\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MyOptimizedPySparkApp\")\n",
    "    .config(\"spark.master\", \"yarn\")\n",
    "    .config(\"spark.driver.memory\", driver_memory)\n",
    "    .config(\"spark.executor.memory\", executor_memory)\n",
    "    .config(\"spark.executor.cores\", executor_cores)\n",
    "    .config(\"spark.executor.instances\", num_executors)\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead)\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions)\n",
    "    .config(\"spark.default.parallelism\", default_parallelism)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession created with:\")\n",
    "for k in [\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.executor.cores\",\n",
    "    \"spark.executor.instances\",\n",
    "    \"spark.yarn.executor.memoryOverhead\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.default.parallelism\",\n",
    "]:\n",
    "    print(f\"  {k}: {spark.conf.get(k)}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "'''\n",
    "# --------------------------------\n",
    "# 0) Load inputs\n",
    "# --------------------------------\n",
    "path_n = \"gs://open-targets-data-releases/25.06/output/\"\n",
    "\n",
    "target = spark.read.parquet(f\"{path_n}target/\")\n",
    "diseases = spark.read.parquet(f\"{path_n}disease/\")\n",
    "evidences = spark.read.parquet(f\"{path_n}evidence\")\n",
    "credible = spark.read.parquet(f\"{path_n}credible_set\")\n",
    "new = spark.read.parquet(f\"{path_n}colocalisation_coloc\")\n",
    "index = spark.read.parquet(f\"{path_n}study/\")\n",
    "variantIndex = spark.read.parquet(f\"{path_n}variant\")\n",
    "biosample = spark.read.parquet(f\"{path_n}biosample\")\n",
    "ecaviar = spark.read.parquet(f\"{path_n}colocalisation_ecaviar\")\n",
    "all_coloc = ecaviar.unionByName(new, allowMissingColumns=True)\n",
    "print(\"Loaded all base tables.\")\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build coloc + GWAS dataset\n",
    "# --------------------------------\n",
    "newColoc = buildColocData(all_coloc, credible, index)\n",
    "print(\"Built newColoc\")\n",
    "\n",
    "gwasComplete = gwasDataset(evidences, credible)\n",
    "print(\"Built gwasComplete\")\n",
    "\n",
    "resolvedColoc = (\n",
    "    newColoc.withColumnRenamed(\"geneId\", \"targetId\")\n",
    "    .join(\n",
    "        gwasComplete.withColumnRenamed(\"studyLocusId\", \"leftStudyLocusId\"),\n",
    "        on=[\"leftStudyLocusId\", \"targetId\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .join(\n",
    "        diseases.selectExpr(\"id as diseaseId\", \"name\", \"parents\", \"therapeuticAreas\"),\n",
    "        on=\"diseaseId\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"diseaseId\",\n",
    "        F.explode_outer(F.concat(F.array(F.col(\"diseaseId\")), F.col(\"parents\"))),\n",
    "    )\n",
    "    .drop(\"parents\", \"oldDiseaseId\")\n",
    "    .withColumn(\n",
    "        \"colocDoE\",\n",
    "        F.when(\n",
    "            F.col(\"rightStudyType\").isin([\"eqtl\", \"pqtl\", \"tuqtl\", \"sceqtl\", \"sctuqtl\"]),\n",
    "            F.when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"GoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"LoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"LoF_protect\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"GoF_protect\"))\n",
    "        ).when(\n",
    "            F.col(\"rightStudyType\").isin([\"sqtl\", \"scsqtl\"]),\n",
    "            F.when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"LoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"GoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"GoF_protect\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"LoF_protect\"))\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "print(\"Built resolvedColoc\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Direction of Effect & ChEMBL indication\n",
    "# --------------------------------\n",
    "datasource_filter = [\n",
    "    \"gwas_credible_sets\",\n",
    "    \"gene_burden\",\n",
    "    \"eva\",\n",
    "    \"eva_somatic\",\n",
    "    \"gene2phenotype\",\n",
    "    \"orphanet\",\n",
    "    \"cancer_gene_census\",\n",
    "    \"intogen\",\n",
    "    \"impc\",\n",
    "    \"chembl\",\n",
    "]\n",
    "assessment, evidences, actionType_unused, oncolabel_unused = temporary_directionOfEffect(path_n, datasource_filter)\n",
    "print(\"Built temporary DoE datasets\")\n",
    "\n",
    "# (Optional) Add MoA to ChEMBL paths as in your later code\n",
    "mecact_path = f\"{path_n}drug_mechanism_of_action/\"\n",
    "mecact = spark.read.parquet(mecact_path)\n",
    "actionType = (\n",
    "    mecact.select(\n",
    "        F.explode_outer(\"chemblIds\").alias(\"drugId\"),\n",
    "        \"actionType\",\n",
    "        \"mechanismOfAction\",\n",
    "        \"targets\",\n",
    "    )\n",
    "    .select(\n",
    "        F.explode_outer(\"targets\").alias(\"targetId\"),\n",
    "        \"drugId\",\n",
    "        \"actionType\",\n",
    "        \"mechanismOfAction\",\n",
    "    )\n",
    "    .groupBy(\"targetId\", \"drugId\")\n",
    "    .agg(F.collect_set(\"actionType\").alias(\"actionType2\"))\n",
    "    .withColumn(\"nMoA\", F.size(F.col(\"actionType2\")))\n",
    ")\n",
    "\n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "        .join(actionType, on=[\"targetId\", \"drugId\"], how=\"left\")\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(\"clinicalPhase\").over(Window.partitionBy(\"targetId\", \"diseaseId\")),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\", \"actionType2\")\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    .drop(\"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\")\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    ")\n",
    "print(\"Built analysis_chembl_indication\")\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Benchmark (filtered coloc) + clinical phase flags\n",
    "# --------------------------------\n",
    "resolvedColocFiltered = resolvedColoc.filter((F.col(\"clpp\") >= 0.01) | (F.col(\"h4\") >= 0.8))\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\").count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\")).drop(\"count\")\n",
    ")\n",
    "benchmark = (\n",
    "    resolvedColocFiltered.filter(F.col(\"name\") != \"COVID-19\")\n",
    "    .join(analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\")\n",
    "    .withColumn(\n",
    "        \"AgreeDrug\",\n",
    "        F.when((F.col(\"drugGoF_protect\").isNotNull()) & (F.col(\"colocDoE\") == \"GoF_protect\"), \"yes\")\n",
    "        .when((F.col(\"drugLoF_protect\").isNotNull()) & (F.col(\"colocDoE\") == \"LoF_protect\"), \"yes\")\n",
    "        .otherwise(\"no\"),\n",
    "    )\n",
    "    .join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    ")\n",
    "\n",
    "benchmark = (\n",
    "    benchmark.join(F.broadcast(negativeTD), on=[\"targetId\", \"diseaseId\"], how=\"left\")\n",
    "    .withColumn(\"PhaseT\", F.when(F.col(\"stopReason\") == \"Negative\", \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=4\", F.when((F.col(\"maxClinPhase\") == 4) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=3\", F.when((F.col(\"maxClinPhase\") >= 3) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=2\", F.when((F.col(\"maxClinPhase\") >= 2) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=1\", F.when((F.col(\"maxClinPhase\") >= 1) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Replace nested loops:\n",
    "#     compute DoE counts once → derive flags → unpivot → single aggregation\n",
    "# --------------------------------\n",
    "doe_cols = [\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "\n",
    "# counts per colocDoE over the grouping you previously used in the loop\n",
    "group_keys = [\n",
    "    \"targetId\", \"diseaseId\", \"maxClinPhase\",\n",
    "    \"actionType2\", \"biosampleName\", \"projectId\", \"rightStudyType\", \"colocalisationMethod\"\n",
    "]\n",
    "\n",
    "doe_counts = (\n",
    "    benchmark.groupBy(*group_keys)\n",
    "    .agg(*[F.sum(F.when(F.col(\"colocDoE\") == c, 1).otherwise(0)).alias(c) for c in doe_cols])\n",
    ")\n",
    "\n",
    "# max name(s) (in case of ties) without arrays of structs\n",
    "greatest_count = F.greatest(*[F.col(c) for c in doe_cols])\n",
    "max_names = F.filter(\n",
    "    F.array(*[F.when(F.col(c) == greatest_count, F.lit(c)) for c in doe_cols]),\n",
    "    lambda x: x.isNotNull()\n",
    ")\n",
    "\n",
    "# presence of drug-side signals (equivalent to *_ch presence in your loop path)\n",
    "has_lof_ch = F.col(\"drugLoF_protect\").isNotNull()\n",
    "has_gof_ch = F.col(\"drugGoF_protect\").isNotNull()\n",
    "\n",
    "test2 = (\n",
    "    benchmark.select(*group_keys, \"drugLoF_protect\", \"drugGoF_protect\")\n",
    "    .join(doe_counts, on=group_keys, how=\"left\")\n",
    "    .withColumn(\"NoneCellYes\",\n",
    "        F.when(has_lof_ch & (~has_gof_ch) & F.array_contains(max_names, F.lit(\"LoF_protect\")), \"yes\")\n",
    "         .when(has_gof_ch & (~has_lof_ch) & F.array_contains(max_names, F.lit(\"GoF_protect\")), \"yes\")\n",
    "         .otherwise(\"no\")\n",
    "    )\n",
    "    .withColumn(\"NdiagonalYes\",\n",
    "        F.when(has_lof_ch & (~has_gof_ch) & (F.array_contains(max_names, F.lit(\"LoF_protect\")) | F.array_contains(max_names, F.lit(\"GoF_risk\"))), \"yes\")\n",
    "         .when(has_gof_ch & (~has_lof_ch) & (F.array_contains(max_names, F.lit(\"GoF_protect\")) | F.array_contains(max_names, F.lit(\"LoF_risk\"))), \"yes\")\n",
    "         .otherwise(\"no\")\n",
    "    )\n",
    "    .withColumn(\"drugCoherency\",\n",
    "        F.when(has_lof_ch & ~has_gof_ch, \"coherent\")\n",
    "         .when(~has_lof_ch & has_gof_ch, \"coherent\")\n",
    "         .when(has_lof_ch & has_gof_ch, \"dispar\")\n",
    "         .otherwise(\"other\")\n",
    "    ).withColumn(\n",
    "    \"hasGenetics2\",\n",
    "    F.when(\n",
    "        reduce(lambda acc, c: acc & F.col(c).isNull(), doe_cols[1:], F.col(doe_cols[0]).isNull()),\n",
    "        F.lit(\"no\")\n",
    "    ).otherwise(F.lit(\"yes\"))\n",
    ")\n",
    "    .withColumn(\"hasGenetics\", F.when(F.col(\"NdiagonalYes\").isNotNull(), \"yes\").otherwise(\"no\")) #### we have to change it\n",
    ")\n",
    "test2.persist()\n",
    "\n",
    "# ---------- Guard: (re)build agg_once if not defined ----------\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def _build_agg_once_from_test2_and_benchmark(test2, benchmark_df):\n",
    "    # Columns we keep across all longified slices\n",
    "    common_cols = [\n",
    "        \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "        \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\",\n",
    "        \"NoneCellYes\",\"NdiagonalYes\",\"hasGenetics2\"  # note: hasGenetics2 from your test2\n",
    "    ]\n",
    "\n",
    "    # Join phase flags once\n",
    "    phase_flags = (\n",
    "        benchmark_df.select(\n",
    "            \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "            \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\"\n",
    "        ).dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"])\n",
    "    )\n",
    "\n",
    "    t2_with_phase = test2.join(\n",
    "        phase_flags, on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    # actionType2 is ARRAY<STRING> → explode\n",
    "    long_action = (\n",
    "        t2_with_phase\n",
    "        .select(*common_cols, F.explode_outer(\"actionType2\").alias(\"value\"))\n",
    "        .withColumn(\"feature\", F.lit(\"actionType2\"))\n",
    "        .select(*common_cols, \"feature\", \"value\")\n",
    "    )\n",
    "\n",
    "    # helper for scalar columns\n",
    "    def longify_scalar(colname: str):\n",
    "        return (\n",
    "            t2_with_phase\n",
    "            .select(*common_cols, F.col(colname).alias(\"value\"))\n",
    "            .withColumn(\"feature\", F.lit(colname))\n",
    "            .select(*common_cols, \"feature\", \"value\")\n",
    "        )\n",
    "\n",
    "    long_biosample = longify_scalar(\"biosampleName\")\n",
    "    long_project   = longify_scalar(\"projectId\")\n",
    "    long_rstype    = longify_scalar(\"rightStudyType\")\n",
    "    long_colocm    = longify_scalar(\"colocalisationMethod\")\n",
    "\n",
    "    # union into one tall table\n",
    "    long_features = (\n",
    "        long_action\n",
    "        .unionByName(long_biosample)\n",
    "        .unionByName(long_project)\n",
    "        .unionByName(long_rstype)\n",
    "        .unionByName(long_colocm)\n",
    "    ).filter(F.col(\"value\").isNotNull())\n",
    "\n",
    "    # single aggregation to compute flags\n",
    "    agg_once_local = (\n",
    "        long_features\n",
    "        .groupBy(\n",
    "            \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "            \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\",\n",
    "            \"feature\",\"value\"\n",
    "        )\n",
    "        .agg(\n",
    "            F.max(F.when(F.col(\"NoneCellYes\")==\"yes\", 1).otherwise(0)).alias(\"NoneCellYes\"),\n",
    "            F.max(F.when(F.col(\"NdiagonalYes\")==\"yes\", 1).otherwise(0)).alias(\"NdiagonalYes\"),\n",
    "            F.max(F.when(F.col(\"hasGenetics2\")==\"yes\", 1).otherwise(0)).alias(\"hasGenetics\"),\n",
    "        )\n",
    "        .selectExpr(\n",
    "            \"*\",\n",
    "            \"CASE WHEN NoneCellYes=1 THEN 'yes' ELSE 'no' END as NoneCellYes_flag\",\n",
    "            \"CASE WHEN NdiagonalYes=1 THEN 'yes' ELSE 'no' END as NdiagonalYes_flag\",\n",
    "            \"CASE WHEN hasGenetics=1 THEN 'yes' ELSE 'no' END as hasGenetics_flag\"\n",
    "        )\n",
    "    )\n",
    "    return agg_once_local\n",
    "\n",
    "if 'agg_once' not in globals():\n",
    "    print(\"[info] agg_once not found — rebuilding it from test2/benchmark …\")\n",
    "    agg_once = _build_agg_once_from_test2_and_benchmark(test2, benchmark)\n",
    "    print(\"[info] agg_once rebuilt.\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Denominator = ALL pairs in analysis_chembl_indication (deduped)\n",
    "# Build 2x2 counts using totals, then Fisher via applyInPandas\n",
    "# ============================\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    ")\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "\n",
    "# ---- 0) Universe of pairs & phase flags (only de-dup, no other filtering)\n",
    "universe = (\n",
    "    analysis_chembl_indication\n",
    "    .select(\"targetId\", \"diseaseId\", \"maxClinPhase\")  # dedupe on these\n",
    "    .distinct()\n",
    "    .join(F.broadcast(negativeTD), on=[\"targetId\",\"diseaseId\"], how=\"left\")\n",
    "    .withColumn(\"PhaseT\", F.when(F.col(\"stopReason\")==\"Negative\", \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=4\", F.when((F.col(\"maxClinPhase\")==4) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=3\", F.when((F.col(\"maxClinPhase\")>=3) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=2\", F.when((F.col(\"maxClinPhase\")>=2) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=1\", F.when((F.col(\"maxClinPhase\")>=1) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    ")\n",
    "print('universe of pairs and phase flags built')\n",
    "\n",
    "# Long view of phase flags for universe\n",
    "phases_universe_long = universe.select(\n",
    "    \"targetId\",\"diseaseId\",\n",
    "    F.expr(\"stack(5, \"\n",
    "           \"'Phase>=4', `Phase>=4`, \"\n",
    "           \"'Phase>=3', `Phase>=3`, \"\n",
    "           \"'Phase>=2', `Phase>=2`, \"\n",
    "           \"'Phase>=1', `Phase>=1`, \"\n",
    "           \"'PhaseT',  `PhaseT`\"\n",
    "           \")\").alias(\"phase_name\",\"prediction\")\n",
    ")\n",
    "print('phase_universe_long built')\n",
    "\n",
    "# Totals per phase (denominator totals)\n",
    "total_pairs_by_phase = (\n",
    "    phases_universe_long\n",
    "    .groupBy(\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"total_pairs\"))\n",
    ")\n",
    "total_pred_yes_by_phase = (\n",
    "    phases_universe_long\n",
    "    .filter(F.col(\"prediction\")==\"yes\")\n",
    "    .groupBy(\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"total_pred_yes\"))\n",
    ")\n",
    "print('phase_universe_long built')\n",
    "\n",
    "# ---- 1) Build analysis_long from agg_once (flags) + phases (prediction)\n",
    "# metrics we’ll analyze\n",
    "metric_flags = [\"NoneCellYes_flag\", \"NdiagonalYes_flag\", \"hasGenetics_flag\"]\n",
    "\n",
    "# phase flags per (target,disease,maxClinPhase)\n",
    "phase_flags = (\n",
    "    benchmark.select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\")\n",
    "    .dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"])\n",
    ")\n",
    "\n",
    "# stack phases for the records present in agg_once (feature,value specific)\n",
    "phases_long_for_records = (\n",
    "    phase_flags.join(agg_once.select(\"targetId\",\"diseaseId\",\"maxClinPhase\").dropDuplicates(),\n",
    "                     on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"inner\")\n",
    "    .select(\n",
    "        \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "        F.expr(\"stack(5, \"\n",
    "               \"'Phase>=4', `Phase>=4`, \"\n",
    "               \"'Phase>=3', `Phase>=3`, \"\n",
    "               \"'Phase>=2', `Phase>=2`, \"\n",
    "               \"'Phase>=1', `Phase>=1`, \"\n",
    "               \"'PhaseT',  `PhaseT`\"\n",
    "               \")\").alias(\"phase_name\",\"prediction\")\n",
    "    )\n",
    ")\n",
    "\n",
    "def attach_metric(metric_col: str):\n",
    "    # comparison = metric flag yes/no at (target,disease,feature,value)\n",
    "    return (\n",
    "        agg_once.select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"feature\",\"value\",\n",
    "                        F.col(metric_col).alias(\"comparison\"))\n",
    "        .join(phases_long_for_records, on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"inner\")\n",
    "        .withColumn(\"metric\", F.lit(metric_col.replace(\"_flag\",\"\")))  # prettier label\n",
    "    )\n",
    "\n",
    "analysis_long = attach_metric(metric_flags[0])\n",
    "for mc in metric_flags[1:]:\n",
    "    analysis_long = analysis_long.unionByName(attach_metric(mc))\n",
    "\n",
    "# ---- 2) Count distinct pairs for 2x2 components using the fixed universe\n",
    "# a = count of pairs with comparison=='yes' AND prediction=='yes'\n",
    "yes_yes = (\n",
    "    analysis_long\n",
    "    .filter((F.col(\"comparison\")==\"yes\") & (F.col(\"prediction\")==\"yes\"))\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"a\"))\n",
    ")\n",
    "# yes_total = count of pairs with comparison=='yes' (regardless of prediction)\n",
    "yes_total = (\n",
    "    analysis_long\n",
    "    .filter(F.col(\"comparison\")==\"yes\")\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"yes_total\"))\n",
    ")\n",
    "\n",
    "# Assemble b,c,d from totals\n",
    "counts = (\n",
    "    yes_total\n",
    "    .join(yes_yes, on=[\"metric\",\"feature\",\"value\",\"phase_name\"], how=\"left\")\n",
    "    .join(total_pairs_by_phase, on=\"phase_name\", how=\"left\")\n",
    "    .join(total_pred_yes_by_phase, on=\"phase_name\", how=\"left\")\n",
    "    .na.fill({\"a\":0})\n",
    "    .withColumn(\"b\", F.col(\"yes_total\") - F.col(\"a\"))\n",
    "    .withColumn(\"c\", F.col(\"total_pred_yes\") - F.col(\"a\"))\n",
    "    .withColumn(\"d\", F.col(\"total_pairs\") - F.col(\"a\") - F.col(\"b\") - F.col(\"c\"))\n",
    "    .select(\n",
    "        \"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "        F.when(F.col(\"a\")<0,0).otherwise(F.col(\"a\")).cast(\"int\").alias(\"a\"),\n",
    "        F.when(F.col(\"b\")<0,0).otherwise(F.col(\"b\")).cast(\"int\").alias(\"b\"),\n",
    "        F.when(F.col(\"c\")<0,0).otherwise(F.col(\"c\")).cast(\"int\").alias(\"c\"),\n",
    "        F.when(F.col(\"d\")<0,0).otherwise(F.col(\"d\")).cast(\"int\").alias(\"d\"),\n",
    "        \"total_pairs\",\"total_pred_yes\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert to two-row format (comparison yes/no) with columns yes/no → ready for Fisher\n",
    "mat_counts = (\n",
    "    counts\n",
    "    .select(\"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "            F.lit(\"yes\").alias(\"comparison\"),\n",
    "            F.col(\"a\").alias(\"yes\"),\n",
    "            F.col(\"b\").alias(\"no\"))\n",
    "    .unionByName(\n",
    "        counts.select(\"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "                      F.lit(\"no\").alias(\"comparison\"),\n",
    "                      F.col(\"c\").alias(\"yes\"),\n",
    "                      F.col(\"d\").alias(\"no\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Safety: ensure ints and no nulls\n",
    "mat_counts = (\n",
    "    mat_counts.fillna(0)\n",
    "              .withColumn(\"yes\", F.col(\"yes\").cast(\"int\"))\n",
    "              .withColumn(\"no\",  F.col(\"no\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "# ---- 3) Fisher per group with applyInPandas\n",
    "result_schema = StructType([\n",
    "    StructField(\"group\",        StringType(),  True),\n",
    "    StructField(\"comparison\",   StringType(),  True),\n",
    "    StructField(\"phase\",        StringType(),  True),\n",
    "    StructField(\"oddsRatio\",    DoubleType(),  True),\n",
    "    StructField(\"pValue\",       DoubleType(),  True),\n",
    "    StructField(\"lowerInterval\",DoubleType(),  True),\n",
    "    StructField(\"upperInterval\",DoubleType(),  True),\n",
    "    StructField(\"total\",        StringType(),  True),\n",
    "    StructField(\"values\",       ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"relSuccess\",   DoubleType(),  True),\n",
    "    StructField(\"rsLower\",      DoubleType(),  True),\n",
    "    StructField(\"rsUpper\",      DoubleType(),  True),\n",
    "    StructField(\"path\",         StringType(),  True),\n",
    "])\n",
    "\n",
    "def _relative_success(matrix_2x2: np.ndarray):\n",
    "    a, b = matrix_2x2[0,0], matrix_2x2[0,1]\n",
    "    c, d = matrix_2x2[1,0], matrix_2x2[1,1]\n",
    "    rate_yes = a / (a + b) if (a + b) > 0 else 0.0\n",
    "    rate_no  = c / (c + d) if (c + d) > 0 else 0.0\n",
    "    rs = rate_yes - rate_no\n",
    "    import math\n",
    "    se = 0.0\n",
    "    if (a+b) > 0:\n",
    "        se += rate_yes * (1 - rate_yes) / (a + b)\n",
    "    if (c+d) > 0:\n",
    "        se += rate_no  * (1 - rate_no)  / (c + d)\n",
    "    se = math.sqrt(se)\n",
    "    lo, hi = rs - 1.96*se, rs + 1.96*se\n",
    "    return float(rs), float(lo), float(hi)\n",
    "\n",
    "def fisher_by_group(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    sub = pdf[[\"comparison\",\"yes\",\"no\"]].copy()\n",
    "    sub = sub.set_index(\"comparison\").reindex([\"yes\",\"no\"]).fillna(0)\n",
    "    mat = sub[[\"yes\",\"no\"]].to_numpy(dtype=int)\n",
    "\n",
    "    total = int(mat.sum())\n",
    "    if total == 0:\n",
    "        # Return a neutral row to avoid SciPy errors on empty tables\n",
    "        return pd.DataFrame([{\n",
    "            \"group\":        pdf[\"metric\"].iloc[0],\n",
    "            \"comparison\":   f\"{pdf['value'].iloc[0]}_only\",\n",
    "            \"phase\":        pdf[\"phase_name\"].iloc[0],\n",
    "            \"oddsRatio\":    float(\"nan\"),\n",
    "            \"pValue\":       float(\"nan\"),\n",
    "            \"lowerInterval\":float(\"nan\"),\n",
    "            \"upperInterval\":float(\"nan\"),\n",
    "            \"total\":        \"0\",\n",
    "            \"values\":       mat.tolist(),\n",
    "            \"relSuccess\":   float(\"nan\"),\n",
    "            \"rsLower\":      float(\"nan\"),\n",
    "            \"rsUpper\":      float(\"nan\"),\n",
    "            \"path\":         \"\",\n",
    "        }])\n",
    "\n",
    "    from scipy.stats import fisher_exact\n",
    "    from scipy.stats.contingency import odds_ratio\n",
    "\n",
    "    or_val, p_val = fisher_exact(mat, alternative=\"two-sided\")\n",
    "    ci = odds_ratio(mat).confidence_interval(0.95)\n",
    "    rs, rs_lo, rs_hi = _relative_success(mat)\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"group\":        pdf[\"metric\"].iloc[0],\n",
    "        \"comparison\":   f\"{pdf['value'].iloc[0]}_only\",\n",
    "        \"phase\":        pdf[\"phase_name\"].iloc[0],\n",
    "        \"oddsRatio\":    round(float(or_val), 2),\n",
    "        \"pValue\":       float(p_val),\n",
    "        \"lowerInterval\":round(float(ci[0]), 2),\n",
    "        \"upperInterval\":round(float(ci[1]), 2),\n",
    "        \"total\":        str(total),\n",
    "        \"values\":       mat.tolist(),\n",
    "        \"relSuccess\":   round(float(rs), 2),\n",
    "        \"rsLower\":      round(float(rs_lo), 2),\n",
    "        \"rsUpper\":      round(float(rs_hi), 2),\n",
    "        \"path\":         \"\",\n",
    "    }])\n",
    "\n",
    "# (optional) Arrow for speed\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "results_df = (\n",
    "    mat_counts\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .applyInPandas(fisher_by_group, schema=result_schema)\n",
    ")\n",
    "\n",
    "# ---- 4) Spreadsheet formatting + annotation + CSV\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# build disdic from agg_once\n",
    "disdic = {r[\"value\"]: r[\"feature\"] for r in agg_once.select(\"feature\",\"value\").distinct().collect()}\n",
    "\n",
    "patterns = [\"_only\", \"_isRightTissueSignalAgreed\"]\n",
    "regex_pattern = \"(\" + \"|\".join(patterns) + \")\"\n",
    "\n",
    "df_fmt = (\n",
    "    spreadSheetFormatter(results_df)\n",
    "    .withColumn(\"prefix\", F.regexp_replace(F.col(\"comparison\"), regex_pattern + \".*\", \"\"))\n",
    "    .withColumn(\"suffix\", F.regexp_extract(F.col(\"comparison\"), regex_pattern, 0))\n",
    ")\n",
    "\n",
    "mapping_expr = create_map([F.lit(x) for x in chain(*disdic.items())])\n",
    "df_annot = df_fmt.withColumn(\"annotation\", mapping_expr.getItem(F.col(\"prefix\")))\n",
    "\n",
    "#today_date = date.today().isoformat()\n",
    "#out_csv = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try.csv\"\n",
    "#df_annot.toPandas().to_csv(out_csv, index=False)\n",
    "#print(f\"Analysis written: {out_csv}\")\n",
    "today_date = date.today().isoformat()\n",
    "\n",
    "(out_path, coalesce_n) = (f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try\", 1)\n",
    "print('preparing df_annot to write')\n",
    "(df_annot\n",
    "  .coalesce(coalesce_n)                 # 1 file if feasible; increase if OOM on shuffle\n",
    "  .write.mode(\"overwrite\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(out_path))\n",
    "\n",
    "print(f\"Wrote CSV shards to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing df_annot to write\n"
     ]
    }
   ],
   "source": [
    "today_date = date.today().isoformat()\n",
    "\n",
    "(out_path, coalesce_n) = (f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try\", 1)\n",
    "print('preparing df_annot to write')\n",
    "(df_annot\n",
    "  .coalesce(coalesce_n)                 # 1 file if feasible; increase if OOM on shuffle\n",
    "  .write.mode(\"overwrite\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(out_path))\n",
    "\n",
    "print(f\"Wrote CSV shards to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_125 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_36 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_101 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_112 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_63 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_115 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_34 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_97 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_98 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_13 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_106 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_34 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_160 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_117 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_85 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_36 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_91 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_29 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_11 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_40 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_40 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_119 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_76 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_126 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_109 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_84 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_54 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_38 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_74 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_5 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_90 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_94 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_32 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_19 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_19 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_38 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_15 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_66 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_66 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_5 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_65 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_111 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_72 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_107 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_17 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_121 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_18 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_98 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_104 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_45 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_75 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_77 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_35 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_90 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_168 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_84 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_56 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_88 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_148 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_120 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_33 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_124 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_21 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_31 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_126 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_53 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_100 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_75 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_110 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_46 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_87 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_21 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_117 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_101 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_111 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_27 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_107 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_28 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_74 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_42 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_113 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_80 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_49 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_80 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_19 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_162 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_92 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_91 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_17 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_108 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_33 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_23 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_16 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_92 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_104 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_116 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_36 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_108 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_95 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_93 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_116 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_112 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_131 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_98 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_78 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_48 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_57 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_37 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_24 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_96 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_125 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_15 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_115 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_8 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_123 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_51 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_21 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_34 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_93 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_80 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_40 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_112 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_32 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_121 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_149 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_65 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_80 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_81 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_41 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_64 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_83 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_88 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_108 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_55 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_13 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_93 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_51 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_72 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_81 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_79 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_147 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_117 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_42 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_123 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_18 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_26 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_0 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_31 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_104 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_110 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_70 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_72 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_68 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_122 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_52 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_48 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_90 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_118 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_89 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_13 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_85 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_39 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_72 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_135 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_7 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_78 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_144 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_84 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_49 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_103 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_120 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_40 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_45 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_3 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_58 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_70 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_71 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_99 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_9 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_5 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_102 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_24 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_1 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_39 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_4 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_17 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_117 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_107 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_67 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_6 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_50 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_141 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_127 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_28 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_127 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_108 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_71 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_84 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_114 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_91 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_127 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_115 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_42 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_3 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_75 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_28 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_119 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_46 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_89 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_10 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_3 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_32 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_28 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_35 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_119 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_109 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_83 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_78 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_120 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_57 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_69 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_1 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_99 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_109 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_83 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_113 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_45 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_94 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_114 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_118 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_59 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_48 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_69 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_95 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_93 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_122 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_17 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_37 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_96 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_37 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_101 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_57 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_60 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_91 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_14 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_124 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_86 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_73 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_115 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_35 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_55 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_99 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_84 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_18 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_9 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_2 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_27 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_102 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_73 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_85 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_90 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_73 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_38 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_50 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_55 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_112 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_92 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_39 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_97 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_116 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_61 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_25 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_73 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_98 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_49 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_53 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_31 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_22 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_75 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_7 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_106 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_76 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_105 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_79 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_101 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_75 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_52 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_35 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_53 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_123 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_33 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_63 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_48 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_88 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_13 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_94 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_55 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_46 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_16 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_94 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_66 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_143 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_23 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_38 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_70 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_33 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_51 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_57 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_91 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_124 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_42 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_35 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_63 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_132 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_100 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_7 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_15 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_114 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_166 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_96 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_126 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_118 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_22 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_77 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_9 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_86 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_46 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_25 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_21 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_95 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_92 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_111 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_19 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_111 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_81 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_57 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_105 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_107 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_18 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_25 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_52 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_79 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_5 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_63 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_102 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_48 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_34 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_32 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_66 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_124 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_78 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_7 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_33 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_89 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_20 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_116 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_19 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_87 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_113 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_16 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_106 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_140 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_71 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_71 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_31 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_76 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_22 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_119 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_41 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_64 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_37 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_26 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_83 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_26 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_36 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_25 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_114 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_97 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_120 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_126 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_87 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_74 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_125 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_16 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_123 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_39 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_40 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_23 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_26 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_23 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_12 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_94 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_46 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_67 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_121 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_53 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_87 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_68 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_107 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_82 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_86 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_156 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_76 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_113 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_41 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_65 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_15 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_82 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_109 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_20 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_105 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_125 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_88 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_20 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_103 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_29 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_77 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_24 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_155 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_51 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_83 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_97 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_122 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_66 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_159 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_89 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_82 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_82 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_47 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_114 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_72 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_104 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_34 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_99 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_1 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_115 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_15 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_1 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_118 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_96 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_22 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_125 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_18 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_106 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_64 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_11 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_17 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_79 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_85 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_64 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_68 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_104 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_81 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_73 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_41 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_24 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_16 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_30 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_89 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_68 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_121 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_27 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_117 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_102 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_44 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_3 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_95 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_95 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_103 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_67 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_79 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_118 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_45 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_111 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_82 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_9 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_67 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_88 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_65 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_25 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_127 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_63 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_169 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_39 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_85 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_20 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_49 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_86 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_20 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_11 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_70 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_92 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_50 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_52 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_116 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_9 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_13 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_112 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_67 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_110 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_99 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_27 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_70 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_11 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_164 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_41 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_90 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_105 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_68 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_69 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_97 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_37 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_77 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_50 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_76 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_7 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_78 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_28 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_31 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_100 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_23 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_36 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_126 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_22 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_101 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_3 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_129 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_87 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_50 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_42 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_55 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_122 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_86 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_123 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_53 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_137 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_51 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_11 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_103 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_77 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_110 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_38 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_52 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_62 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_64 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_74 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_1 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_65 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_108 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_93 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_43 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_24 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_96 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_98 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_80 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_100 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_5 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_74 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_69 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_32 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_26 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_71 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_81 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_45 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_152 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_21 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_69 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_127 !\n",
      "25/09/17 14:50:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_49 !\n",
      "25/09/17 14:50:13 WARN YarnAllocator: Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "25/09/17 14:50:13 ERROR YarnScheduler: Lost executor 1 on jr-temp-doe-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "25/09/17 14:50:13 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "25/09/17 14:50:13 WARN TaskSetManager: Lost task 100.0 in stage 110.0 (TID 6276) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "25/09/17 14:50:13 WARN TaskSetManager: Lost task 100.0 in stage 114.0 (TID 6279) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "25/09/17 14:50:13 WARN TaskSetManager: Lost task 280.0 in stage 120.0 (TID 6549) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "25/09/17 14:50:13 WARN TaskSetManager: Lost task 100.0 in stage 124.0 (TID 6277) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "25/09/17 14:50:13 WARN TaskSetManager: Lost task 100.0 in stage 126.0 (TID 6280) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "25/09/17 14:50:13 WARN TaskSetManager: Lost task 100.0 in stage 131.0 (TID 6278) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "25/09/17 14:50:13 WARN TaskSetManager: Lost task 122.0 in stage 132.0 (TID 6548) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "25/09/17 14:50:13 WARN TaskSetManager: Lost task 96.0 in stage 133.0 (TID 6550) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0004_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 14:50:13.761]Container killed on request. Exit code is 137\n",
      "[2025-09-17 14:50:13.761]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 14:50:13.762]Killed by external signal\n",
      ".\n",
      "[Stage 110:(153 + 1) / 174][Stage 112:(76 + 1) / 174][Stage 114:(153 + 1) / 174]\r"
     ]
    }
   ],
   "source": [
    "today_date = date.today().isoformat()\n",
    "out_csv = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try.csv\"\n",
    "df_annot.toPandas().to_csv(out_csv, index=False)\n",
    "print(f\"Analysis written: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date = date.today().isoformat()\n",
    "out_csv = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try.csv\"\n",
    "df_annot.toPandas().to_csv(out_csv, index=False)\n",
    "print(f\"Analysis written: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m (analysis_long\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(\n\u001b[0;32m----> 3\u001b[0m         (\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetric\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoneCellYes\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m      4\u001b[0m         (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlasoo_2018\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# before we added \"_only\" suffix\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     ))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/functions.py:223\u001b[0m, in \u001b[0;36mcol\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@try_remote_functions\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcol\u001b[39m(col: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    Returns a :class:`~pyspark.sql.Column` based on the given column name.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    Column<'x'>\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/functions.py:96\u001b[0m, in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03mInvokes JVM function identified by name with args\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m jf \u001b[38;5;241m=\u001b[39m \u001b[43m_get_jvm_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_active_spark_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jf(\u001b[38;5;241m*\u001b[39margs))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/functions.py:87\u001b[0m, in \u001b[0;36m_get_jvm_function\u001b[0;34m(name, sc)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mRetrieves JVM function identified by name from\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03mJava gateway associated with sc.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m, name)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "(analysis_long\n",
    "    .filter(\n",
    "        (F.col(\"metric\") == \"NoneCellYes\") &\n",
    "        (F.col(\"value\") == \"Alasoo_2018\")   # before we added \"_only\" suffix\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_101 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_112 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_63 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_34 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_49 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_98 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_53 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_10 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_31 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_59 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_56 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_106 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_75 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_7 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_54 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_106 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_117 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_85 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_35 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_36 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_52 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_40 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_11 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_33 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_63 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_12 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_30 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_88 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_119 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_76 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_109 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_84 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_38 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_46 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_16 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_32 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_19 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_19 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_94 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_38 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_15 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_66 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_60 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_4 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_66 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_6 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_23 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_38 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_70 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_51 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_57 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_42 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_58 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_17 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_63 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_7 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_15 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_121 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_98 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_114 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_96 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_126 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_75 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_77 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_118 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_30 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_35 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_77 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_9 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_25 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_120 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_30 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_33 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_95 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_19 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_54 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_53 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_61 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_46 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_25 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_52 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_117 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_79 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_74 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_42 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_48 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_80 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_44 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_49 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_7 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_19 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_56 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_4 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_92 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_17 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_33 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_116 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_19 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_23 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_113 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_16 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_92 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_104 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_71 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_31 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_6 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_116 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_41 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_60 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_64 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_83 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_36 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_120 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_126 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_74 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_125 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_16 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_61 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_48 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_57 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_24 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_39 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_40 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_23 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_62 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_115 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_94 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_67 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_121 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_34 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_80 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_82 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_86 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_76 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_113 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_40 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_41 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_112 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_65 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_32 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_121 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_80 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_15 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_82 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_109 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_125 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_88 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_41 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_64 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_61 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_51 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_24 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_72 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_51 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_83 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_79 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_60 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_122 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_66 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_89 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_82 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_114 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_72 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_42 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_34 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_115 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_15 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_31 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_104 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_6 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_118 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_58 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_59 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_96 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_125 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_106 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_72 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_11 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_10 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_68 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_122 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_52 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_17 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_48 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_68 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_12 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_104 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_118 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_73 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_89 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_24 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_85 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_16 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_43 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_68 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_84 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_10 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_117 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_120 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_6 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_45 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_3 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_95 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_3 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_59 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_71 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_44 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_9 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_79 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_62 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_24 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_45 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_39 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_9 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_67 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_88 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_17 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_10 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_25 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_65 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_127 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_39 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_85 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_50 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_86 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_11 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_70 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_127 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_4 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_50 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_116 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_9 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_84 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_114 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_67 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_62 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_99 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_70 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_127 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_11 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_75 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_119 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_56 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_23 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_54 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_126 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_101 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_3 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_113 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_86 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_12 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_101 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_9 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_24 !\n",
      "25/09/10 16:24:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_25 !\n",
      "25/09/10 16:24:34 WARN YarnAllocator: Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:34 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 6 for reason Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:34 ERROR YarnScheduler: Lost executor 6 on jr-temp-doe-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:34 WARN TaskSetManager: Lost task 31.0 in stage 1087.0 (TID 42844) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:34 WARN TaskSetManager: Lost task 39.0 in stage 1088.0 (TID 42839) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:34 WARN TaskSetManager: Lost task 40.0 in stage 1088.0 (TID 42842) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:34 WARN TaskSetManager: Lost task 31.0 in stage 1089.0 (TID 42838) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:34 WARN TaskSetManager: Lost task 31.0 in stage 1093.0 (TID 42837) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:34 WARN TaskSetManager: Lost task 31.0 in stage 1095.0 (TID 42836) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:34 WARN TaskSetManager: Lost task 39.0 in stage 1097.0 (TID 42841) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:34 WARN TaskSetManager: Lost task 31.0 in stage 1098.0 (TID 42843) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000006 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:34.690]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:34.690]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:34.690]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_36 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_97 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_81 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_97 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_47 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_34 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_22 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_123 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_7 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_99 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_0 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_105 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_6 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_79 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_110 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_22 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_91 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_85 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_48 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_64 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_40 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_90 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_94 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_81 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_55 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_54 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_74 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_5 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_39 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_90 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_72 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_94 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_30 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_7 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_78 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_89 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_66 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_103 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_4 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_6 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_6 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_102 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_44 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_38 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_47 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_95 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_3 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_91 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_124 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_58 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_70 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_111 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_103 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_107 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_35 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_100 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_7 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_9 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_102 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_111 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_82 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_4 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_9 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_45 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_107 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_67 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_6 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_90 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_84 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_22 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_56 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_49 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_46 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_88 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_108 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_4 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_92 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_124 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_92 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_111 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_21 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_111 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_9 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_31 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_112 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_100 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_81 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_75 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_91 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_110 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_57 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_3 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_41 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_110 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_105 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_87 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_107 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_18 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_90 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_28 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_105 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_68 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_27 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_46 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_69 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_97 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_37 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_30 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_77 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_63 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_50 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_102 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_76 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_89 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_10 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_3 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_7 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_80 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_32 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_124 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_78 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_78 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_100 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_33 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_119 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_109 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_83 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_78 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_91 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_36 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_57 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_69 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_99 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_47 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_22 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_3 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_87 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_16 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_87 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_12 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_83 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_45 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_94 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_42 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_55 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_36 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_59 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_71 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_47 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_4 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_108 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_95 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_76 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_6 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_69 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_95 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_93 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_93 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_22 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_119 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_122 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_17 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_37 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_123 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_26 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_37 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_25 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_96 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_97 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_53 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_98 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_57 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_87 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_44 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_60 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_78 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_91 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_123 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_14 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_124 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_86 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_43 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_73 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_35 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_37 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_55 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_99 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_51 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_103 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_11 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_77 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_15 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_110 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_38 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_23 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_52 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_9 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_12 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_2 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_62 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_64 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_8 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_1 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_65 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_108 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_93 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_102 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_123 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_43 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_53 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_87 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_96 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_107 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_93 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_98 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_100 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_47 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_90 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_38 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_73 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_74 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_65 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_55 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_50 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_69 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_112 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_32 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_92 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_105 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_43 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_81 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_20 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_103 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_81 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_108 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_55 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_43 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_58 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_45 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_97 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_29 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_69 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_77 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_13 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_61 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_93 !\n",
      "25/09/10 16:24:58 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_49 !\n",
      "25/09/10 16:24:58 WARN YarnAllocator: Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 7 for reason Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 ERROR YarnScheduler: Lost executor 7 on jr-temp-doe-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 WARN TaskSetManager: Lost task 26.0 in stage 1087.0 (TID 43468) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 WARN TaskSetManager: Lost task 99.0 in stage 1088.0 (TID 43382) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 WARN TaskSetManager: Lost task 26.0 in stage 1089.0 (TID 43472) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 WARN TaskSetManager: Lost task 99.0 in stage 1090.0 (TID 43380) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 WARN TaskSetManager: Lost task 10.0 in stage 1091.0 (TID 43488) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 WARN TaskSetManager: Lost task 99.0 in stage 1092.0 (TID 43379) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 WARN TaskSetManager: Lost task 26.0 in stage 1093.0 (TID 43473) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:24:58 WARN TaskSetManager: Lost task 99.0 in stage 1094.0 (TID 43381) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000007 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:24:58.601]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:24:58.601]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:24:58.602]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_36 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_36 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_52 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_52 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_48 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_38 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_43 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_38 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_45 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_47 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_57 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_45 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_52 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_52 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_69 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_37 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_48 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_36 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_57 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_69 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_47 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_45 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_55 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_47 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_69 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_41 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_37 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_36 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_37 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_57 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_48 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_43 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_57 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_37 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_55 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_38 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_41 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_47 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_38 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_55 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_43 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_41 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_55 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_43 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_45 !\n",
      "25/09/10 16:25:43 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_69 !\n",
      "25/09/10 16:25:43 WARN YarnAllocator: Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 9 for reason Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 ERROR YarnScheduler: Lost executor 9 on jr-temp-doe-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 WARN TaskSetManager: Lost task 80.0 in stage 1087.0 (TID 43869) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 WARN TaskSetManager: Lost task 103.0 in stage 1088.0 (TID 43782) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 WARN TaskSetManager: Lost task 77.0 in stage 1095.0 (TID 43870) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 WARN TaskSetManager: Lost task 102.0 in stage 1096.0 (TID 43784) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 WARN TaskSetManager: Lost task 102.0 in stage 1097.0 (TID 43783) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 WARN TaskSetManager: Lost task 79.0 in stage 1098.0 (TID 43871) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 WARN TaskSetManager: Lost task 106.0 in stage 1099.0 (TID 43859) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:25:43 WARN TaskSetManager: Lost task 7.0 in stage 1100.0 (TID 43872) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000009 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:25:43.369]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:25:43.369]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:25:43.370]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_73 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_112 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_63 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_34 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_62 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_49 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_13 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_10 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_53 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_31 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_59 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_56 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_22 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_75 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_54 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_76 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_105 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_75 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_85 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_35 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_53 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_29 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_103 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_11 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_40 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_33 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_63 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_12 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_30 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_13 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_76 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_84 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_46 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_16 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_32 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_19 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_19 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_15 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_66 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_60 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_66 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_23 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_70 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_33 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_51 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_5 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_42 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_65 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_72 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_0 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_58 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_17 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_63 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_15 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_121 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_18 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_2 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_2 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_75 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_118 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_8 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_30 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_35 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_22 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_86 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_25 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_30 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_21 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_33 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_19 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_54 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_61 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_53 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_61 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_46 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_105 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_25 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_21 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_101 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_79 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_5 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_28 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_74 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_42 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_34 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_80 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_44 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_49 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_32 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_66 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_19 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_56 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_102 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_20 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_17 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_33 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_116 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_23 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_16 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_8 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_71 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_59 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_31 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_108 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_22 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_116 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_60 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_64 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_83 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_26 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_8 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_74 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_16 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_61 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_123 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_2 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_24 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_39 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_40 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_23 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_26 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_62 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_46 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_67 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_123 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_121 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_51 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_21 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_68 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_34 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_80 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_82 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_86 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_76 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_40 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_112 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_65 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_32 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_121 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_80 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_15 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_82 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_20 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_105 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_64 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_83 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_103 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_108 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_61 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_51 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_72 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_51 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_83 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_79 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_60 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_66 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_14 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_82 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_89 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_82 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_72 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_42 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_104 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_123 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_18 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_34 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_1 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_26 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_15 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_31 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_56 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_1 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_118 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_58 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_59 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_18 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_70 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_64 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_72 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_10 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_11 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_68 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_17 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_79 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_12 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_68 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_118 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_73 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_41 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_29 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_89 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_24 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_13 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_16 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_85 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_68 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_8 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_84 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_49 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_121 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_27 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_10 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_103 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_40 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_59 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_71 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_103 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_99 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_67 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_44 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_79 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_5 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_62 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_29 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_24 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_1 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_39 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_67 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_0 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_17 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_14 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_10 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_25 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_65 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_63 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_39 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_85 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_20 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_50 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_86 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_20 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_11 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_100 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_70 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_28 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_108 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_58 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_50 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_116 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_13 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_112 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_2 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_71 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_84 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_67 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_62 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_27 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_70 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_11 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_42 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_75 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_105 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_46 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_30 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_50 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_76 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_89 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_32 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_31 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_28 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_28 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_35 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_83 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_56 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_44 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_23 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_14 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_54 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_1 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_22 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_29 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_12 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_50 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_48 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_86 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_123 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_12 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_27 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_53 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_44 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_73 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_35 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_103 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_54 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_84 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_18 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_64 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_74 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_65 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_27 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_60 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_108 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_24 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_0 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_73 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_80 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_85 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_5 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_74 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_112 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_26 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_71 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_39 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_58 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_0 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_21 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_14 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_49 !\n",
      "25/09/10 16:26:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_25 !\n",
      "25/09/10 16:26:40 WARN YarnAllocator: Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 8 for reason Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 ERROR YarnScheduler: Lost executor 8 on jr-temp-doe-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 WARN TaskSetManager: Lost task 113.0 in stage 1088.0 (TID 44284) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 WARN TaskSetManager: Lost task 112.0 in stage 1090.0 (TID 44272) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 WARN TaskSetManager: Lost task 113.0 in stage 1090.0 (TID 44285) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 WARN TaskSetManager: Lost task 112.0 in stage 1092.0 (TID 44273) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 WARN TaskSetManager: Lost task 112.0 in stage 1094.0 (TID 44274) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 WARN TaskSetManager: Lost task 113.0 in stage 1094.0 (TID 44286) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 WARN TaskSetManager: Lost task 112.0 in stage 1096.0 (TID 44271) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:26:40 WARN TaskSetManager: Lost task 113.0 in stage 1097.0 (TID 44283) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000008 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:26:40.408]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:26:40.408]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:26:40.408]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_167 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_36 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_101 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_112 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_115 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_98 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_97 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_62 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_98 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_49 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_128 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_31 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_59 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_56 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_106 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_22 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_7 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_54 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_160 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_106 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_76 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_117 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_101 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_85 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_52 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_36 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_35 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_40 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_33 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_88 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_30 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_94 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_119 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_76 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_109 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_126 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_38 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_46 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_90 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_94 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_171 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_38 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_4 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_6 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_23 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_47 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_33 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_51 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_57 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_91 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_124 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_42 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_65 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_111 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_72 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_107 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_132 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_100 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_122 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_7 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_138 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_106 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_98 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_114 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_104 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_96 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_126 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_77 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_30 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_90 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_35 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_168 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_22 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_77 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_134 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_25 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_120 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_30 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_148 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_33 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_95 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_124 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_111 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_54 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_146 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_111 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_126 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_100 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_81 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_110 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_46 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_87 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_107 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_25 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_52 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_117 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_111 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_107 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_28 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_74 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_42 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_102 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_113 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_120 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_80 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_49 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_124 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_78 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_7 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_56 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_162 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_89 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_4 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_92 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_91 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_108 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_33 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_23 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_113 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_106 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_130 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_104 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_92 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_140 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_71 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_116 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_59 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_95 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_6 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_31 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_93 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_22 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_119 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_64 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_83 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_26 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_36 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_112 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_154 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_114 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_120 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_126 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_87 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_74 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_125 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_78 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_123 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_43 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_57 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_37 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_24 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_40 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_23 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_26 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_96 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_125 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_62 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_115 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_94 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_46 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_67 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_123 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_51 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_87 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_107 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_93 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_80 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_156 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_76 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_113 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_40 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_112 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_65 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_80 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_109 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_125 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_81 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_88 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_64 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_83 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_88 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_103 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_55 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_77 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_93 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_51 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_72 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_124 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_51 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_83 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_97 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_122 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_117 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_114 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_72 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_42 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_123 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_99 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_26 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_115 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_31 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_56 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_104 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_6 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_150 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_96 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_59 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_110 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_125 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_106 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_64 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_72 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_122 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_52 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_90 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_104 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_81 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_24 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_85 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_43 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_78 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_136 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_144 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_49 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_27 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_117 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_120 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_103 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_109 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_6 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_102 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_40 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_3 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_45 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_95 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_105 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_3 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_59 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_71 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_103 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_67 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_9 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_118 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_102 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_62 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_45 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_24 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_111 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_9 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_67 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_88 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_25 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_65 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_165 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_127 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_107 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_85 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_127 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_28 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_4 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_92 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_52 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_9 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_112 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_71 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_67 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_91 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_110 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_62 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_99 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_27 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_42 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_90 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_119 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_97 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_69 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_46 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_37 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_30 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_76 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_3 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_7 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_28 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_31 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_100 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_28 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_35 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_119 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_83 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_56 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_78 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_23 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_36 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_57 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_142 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_69 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_54 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_99 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_47 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_22 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_3 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_87 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_45 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_94 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_114 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_55 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_118 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_4 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_47 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_69 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_95 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_93 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_123 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_37 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_27 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_37 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_57 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_91 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_124 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_115 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_35 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_55 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_99 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_103 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_54 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_77 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_110 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_38 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_9 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_64 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_74 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_65 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_27 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_102 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_24 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_158 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_113 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_80 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_100 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_85 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_47 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_38 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_74 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_55 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_112 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_92 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_43 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_26 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_71 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_81 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_43 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_45 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_152 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_97 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_69 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_25 !\n",
      "25/09/10 16:29:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_49 !\n",
      "25/09/10 16:29:15 WARN YarnAllocator: Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 10 for reason Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 ERROR YarnScheduler: Lost executor 10 on jr-temp-doe-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 WARN TaskSetManager: Lost task 97.0 in stage 1123.0 (TID 45522) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 WARN TaskSetManager: Lost task 94.0 in stage 1123.0 (TID 45519) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 WARN TaskSetManager: Lost task 96.0 in stage 1123.0 (TID 45521) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 WARN TaskSetManager: Lost task 95.0 in stage 1123.0 (TID 45520) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 WARN TaskSetManager: Lost task 97.0 in stage 1124.0 (TID 45516) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 WARN TaskSetManager: Lost task 99.0 in stage 1124.0 (TID 45524) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 WARN TaskSetManager: Lost task 96.0 in stage 1124.0 (TID 45515) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "25/09/10 16:29:15 WARN TaskSetManager: Lost task 98.0 in stage 1124.0 (TID 45523) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000010 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:29:15.759]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:29:15.760]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:29:15.760]Killed by external signal\n",
      ".\n",
      "                                                                                 128]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows that will enter Fisher 2×2 for Alasoo_2018 / NoneCellYes: 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_125 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_73 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_63 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_34 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_98 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_97 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_98 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_13 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_53 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_10 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_106 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_75 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_157 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_106 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_76 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_105 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_117 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_101 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_75 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_85 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_145 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_53 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_29 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_123 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_11 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_63 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_12 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_88 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_13 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_94 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_76 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_109 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_126 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_84 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_16 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_5 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_90 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_32 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_19 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_19 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_94 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_139 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_143 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_15 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_66 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_60 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_66 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_70 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_91 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_5 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_124 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_58 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_0 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_17 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_63 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_100 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_15 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_121 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_18 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_98 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_166 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_2 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_104 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_2 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_96 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_126 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_75 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_77 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_118 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_8 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_90 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_77 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_9 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_86 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_120 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_21 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_95 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_151 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_124 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_21 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_19 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_61 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_53 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_100 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_81 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_61 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_105 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_87 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_18 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_21 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_117 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_79 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_5 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_74 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_102 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_113 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_34 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_48 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_80 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_44 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_32 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_66 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_124 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_78 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_19 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_89 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_92 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_91 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_17 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_20 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_116 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_19 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_16 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_16 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_106 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_104 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_92 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_8 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_119 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_108 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_95 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_93 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_116 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_41 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_60 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_83 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_131 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_120 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_8 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_74 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_87 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_16 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_61 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_78 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_123 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_48 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_2 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_39 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_96 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_15 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_23 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_12 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_94 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_8 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_123 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_121 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_87 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_21 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_68 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_34 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_93 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_82 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_86 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_80 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_76 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_41 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_32 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_121 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_149 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_80 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_15 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_82 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_109 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_20 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_105 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_81 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_41 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_88 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_20 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_83 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_88 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_103 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_108 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_61 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_29 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_77 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_13 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_93 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_155 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_83 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_97 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_79 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_147 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_60 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_66 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_14 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_159 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_117 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_82 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_89 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_82 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_123 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_18 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_34 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_99 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_1 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_0 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_15 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_104 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_1 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_118 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_58 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_170 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_18 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_106 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_70 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_11 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_10 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_68 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_122 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_79 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_17 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_48 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_68 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_12 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_90 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_104 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_81 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_118 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_73 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_41 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_29 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_89 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_13 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_16 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_85 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_135 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_7 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_78 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_68 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_8 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_84 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_121 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_10 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_117 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_120 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_103 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_102 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_95 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_103 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_173 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_121 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_44 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_79 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_5 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_118 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_102 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_29 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_1 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_39 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_4 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_0 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_17 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_88 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_14 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_117 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_10 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_127 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_63 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_169 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_39 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_6 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_85 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_20 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_50 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_172 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_141 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_86 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_161 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_20 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_11 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_153 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_70 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_127 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_127 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_108 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_92 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_58 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_50 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_116 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_13 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_2 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_84 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_114 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_91 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_99 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_70 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_127 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_11 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_115 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_164 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_3 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_75 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_90 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_105 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_97 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_110 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_50 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_76 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_89 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_10 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_32 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_100 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_109 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_83 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_44 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_78 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_120 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_14 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_126 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_1 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_99 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_101 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_129 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_29 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_109 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_87 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_50 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_12 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_113 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_94 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_133 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_48 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_122 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_163 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_95 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_93 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_122 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_17 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_86 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_123 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_12 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_53 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_101 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_44 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_137 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_91 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_14 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_124 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_73 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_99 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_103 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_11 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_84 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_18 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_77 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_2 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_74 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_1 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_60 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_108 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_102 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_0 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_96 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_73 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_98 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_80 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_100 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_85 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_5 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_74 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_92 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_81 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_39 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_58 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_0 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_21 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_97 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_116 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_14 !\n",
      "25/09/10 16:30:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_127 !\n",
      "25/09/10 16:30:34 WARN YarnAllocator: Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:30:34 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 11 for reason Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:30:34 ERROR YarnScheduler: Lost executor 11 on jr-temp-doe-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:30:34 WARN TaskSetManager: Lost task 56.0 in stage 1175.0 (TID 47749) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:30:34 WARN TaskSetManager: Lost task 101.0 in stage 1176.0 (TID 47651) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:30:34 WARN TaskSetManager: Lost task 56.0 in stage 1177.0 (TID 47751) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:30:34 WARN TaskSetManager: Lost task 101.0 in stage 1178.0 (TID 47652) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:30:34 WARN TaskSetManager: Lost task 56.0 in stage 1179.0 (TID 47748) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:30:34 WARN TaskSetManager: Lost task 56.0 in stage 1181.0 (TID 47750) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:30:34 WARN TaskSetManager: Lost task 56.0 in stage 1184.0 (TID 47752) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:30:34 WARN TaskSetManager: Lost task 106.0 in stage 1185.0 (TID 47755) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000011 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:30:33.670]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:30:33.670]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:30:33.670]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_24 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_51 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_36 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_112 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_51 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_81 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_115 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_62 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_122 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_49 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_31 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_59 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_56 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_114 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_47 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_34 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_22 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_42 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_104 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_7 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_54 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_26 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_115 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_31 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_56 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_6 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_79 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_59 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_110 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_22 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_35 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_36 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_52 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_125 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_91 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_64 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_40 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_33 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_52 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_85 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_48 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_64 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_40 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_30 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_119 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_55 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_54 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_24 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_38 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_46 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_74 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_39 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_43 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_72 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_94 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_30 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_89 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_66 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_49 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_38 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_27 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_4 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_6 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_6 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_40 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_44 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_3 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_23 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_45 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_38 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_95 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_33 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_47 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_51 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_57 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_3 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_42 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_59 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_58 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_70 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_65 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_71 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_111 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_99 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_107 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_35 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_7 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_9 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_62 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_24 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_45 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_111 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_82 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_9 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_45 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_25 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_65 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_107 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_67 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_30 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_35 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_84 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_22 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_56 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_49 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_100 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_46 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_25 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_88 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_28 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_4 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_30 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_33 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_92 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_52 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_111 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_54 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_111 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_9 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_31 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_112 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_71 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_75 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_110 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_62 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_57 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_27 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_42 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_110 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_41 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_46 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_107 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_25 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_52 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_28 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_119 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_101 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_68 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_27 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_28 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_46 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_37 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_30 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_42 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_77 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_63 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_49 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_80 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_3 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_7 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_7 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_78 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_28 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_31 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_28 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_33 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_35 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_56 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_119 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_4 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_56 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_102 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_23 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_36 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_57 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_54 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_47 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_22 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_33 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_3 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_23 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_87 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_83 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_45 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_42 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_71 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_36 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_55 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_59 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_71 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_59 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_4 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_47 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_76 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_6 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_31 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_22 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_119 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_64 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_37 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_26 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_37 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_27 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_26 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_36 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_25 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_96 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_114 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_97 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_98 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_37 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_57 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_125 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_60 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_86 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_43 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_57 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_35 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_37 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_24 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_55 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_40 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_51 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_23 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_26 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_125 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_54 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_62 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_110 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_52 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_38 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_9 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_115 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_46 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_62 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_64 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_65 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_93 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_27 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_43 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_51 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_53 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_24 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_107 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_40 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_112 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_90 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_47 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_73 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_38 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_65 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_65 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_50 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_69 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_55 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_112 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_32 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_43 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_125 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_26 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_64 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_71 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_55 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_43 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_144_45 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_61 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_215_25 !\n",
      "25/09/10 16:31:04 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_138_49 !\n",
      "25/09/10 16:31:04 WARN YarnAllocator: Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 12 for reason Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 ERROR YarnScheduler: Lost executor 12 on jr-temp-doe-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 WARN TaskSetManager: Lost task 106.0 in stage 1174.0 (TID 48007) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 WARN TaskSetManager: Lost task 106.0 in stage 1176.0 (TID 48009) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 WARN TaskSetManager: Lost task 107.0 in stage 1178.0 (TID 48021) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 WARN TaskSetManager: Lost task 106.0 in stage 1178.0 (TID 48012) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 WARN TaskSetManager: Lost task 107.0 in stage 1180.0 (TID 48022) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 WARN TaskSetManager: Lost task 106.0 in stage 1180.0 (TID 48008) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 WARN TaskSetManager: Lost task 107.0 in stage 1182.0 (TID 48023) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "25/09/10 16:31:04 WARN TaskSetManager: Lost task 107.0 in stage 1183.0 (TID 48024) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1757519175126_0001_01_000012 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-10 16:31:04.161]Container killed on request. Exit code is 137\n",
      "[2025-09-10 16:31:04.161]Container exited with a non-zero exit code 137. \n",
      "[2025-09-10 16:31:04.161]Killed by external signal\n",
      ".\n",
      "ERROR:root:Exception while sending command.+ 5) / 128][Stage 1210:(63 + 5) / 128]74]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o2989.showString",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m check_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      3\u001b[0m     analysis_long\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount of rows that will enter Fisher 2×2 for Alasoo_2018 / NoneCellYes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, check_df\u001b[38;5;241m.\u001b[39mcount())\n\u001b[0;32m---> 11\u001b[0m \u001b[43mcheck_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:978\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    971\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    972\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m         },\n\u001b[1;32m    976\u001b[0m     )\n\u001b[0;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o2989.showString"
     ]
    }
   ],
   "source": [
    "# Example: show rows for metric/group = 'NoneCellYes' AND value = 'Alasoo_2018'\n",
    "check_df = (\n",
    "    analysis_long\n",
    "    .filter(\n",
    "        (F.col(\"metric\") == \"NoneCellYes\") &\n",
    "        (F.col(\"value\") == \"Alasoo_2018\")   # before we added \"_only\" suffix\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Count of rows that will enter Fisher 2×2 for Alasoo_2018 / NoneCellYes:\", check_df.count())\n",
    "check_df.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### desglossing the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created with:\n",
      "  spark.driver.memory: 12g\n",
      "  spark.executor.memory: 40g\n",
      "  spark.executor.cores: 10\n",
      "  spark.executor.instances: 1\n",
      "  spark.yarn.executor.memoryOverhead: 6g\n",
      "  spark.sql.shuffle.partitions: 128\n",
      "  spark.default.parallelism: 128\n",
      "  spark.sql.adaptive.enabled: true\n",
      "  spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "Spark UI: http://jr-temp-doe-m.c.open-targets-eu-dev.internal:37033\n",
      "Loaded all base tables.\n",
      "Built newColoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded gwasComplete\n",
      "Built gwasComplete\n",
      "Built resolvedColoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 12:50:13 WARN CacheManager: Asked to cache already cached data.\n",
      "25/09/17 12:50:14 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built temporary DoE datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 12:50:15 WARN CacheManager: Asked to cache already cached data.\n",
      "25/09/17 12:50:15 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built analysis_chembl_indication\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Single-script, loop-free PySpark job (tall/unpivot + single aggregation)\n",
    "\n",
    "import os\n",
    "from datetime import date\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    ")\n",
    "\n",
    "# Your helpers\n",
    "from functions import (\n",
    "    relative_success,\n",
    "    spreadSheetFormatter,\n",
    "    discrepancifier,\n",
    "    temporary_directionOfEffect,\n",
    "    buildColocData,\n",
    "    gwasDataset,\n",
    ")\n",
    "from DoEAssessment import directionOfEffect  # noqa: F401  (kept if you need it later)\n",
    "\n",
    "# -------------------------------\n",
    "# Spark / YARN resource settings (Single-Node Option A)\n",
    "# -------------------------------\n",
    "driver_memory = \"12g\"                 # string with unit\n",
    "executor_memory = \"40g\"               # string with unit (heap)\n",
    "executor_cores = 10                   # int\n",
    "num_executors = 1                     # int (one fat executor on single node)\n",
    "executor_memory_overhead = \"6g\"       # string with unit (PySpark/Arrow/off-heap)\n",
    "shuffle_partitions = 128              # int (~2–3x cores)\n",
    "default_parallelism = 128             # int (match shuffle_partitions)\n",
    "\n",
    "# If you later move to a multi-worker cluster, replace the values above.\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MyOptimizedPySparkApp\")\n",
    "    .config(\"spark.master\", \"yarn\")\n",
    "    # core resources\n",
    "    .config(\"spark.driver.memory\", driver_memory)\n",
    "    .config(\"spark.executor.memory\", executor_memory)\n",
    "    .config(\"spark.executor.cores\", executor_cores)\n",
    "    .config(\"spark.executor.instances\", num_executors)\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead)\n",
    "    # shuffle & parallelism\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions)\n",
    "    .config(\"spark.default.parallelism\", default_parallelism)\n",
    "    # adaptive query execution for better skew/partition sizing\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession created with:\")\n",
    "for k in [\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.executor.cores\",\n",
    "    \"spark.executor.instances\",\n",
    "    \"spark.yarn.executor.memoryOverhead\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.default.parallelism\",\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "]:\n",
    "    print(f\"  {k}: {spark.conf.get(k)}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# -------------------------------\n",
    "# Spark / YARN resource settings\n",
    "# -------------------------------\n",
    "driver_memory = \"16g\"\n",
    "executor_memory = \"32g\"\n",
    "executor_cores = \"8\"\n",
    "num_executors = \"16\"\n",
    "executor_memory_overhead = \"8g\"\n",
    "shuffle_partitions = \"150\"\n",
    "default_parallelism = str(int(executor_cores) * int(num_executors) * 2)  # 80\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MyOptimizedPySparkApp\")\n",
    "    .config(\"spark.master\", \"yarn\")\n",
    "    .config(\"spark.driver.memory\", driver_memory)\n",
    "    .config(\"spark.executor.memory\", executor_memory)\n",
    "    .config(\"spark.executor.cores\", executor_cores)\n",
    "    .config(\"spark.executor.instances\", num_executors)\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead)\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions)\n",
    "    .config(\"spark.default.parallelism\", default_parallelism)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession created with:\")\n",
    "for k in [\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.executor.cores\",\n",
    "    \"spark.executor.instances\",\n",
    "    \"spark.yarn.executor.memoryOverhead\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.default.parallelism\",\n",
    "]:\n",
    "    print(f\"  {k}: {spark.conf.get(k)}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "'''\n",
    "# --------------------------------\n",
    "# 0) Load inputs\n",
    "# --------------------------------\n",
    "path_n = \"gs://open-targets-data-releases/25.06/output/\"\n",
    "\n",
    "target = spark.read.parquet(f\"{path_n}target/\")\n",
    "diseases = spark.read.parquet(f\"{path_n}disease/\")\n",
    "evidences = spark.read.parquet(f\"{path_n}evidence\")\n",
    "credible = spark.read.parquet(f\"{path_n}credible_set\")\n",
    "new = spark.read.parquet(f\"{path_n}colocalisation_coloc\")\n",
    "index = spark.read.parquet(f\"{path_n}study/\")\n",
    "variantIndex = spark.read.parquet(f\"{path_n}variant\")\n",
    "biosample = spark.read.parquet(f\"{path_n}biosample\")\n",
    "ecaviar = spark.read.parquet(f\"{path_n}colocalisation_ecaviar\")\n",
    "all_coloc = ecaviar.unionByName(new, allowMissingColumns=True)\n",
    "print(\"Loaded all base tables.\")\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build coloc + GWAS dataset\n",
    "# --------------------------------\n",
    "newColoc = buildColocData(all_coloc, credible, index)\n",
    "print(\"Built newColoc\")\n",
    "\n",
    "gwasComplete = gwasDataset(evidences, credible)\n",
    "print(\"Built gwasComplete\")\n",
    "\n",
    "resolvedColoc = (\n",
    "    newColoc.withColumnRenamed(\"geneId\", \"targetId\")\n",
    "    .join(\n",
    "        gwasComplete.withColumnRenamed(\"studyLocusId\", \"leftStudyLocusId\"),\n",
    "        on=[\"leftStudyLocusId\", \"targetId\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .join(\n",
    "        diseases.selectExpr(\"id as diseaseId\", \"name\", \"parents\", \"therapeuticAreas\"),\n",
    "        on=\"diseaseId\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"diseaseId\",\n",
    "        F.explode_outer(F.concat(F.array(F.col(\"diseaseId\")), F.col(\"parents\"))),\n",
    "    )\n",
    "    .drop(\"parents\", \"oldDiseaseId\")\n",
    "    .withColumn(\n",
    "        \"colocDoE\",\n",
    "        F.when(\n",
    "            F.col(\"rightStudyType\").isin([\"eqtl\", \"pqtl\", \"tuqtl\", \"sceqtl\", \"sctuqtl\"]),\n",
    "            F.when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"GoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"LoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"LoF_protect\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"GoF_protect\"))\n",
    "        ).when(\n",
    "            F.col(\"rightStudyType\").isin([\"sqtl\", \"scsqtl\"]),\n",
    "            F.when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"LoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"GoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"GoF_protect\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"LoF_protect\"))\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "print(\"Built resolvedColoc\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Direction of Effect & ChEMBL indication\n",
    "# --------------------------------\n",
    "datasource_filter = [\n",
    "    \"gwas_credible_sets\",\n",
    "    \"gene_burden\",\n",
    "    \"eva\",\n",
    "    \"eva_somatic\",\n",
    "    \"gene2phenotype\",\n",
    "    \"orphanet\",\n",
    "    \"cancer_gene_census\",\n",
    "    \"intogen\",\n",
    "    \"impc\",\n",
    "    \"chembl\",\n",
    "]\n",
    "assessment, evidences, actionType_unused, oncolabel_unused = temporary_directionOfEffect(path_n, datasource_filter)\n",
    "print(\"Built temporary DoE datasets\")\n",
    "\n",
    "# (Optional) Add MoA to ChEMBL paths as in your later code\n",
    "mecact_path = f\"{path_n}drug_mechanism_of_action/\"\n",
    "mecact = spark.read.parquet(mecact_path)\n",
    "actionType = (\n",
    "    mecact.select(\n",
    "        F.explode_outer(\"chemblIds\").alias(\"drugId\"),\n",
    "        \"actionType\",\n",
    "        \"mechanismOfAction\",\n",
    "        \"targets\",\n",
    "    )\n",
    "    .select(\n",
    "        F.explode_outer(\"targets\").alias(\"targetId\"),\n",
    "        \"drugId\",\n",
    "        \"actionType\",\n",
    "        \"mechanismOfAction\",\n",
    "    )\n",
    "    .groupBy(\"targetId\", \"drugId\")\n",
    "    .agg(F.collect_set(\"actionType\").alias(\"actionType2\"))\n",
    "    .withColumn(\"nMoA\", F.size(F.col(\"actionType2\")))\n",
    ")\n",
    "\n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "        .join(actionType, on=[\"targetId\", \"drugId\"], how=\"left\")\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(\"clinicalPhase\").over(Window.partitionBy(\"targetId\", \"diseaseId\")),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\", \"actionType2\")\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    .drop(\"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\")\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    ")\n",
    "print(\"Built analysis_chembl_indication\")\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Benchmark (filtered coloc) + clinical phase flags\n",
    "# --------------------------------\n",
    "resolvedColocFiltered = resolvedColoc.filter((F.col(\"clpp\") >= 0.01) | (F.col(\"h4\") >= 0.8))\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\").count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\")).drop(\"count\")\n",
    ")\n",
    "benchmark = (\n",
    "    resolvedColocFiltered.filter(F.col(\"name\") != \"COVID-19\")\n",
    "    .join(analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\")\n",
    "    .withColumn(\n",
    "        \"AgreeDrug\",\n",
    "        F.when((F.col(\"drugGoF_protect\").isNotNull()) & (F.col(\"colocDoE\") == \"GoF_protect\"), \"yes\")\n",
    "        .when((F.col(\"drugLoF_protect\").isNotNull()) & (F.col(\"colocDoE\") == \"LoF_protect\"), \"yes\")\n",
    "        .otherwise(\"no\"),\n",
    "    )\n",
    "    .join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    ")\n",
    "\n",
    "benchmark = (\n",
    "    benchmark.join(F.broadcast(negativeTD), on=[\"targetId\", \"diseaseId\"], how=\"left\")\n",
    "    .withColumn(\"PhaseT\", F.when(F.col(\"stopReason\") == \"Negative\", \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=4\", F.when((F.col(\"maxClinPhase\") == 4) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=3\", F.when((F.col(\"maxClinPhase\") >= 3) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=2\", F.when((F.col(\"maxClinPhase\") >= 2) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=1\", F.when((F.col(\"maxClinPhase\") >= 1) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Replace nested loops:\n",
    "#     compute DoE counts once → derive flags → unpivot → single aggregation\n",
    "# --------------------------------\n",
    "doe_cols = [\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "\n",
    "# counts per colocDoE over the grouping you previously used in the loop\n",
    "group_keys = [\n",
    "    \"targetId\", \"diseaseId\", \"maxClinPhase\",\n",
    "    \"actionType2\", \"biosampleName\", \"projectId\", \"rightStudyType\", \"colocalisationMethod\"\n",
    "]\n",
    "\n",
    "doe_counts = (\n",
    "    benchmark.groupBy(*group_keys)\n",
    "    .agg(*[F.sum(F.when(F.col(\"colocDoE\") == c, 1).otherwise(0)).alias(c) for c in doe_cols])\n",
    ")\n",
    "\n",
    "# max name(s) (in case of ties) without arrays of structs\n",
    "greatest_count = F.greatest(*[F.col(c) for c in doe_cols])\n",
    "max_names = F.filter(\n",
    "    F.array(*[F.when(F.col(c) == greatest_count, F.lit(c)) for c in doe_cols]),\n",
    "    lambda x: x.isNotNull()\n",
    ")\n",
    "\n",
    "# presence of drug-side signals (equivalent to *_ch presence in your loop path)\n",
    "has_lof_ch = F.col(\"drugLoF_protect\").isNotNull()\n",
    "has_gof_ch = F.col(\"drugGoF_protect\").isNotNull()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- prerequisites used below ---\n",
    "# doe_cols = [\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "\n",
    "# Recompute safe maxima + names (handles nulls as 0)\n",
    "safe_max = F.greatest(*[F.coalesce(F.col(c), F.lit(0)) for c in doe_cols])\n",
    "max_names = F.filter(\n",
    "    F.array(*[F.when(F.coalesce(F.col(c), F.lit(0)) == safe_max, F.lit(c)) for c in doe_cols]),\n",
    "    lambda x: x.isNotNull()\n",
    ")\n",
    "max_names_set = F.array_sort(F.array_distinct(max_names))\n",
    "\n",
    "# Define coherent cross-pairs for GWAS/coloc DoE ties (order-independent)\n",
    "pair1 = F.array_sort(F.array(F.lit(\"GoF_protect\"), F.lit(\"LoF_risk\")))\n",
    "pair2 = F.array_sort(F.array(F.lit(\"LoF_protect\"), F.lit(\"GoF_risk\")))\n",
    "\n",
    "# GWAS/coloc is comparable if: single maximum OR exactly one of the coherent pairs\n",
    "gwasComparable = (\n",
    "    (F.size(max_names_set) == 1) |\n",
    "    ((F.size(max_names_set) == 2) & ((max_names_set == pair1) | (max_names_set == pair2)))\n",
    ")\n",
    "\n",
    "# Drug is comparable if exactly one of the two protect signals is present\n",
    "has_lof_ch = F.col(\"drugLoF_protect\").isNotNull()\n",
    "has_gof_ch = F.col(\"drugGoF_protect\").isNotNull()\n",
    "drugComparable = (has_lof_ch != has_gof_ch)   # XOR in Spark\n",
    "\n",
    "# Keep your existing drugCoherency label (optional; unchanged)\n",
    "drugCoherencyCol = (\n",
    "    F.when(has_lof_ch & ~has_gof_ch, \"coherent\")\n",
    "     .when(~has_lof_ch & has_gof_ch, \"coherent\")\n",
    "     .when(has_lof_ch & has_gof_ch, \"dispar\")\n",
    "     .otherwise(\"other\")\n",
    ")\n",
    "\n",
    "# Apply the “compare only if both sides are coherent” rule to the flags\n",
    "test2 = (\n",
    "    benchmark.select(*group_keys, \"drugLoF_protect\", \"drugGoF_protect\")\n",
    "    .join(doe_counts, on=group_keys, how=\"left\")\n",
    "    .withColumn(\"gwasComparable\", gwasComparable)\n",
    "    .withColumn(\"drugComparable\", drugComparable)\n",
    "    .withColumn(\n",
    "        \"NoneCellYes\",\n",
    "        F.when(\n",
    "            gwasComparable & drugComparable &\n",
    "            has_lof_ch & F.array_contains(max_names, F.lit(\"LoF_protect\")),\n",
    "            \"yes\"\n",
    "        )\n",
    "        .when(\n",
    "            gwasComparable & drugComparable &\n",
    "            has_gof_ch & F.array_contains(max_names, F.lit(\"GoF_protect\")),\n",
    "            \"yes\"\n",
    "        )\n",
    "        .otherwise(\"no\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"NdiagonalYes\",\n",
    "        F.when(\n",
    "            gwasComparable & drugComparable &\n",
    "            has_lof_ch &\n",
    "            (F.array_contains(max_names, F.lit(\"LoF_protect\")) | F.array_contains(max_names, F.lit(\"GoF_risk\"))),\n",
    "            \"yes\"\n",
    "        )\n",
    "        .when(\n",
    "            gwasComparable & drugComparable &\n",
    "            has_gof_ch &\n",
    "            (F.array_contains(max_names, F.lit(\"GoF_protect\")) | F.array_contains(max_names, F.lit(\"LoF_risk\"))),\n",
    "            \"yes\"\n",
    "        )\n",
    "        .otherwise(\"no\")\n",
    "    )\n",
    "    .withColumn(\"drugCoherency\", drugCoherencyCol)\n",
    "    .withColumn(\n",
    "        \"hasGenetics2\",\n",
    "        F.when(\n",
    "            reduce(lambda acc, c: acc & F.col(c).isNull(), doe_cols[1:], F.col(doe_cols[0]).isNull()),\n",
    "            F.lit(\"no\")\n",
    "        ).otherwise(F.lit(\"yes\"))\n",
    "    )\n",
    "    # If you want hasGenetics to reflect hasGenetics2, set it directly; otherwise keep your placeholder:\n",
    "    .withColumn(\"hasGenetics\", F.col(\"hasGenetics2\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[targetId: string, diseaseId: string, maxClinPhase: double, actionType2: array<string>, biosampleName: string, projectId: string, rightStudyType: string, colocalisationMethod: string, drugLoF_protect: bigint, drugGoF_protect: bigint, LoF_protect: bigint, GoF_risk: bigint, LoF_risk: bigint, GoF_protect: bigint, gwasComparable: boolean, drugComparable: boolean, NoneCellYes: string, NdiagonalYes: string, drugCoherency: string, hasGenetics2: string, hasGenetics: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15854"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.filter(F.col('NoneCellYes')=='yes').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Max DoE names (safe to nulls) + coherency annotation only ---\n",
    "# If you already have `doe_cols` defined:\n",
    "# doe_cols = [\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "\n",
    "# 1) Recompute maxima safely (treat nulls as 0) and collect all tied names\n",
    "safe_max = F.greatest(*[F.coalesce(F.col(c), F.lit(0)) for c in doe_cols])\n",
    "max_names = F.filter(\n",
    "    F.array(*[F.when(F.coalesce(F.col(c), F.lit(0)) == safe_max, F.lit(c)) for c in doe_cols]),\n",
    "    lambda x: x.isNotNull()\n",
    ")\n",
    "\n",
    "# 2) Build a sorted distinct array to compare sets ignoring order\n",
    "max_names_set = F.array_sort(F.array_distinct(max_names))\n",
    "\n",
    "# 3) Define the only coherent cross-pairs\n",
    "pair1 = F.array_sort(F.array(F.lit(\"GoF_protect\"), F.lit(\"LoF_risk\")))\n",
    "pair2 = F.array_sort(F.array(F.lit(\"LoF_protect\"), F.lit(\"GoF_risk\")))\n",
    "\n",
    "# 4) Annotate coherency of DoE maxima (NO filtering, just a label)\n",
    "test2 = (\n",
    "    benchmark.select(*group_keys, \"drugLoF_protect\", \"drugGoF_protect\")\n",
    "    .join(doe_counts, on=group_keys, how=\"left\")\n",
    "    .withColumn(\n",
    "        \"maxDoECoherency\",\n",
    "        F.when(F.size(max_names_set) == 1, F.lit(\"single\"))\n",
    "         .when(\n",
    "             (F.size(max_names_set) == 2) &\n",
    "             ((max_names_set == pair1) | (max_names_set == pair2)),\n",
    "             F.lit(\"coherent\")\n",
    "         )\n",
    "         .when(F.size(max_names_set) >= 2, F.lit(\"incoherent\"))   # includes size 3 or 4\n",
    "         .otherwise(F.lit(\"single\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test2 = (\n",
    "    benchmark.select(*group_keys, \"drugLoF_protect\", \"drugGoF_protect\")\n",
    "    .join(doe_counts, on=group_keys, how=\"left\")\n",
    "    .withColumn(\"NoneCellYes\",\n",
    "        F.when(has_lof_ch & (~has_gof_ch) & F.array_contains(max_names, F.lit(\"LoF_protect\")), \"yes\")\n",
    "         .when(has_gof_ch & (~has_lof_ch) & F.array_contains(max_names, F.lit(\"GoF_protect\")), \"yes\")\n",
    "         .otherwise(\"no\")\n",
    "    )\n",
    "    .withColumn(\"NdiagonalYes\",\n",
    "        F.when(has_lof_ch & (~has_gof_ch) & (F.array_contains(max_names, F.lit(\"LoF_protect\")) | F.array_contains(max_names, F.lit(\"GoF_risk\"))), \"yes\")\n",
    "         .when(has_gof_ch & (~has_lof_ch) & (F.array_contains(max_names, F.lit(\"GoF_protect\")) | F.array_contains(max_names, F.lit(\"LoF_risk\"))), \"yes\")\n",
    "         .otherwise(\"no\")\n",
    "    )\n",
    "    .withColumn(\"drugCoherency\",\n",
    "        F.when(has_lof_ch & ~has_gof_ch, \"coherent\")\n",
    "         .when(~has_lof_ch & has_gof_ch, \"coherent\")\n",
    "         .when(has_lof_ch & has_gof_ch, \"dispar\")\n",
    "         .otherwise(\"other\")\n",
    "    ).withColumn(\n",
    "    \"hasGenetics2\",\n",
    "    F.when(\n",
    "        reduce(lambda acc, c: acc & F.col(c).isNull(), doe_cols[1:], F.col(doe_cols[0]).isNull()),\n",
    "        F.lit(\"no\")\n",
    "    ).otherwise(F.lit(\"yes\"))\n",
    ")\n",
    "    .withColumn(\"hasGenetics\", F.when(F.col(\"NdiagonalYes\").isNotNull(), \"yes\").otherwise(\"no\")) #### we have to change it\n",
    ")\n",
    "test2.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Guard: (re)build agg_once if not defined ----------\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def _build_agg_once_from_test2_and_benchmark(test2_df, benchmark_df):\n",
    "    # Columns we keep across all longified slices\n",
    "    common_cols = [\n",
    "        \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "        \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\",\n",
    "        \"NoneCellYes\",\"NdiagonalYes\",\"hasGenetics2\",\n",
    "        # (optional diagnostics if you want them downstream)\n",
    "        # \"gwasComparable\",\"drugComparable\",\"maxDoECoherency\"\n",
    "    ]\n",
    "\n",
    "    # Join phase flags once (LEFT join is correct here)\n",
    "    phase_flags = (\n",
    "        benchmark_df.select(\n",
    "            \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "            \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\"\n",
    "        ).dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"])\n",
    "    )\n",
    "\n",
    "    # Only rows present in test2 matter for feature/value tall view\n",
    "    t2_with_phase = (\n",
    "        test2_df\n",
    "        .join(phase_flags, on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"right\")\n",
    "        # make flags explicit; null → \"no\"\n",
    "        .withColumn(\"NoneCellYes\",  F.coalesce(F.col(\"NoneCellYes\"),  F.lit(\"no\")))\n",
    "        .withColumn(\"NdiagonalYes\", F.coalesce(F.col(\"NdiagonalYes\"), F.lit(\"no\")))\n",
    "        .withColumn(\"hasGenetics2\", F.coalesce(F.col(\"hasGenetics2\"), F.lit(\"no\")))\n",
    "    )\n",
    "\n",
    "    # actionType2 is ARRAY<STRING> → explode\n",
    "    long_action = (\n",
    "        t2_with_phase\n",
    "        .select(*common_cols, F.explode_outer(\"actionType2\").alias(\"value\"))\n",
    "        .withColumn(\"feature\", F.lit(\"actionType2\"))\n",
    "        .select(*common_cols, \"feature\", \"value\")\n",
    "    )\n",
    "\n",
    "    # helper for scalar columns\n",
    "    def longify_scalar(colname: str):\n",
    "        return (\n",
    "            t2_with_phase\n",
    "            .select(*common_cols, F.col(colname).alias(\"value\"))\n",
    "            .withColumn(\"feature\", F.lit(colname))\n",
    "            .select(*common_cols, \"feature\", \"value\")\n",
    "        )\n",
    "\n",
    "    long_biosample = longify_scalar(\"biosampleName\")\n",
    "    long_project   = longify_scalar(\"projectId\")\n",
    "    long_rstype    = longify_scalar(\"rightStudyType\")\n",
    "    long_colocm    = longify_scalar(\"colocalisationMethod\")\n",
    "\n",
    "    # union into one tall table (drop value=null)\n",
    "    long_features = (\n",
    "        long_action\n",
    "        .unionByName(long_biosample)\n",
    "        .unionByName(long_project)\n",
    "        .unionByName(long_rstype)\n",
    "        .unionByName(long_colocm)\n",
    "        #.filter(F.col(\"value\").isNotNull())\n",
    "    )\n",
    "\n",
    "    # single aggregation to compute flags\n",
    "    agg_once_local = (\n",
    "        long_features\n",
    "        .groupBy(\n",
    "            \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "            \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\",\n",
    "            \"feature\",\"value\"\n",
    "        )\n",
    "        .agg(\n",
    "            F.max(F.when(F.col(\"NoneCellYes\")  == \"yes\", 1).otherwise(0)).alias(\"NoneCellYes\"),\n",
    "            F.max(F.when(F.col(\"NdiagonalYes\") == \"yes\", 1).otherwise(0)).alias(\"NdiagonalYes\"),\n",
    "            F.max(F.when(F.col(\"hasGenetics2\") == \"yes\", 1).otherwise(0)).alias(\"hasGenetics\"),\n",
    "        )\n",
    "        .selectExpr(\n",
    "            \"*\",\n",
    "            \"CASE WHEN NoneCellYes=1  THEN 'yes' ELSE 'no' END as NoneCellYes_flag\",\n",
    "            \"CASE WHEN NdiagonalYes=1 THEN 'yes' ELSE 'no' END as NdiagonalYes_flag\",\n",
    "            \"CASE WHEN hasGenetics=1  THEN 'yes' ELSE 'no' END as hasGenetics_flag\"\n",
    "        )\n",
    "    )\n",
    "    return agg_once_local\n",
    "\n",
    "if 'agg_once' not in globals():\n",
    "    print(\"[info] agg_once not found — rebuilding it from test2/benchmark …\")\n",
    "    agg_once = _build_agg_once_from_test2_and_benchmark(test2, benchmark)\n",
    "    print(\"[info] agg_once rebuilt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:======>        (32 + 8) / 80][Stage 168:=========>   (126 + 8) / 174]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.0][Stage 200:=>(9 + 5) / 17] ]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magg_once\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_125 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_90 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_101 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_57 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_59 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_59 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_79 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_34 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_30 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_157 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_160 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_14 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_34 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_85 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_115 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_91 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_29 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_12 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_40 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_41 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_94 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_64 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_10 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_54 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_5 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_139 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_37 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_54 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_0 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_4 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_122 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_85 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_4 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_46 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_27 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_91 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_25 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_2 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_53 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_45 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_75 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_8 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_168 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_6 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_86 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_97 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_56 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_45 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_12 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_30 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_21 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_31 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_126 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_112 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_8 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_22 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_13 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_103 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_68 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_101 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_27 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_107 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_103 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_80 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_31 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_19 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_162 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_4 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_56 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_108 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_16 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_101 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_14 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_116 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_36 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_119 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_75 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_126 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_74 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_112 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_131 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_154 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_98 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_49 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_55 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_24 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_15 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_8 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_7 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_93 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_116 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_40 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_76 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_121 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_149 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_65 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_100 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_24 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_64 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_105 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_24 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_5 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_13 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_95 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_9 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_124 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_81 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_79 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_147 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_124 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_73 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_52 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_0 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_31 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_114 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_118 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_43 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_26 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_17 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_89 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_39 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_72 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_29 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_7 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_144 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_55 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_59 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_105 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_58 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_70 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_99 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_2 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_122 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_4 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_26 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_14 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_117 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_67 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_47 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_6 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_43 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_172 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_141 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_2 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_161 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_101 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_12 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_65 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_127 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_75 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_84 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_84 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_62 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_3 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_47 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_85 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_28 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_120 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_46 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_62 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_71 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_30 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_40 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_110 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_109 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_19 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_96 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_117 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_10 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_109 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_123 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_26 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_110 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_114 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_118 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_59 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_25 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_163 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_47 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_96 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_113 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_17 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_106 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_75 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_96 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_62 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_48 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_64 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_124 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_60 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_0 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_102 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_60 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_14 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_124 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_86 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_115 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_2 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_27 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_102 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_113 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_113 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_79 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_31 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_93 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_62 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_46 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_50 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_55 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_117 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_89 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_82 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_21 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_77 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_110 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_61 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_128 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_126 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_18 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_52 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_48 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_103 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_102 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_14 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_42 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_55 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_99 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_84 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_171 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_102 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_66 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_143 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_116 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_78 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_38 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_57 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_51 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_92 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_42 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_6 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_78 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_35 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_70 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_55 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_166 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_66 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_33 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_79 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_80 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_77 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_96 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_9 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_134 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_46 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_71 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_39 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_29 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_46 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_59 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_92 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_89 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_44 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_146 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_110 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_57 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_42 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_57 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_18 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_67 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_63 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_30 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_48 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_120 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_78 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_81 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_33 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_12 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_65 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_19 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_113 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_130 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_140 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_84 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_10 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_71 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_50 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_76 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_6 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_41 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_37 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_26 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_109 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_72 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_25 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_120 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_126 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_111 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_48 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_23 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_107 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_12 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_2 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_94 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_71 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_121 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_109 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_53 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_6 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_77 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_120 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_156 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_40 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_82 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_93 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_82 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_67 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_78 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_100 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_127 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_20 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_52 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_103 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_104 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_19 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_29 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_16 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_24 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_155 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_64 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_121 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_60 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_85 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_93 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_38 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_94 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_29 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_47 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_65 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_104 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_57 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_58 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_27 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_4 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_150 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_96 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_22 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_48 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_170 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_27 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_47 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_64 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_43 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_30 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_89 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_136 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_31 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_36 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_10 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_117 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_100 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_3 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_69 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_0 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_44 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_95 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_8 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_40 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_173 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_82 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_35 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_88 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_165 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_25 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_49 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_82 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_89 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_100 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_120 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_52 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_25 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_116 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_67 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_41 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_126 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_61 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_68 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_20 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_121 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_60 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_77 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_0 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_127 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_42 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_41 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_15 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_142 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_24 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_83 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_127 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_10 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_77 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_129 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_125 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_87 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_42 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_133 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_23 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_41 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_11 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_8 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_94 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_28 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_43 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_98 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_51 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_11 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_108 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_110 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_52 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_62 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_1 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_65 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_93 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_43 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_124 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_158 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_100 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_63 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_119 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_101 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_60 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_113 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_69 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_32 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_32 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_26 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_71 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_117 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_267_1 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_152 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_186_19 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_192_116 !\n",
      "25/09/17 13:11:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_40_127 !\n",
      "25/09/17 13:11:26 WARN YarnAllocator: Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "25/09/17 13:11:26 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "25/09/17 13:11:26 ERROR YarnScheduler: Lost executor 1 on jr-temp-doe-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "25/09/17 13:11:26 WARN TaskSetManager: Lost task 29.0 in stage 221.0 (TID 7066) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "25/09/17 13:11:26 WARN TaskSetManager: Lost task 31.0 in stage 221.0 (TID 7069) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "25/09/17 13:11:26 WARN TaskSetManager: Lost task 26.0 in stage 221.0 (TID 7060) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "25/09/17 13:11:26 WARN TaskSetManager: Lost task 27.0 in stage 221.0 (TID 7062) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "25/09/17 13:11:26 WARN TaskSetManager: Lost task 28.0 in stage 221.0 (TID 7065) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "25/09/17 13:11:26 WARN TaskSetManager: Lost task 30.0 in stage 221.0 (TID 7068) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "25/09/17 13:11:26 WARN TaskSetManager: Lost task 25.0 in stage 221.0 (TID 7059) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "25/09/17 13:11:26 WARN TaskSetManager: Lost task 24.0 in stage 221.0 (TID 7058) (jr-temp-doe-m.c.open-targets-eu-dev.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1758107181156_0002_01_000001 on host: jr-temp-doe-m.c.open-targets-eu-dev.internal. Exit status: 137. Diagnostics: [2025-09-17 13:11:25.786]Container killed on request. Exit code is 137\n",
      "[2025-09-17 13:11:25.786]Container exited with a non-zero exit code 137. \n",
      "[2025-09-17 13:11:25.786]Killed by external signal\n",
      ".\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "agg_once.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+------------+--------------------+--------------------+----------+--------------+--------------------+---------------+---------------+-----------+--------+--------+-----------+-----------+------------+-------------+------------+-----------+\n",
      "|       targetId|  diseaseId|maxClinPhase|         actionType2|       biosampleName| projectId|rightStudyType|colocalisationMethod|drugLoF_protect|drugGoF_protect|LoF_protect|GoF_risk|LoF_risk|GoF_protect|NoneCellYes|NdiagonalYes|drugCoherency|hasGenetics2|hasGenetics|\n",
      "+---------------+-----------+------------+--------------------+--------------------+----------+--------------+--------------------+---------------+---------------+-----------+--------+--------+-----------+-----------+------------+-------------+------------+-----------+\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|dorsolateral pref...|CommonMind|          eqtl|             eCAVIAR|           NULL|              1|          0|       0|       0|          0|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|             eCAVIAR|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|             eCAVIAR|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|             eCAVIAR|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|             eCAVIAR|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|             eCAVIAR|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|             eCAVIAR|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|             eCAVIAR|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|             eCAVIAR|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|               COLOC|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|               COLOC|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|               COLOC|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|               COLOC|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|               COLOC|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|               COLOC|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|               COLOC|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|   breast epithelium|      GTEx|          sqtl|               COLOC|           NULL|              1|          0|       0|       0|          8|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|       tibial artery|      GTEx|          eqtl|             eCAVIAR|           NULL|              1|          0|       0|       0|          2|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|       tibial artery|      GTEx|          eqtl|               COLOC|           NULL|              1|          0|       0|       0|          2|        yes|         yes|     coherent|         yes|        yes|\n",
      "|ENSG00000164116|EFO_0000537|         4.0|[POSITIVE ALLOSTE...|       tibial artery|      GTEx|          eqtl|               COLOC|           NULL|              1|          0|       0|       0|          2|        yes|         yes|     coherent|         yes|        yes|\n",
      "+---------------+-----------+------------+--------------------+--------------------+----------+--------------+--------------------+---------------+---------------+-----------+--------+--------+-----------+-----------+------------+-------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2.filter(F.col('NoneCellYes')=='yes').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] agg_once not found — rebuilding it from test2/benchmark …\n",
      "[info] agg_once rebuilt.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- Guard: (re)build agg_once if not defined ----------\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def _build_agg_once_from_test2_and_benchmark(test2, benchmark_df):\n",
    "    # Columns we keep across all longified slices\n",
    "    common_cols = [\n",
    "        \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "        \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\",\n",
    "        \"NoneCellYes\",\"NdiagonalYes\",\"hasGenetics2\"  # note: hasGenetics2 from your test2\n",
    "    ]\n",
    "\n",
    "    # Join phase flags once\n",
    "    phase_flags = (\n",
    "        benchmark_df.select(\n",
    "            \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "            \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\"\n",
    "        ).dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"])\n",
    "    )\n",
    "\n",
    "    t2_with_phase = test2.join(\n",
    "        phase_flags, on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"right\" #### maybe this should be right\n",
    "    )\n",
    "\n",
    "    # actionType2 is ARRAY<STRING> → explode\n",
    "    long_action = (\n",
    "        t2_with_phase\n",
    "        .select(*common_cols, F.explode_outer(\"actionType2\").alias(\"value\"))\n",
    "        .withColumn(\"feature\", F.lit(\"actionType2\"))\n",
    "        .select(*common_cols, \"feature\", \"value\")\n",
    "    )\n",
    "\n",
    "    # helper for scalar columns\n",
    "    def longify_scalar(colname: str):\n",
    "        return (\n",
    "            t2_with_phase\n",
    "            .select(*common_cols, F.col(colname).alias(\"value\"))\n",
    "            .withColumn(\"feature\", F.lit(colname))\n",
    "            .select(*common_cols, \"feature\", \"value\")\n",
    "        )\n",
    "\n",
    "    long_biosample = longify_scalar(\"biosampleName\")\n",
    "    long_project   = longify_scalar(\"projectId\")\n",
    "    long_rstype    = longify_scalar(\"rightStudyType\")\n",
    "    long_colocm    = longify_scalar(\"colocalisationMethod\")\n",
    "\n",
    "    # union into one tall table\n",
    "    long_features = (\n",
    "        long_action\n",
    "        .unionByName(long_biosample)\n",
    "        .unionByName(long_project)\n",
    "        .unionByName(long_rstype)\n",
    "        .unionByName(long_colocm)\n",
    "    )#.filter(F.col(\"value\").isNotNull())\n",
    "\n",
    "    # single aggregation to compute flags\n",
    "    agg_once_local = (\n",
    "        long_features\n",
    "        .groupBy(\n",
    "            \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "            \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\",\n",
    "            \"feature\",\"value\"\n",
    "        )\n",
    "        .agg(\n",
    "            F.max(F.when(F.col(\"NoneCellYes\")==\"yes\", 1).otherwise(0)).alias(\"NoneCellYes\"),\n",
    "            F.max(F.when(F.col(\"NdiagonalYes\")==\"yes\", 1).otherwise(0)).alias(\"NdiagonalYes\"),\n",
    "            F.max(F.when(F.col(\"hasGenetics2\")==\"yes\", 1).otherwise(0)).alias(\"hasGenetics\"),\n",
    "        )\n",
    "        .selectExpr(\n",
    "            \"*\",\n",
    "            \"CASE WHEN NoneCellYes=1 THEN 'yes' ELSE 'no' END as NoneCellYes_flag\",\n",
    "            \"CASE WHEN NdiagonalYes=1 THEN 'yes' ELSE 'no' END as NdiagonalYes_flag\",\n",
    "            \"CASE WHEN hasGenetics=1 THEN 'yes' ELSE 'no' END as hasGenetics_flag\"\n",
    "        )\n",
    "    )\n",
    "    return agg_once_local\n",
    "\n",
    "if 'agg_once' not in globals():\n",
    "    print(\"[info] agg_once not found — rebuilding it from test2/benchmark …\")\n",
    "    agg_once = _build_agg_once_from_test2_and_benchmark(test2, benchmark)\n",
    "    print(\"[info] agg_once rebuilt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 22:01:21 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[targetId: string, diseaseId: string, maxClinPhase: double, Phase>=4: string, Phase>=3: string, Phase>=2: string, Phase>=1: string, PhaseT: string, feature: string, value: string, NoneCellYes: int, NdiagonalYes: int, hasGenetics: int, NoneCellYes_flag: string, NdiagonalYes_flag: string, hasGenetics_flag: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_once.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85938"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_once.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             feature|count|\n",
      "+--------------------+-----+\n",
      "|         actionType2|78109|\n",
      "|           projectId| 2055|\n",
      "|colocalisationMethod|  937|\n",
      "|       biosampleName| 3779|\n",
      "|      rightStudyType| 1058|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_once.groupBy('feature').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================\n",
    "# Denominator = ALL pairs in analysis_chembl_indication (deduped)\n",
    "# Build 2x2 counts using totals, then Fisher via applyInPandas\n",
    "# ============================\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    ")\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "\n",
    "# ---- 0) Universe of pairs & phase flags (only de-dup, no other filtering)\n",
    "universe = (\n",
    "    analysis_chembl_indication\n",
    "    .select(\"targetId\", \"diseaseId\", \"maxClinPhase\")  # dedupe on these\n",
    "    .distinct()\n",
    "    .join(F.broadcast(negativeTD), on=[\"targetId\",\"diseaseId\"], how=\"left\")\n",
    "    .withColumn(\"PhaseT\", F.when(F.col(\"stopReason\")==\"Negative\", \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=4\", F.when((F.col(\"maxClinPhase\")==4) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=3\", F.when((F.col(\"maxClinPhase\")>=3) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=2\", F.when((F.col(\"maxClinPhase\")>=2) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=1\", F.when((F.col(\"maxClinPhase\")>=1) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    ")\n",
    "\n",
    "# Long view of phase flags for universe\n",
    "phases_universe_long = universe.select(\n",
    "    \"targetId\",\"diseaseId\",\n",
    "    F.expr(\"stack(5, \"\n",
    "           \"'Phase>=4', `Phase>=4`, \"\n",
    "           \"'Phase>=3', `Phase>=3`, \"\n",
    "           \"'Phase>=2', `Phase>=2`, \"\n",
    "           \"'Phase>=1', `Phase>=1`, \"\n",
    "           \"'PhaseT',  `PhaseT`\"\n",
    "           \")\").alias(\"phase_name\",\"prediction\")\n",
    ")\n",
    "\n",
    "# Totals per phase (denominator totals)\n",
    "total_pairs_by_phase = (\n",
    "    phases_universe_long\n",
    "    .groupBy(\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"total_pairs\"))\n",
    ")\n",
    "total_pred_yes_by_phase = (\n",
    "    phases_universe_long\n",
    "    .filter(F.col(\"prediction\")==\"yes\")\n",
    "    .groupBy(\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"total_pred_yes\"))\n",
    ")\n",
    "\n",
    "# ---- 1) Build analysis_long from agg_once (flags) + phases (prediction)\n",
    "# metrics we’ll analyze\n",
    "metric_flags = [\"NoneCellYes_flag\", \"NdiagonalYes_flag\", \"hasGenetics_flag\"]\n",
    "\n",
    "# phase flags per (target,disease,maxClinPhase)\n",
    "phase_flags = (\n",
    "    benchmark.select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\")\n",
    "    .dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"])\n",
    ")\n",
    "\n",
    "# stack phases for the records present in agg_once (feature,value specific)\n",
    "phases_long_for_records = (\n",
    "    phase_flags.join(agg_once.select(\"targetId\",\"diseaseId\",\"maxClinPhase\").dropDuplicates(),\n",
    "                     on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"inner\")\n",
    "    .select(\n",
    "        \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "        F.expr(\"stack(5, \"\n",
    "               \"'Phase>=4', `Phase>=4`, \"\n",
    "               \"'Phase>=3', `Phase>=3`, \"\n",
    "               \"'Phase>=2', `Phase>=2`, \"\n",
    "               \"'Phase>=1', `Phase>=1`, \"\n",
    "               \"'PhaseT',  `PhaseT`\"\n",
    "               \")\").alias(\"phase_name\",\"prediction\")\n",
    "    )\n",
    ")\n",
    "\n",
    "def attach_metric(metric_col: str):\n",
    "    # comparison = metric flag yes/no at (target,disease,feature,value)\n",
    "    return (\n",
    "        agg_once.select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"feature\",\"value\",\n",
    "                        F.col(metric_col).alias(\"comparison\"))\n",
    "        .join(phases_long_for_records, on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"inner\")\n",
    "        .withColumn(\"metric\", F.lit(metric_col.replace(\"_flag\",\"\")))  # prettier label\n",
    "    )\n",
    "\n",
    "analysis_long = attach_metric(metric_flags[0])\n",
    "for mc in metric_flags[1:]:\n",
    "    analysis_long = analysis_long.unionByName(attach_metric(mc))\n",
    "\n",
    "# ---- 2) Count distinct pairs for 2x2 components using the fixed universe\n",
    "# a = count of pairs with comparison=='yes' AND prediction=='yes'\n",
    "yes_yes = (\n",
    "    analysis_long\n",
    "    .filter((F.col(\"comparison\")==\"yes\") & (F.col(\"prediction\")==\"yes\"))\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"a\"))\n",
    ")\n",
    "# yes_total = count of pairs with comparison=='yes' (regardless of prediction)\n",
    "yes_total = (\n",
    "    analysis_long\n",
    "    .filter(F.col(\"comparison\")==\"yes\")\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"yes_total\"))\n",
    ")\n",
    "\n",
    "# Assemble b,c,d from totals\n",
    "counts = (\n",
    "    yes_total\n",
    "    .join(yes_yes, on=[\"metric\",\"feature\",\"value\",\"phase_name\"], how=\"left\")\n",
    "    .join(total_pairs_by_phase, on=\"phase_name\", how=\"left\")\n",
    "    .join(total_pred_yes_by_phase, on=\"phase_name\", how=\"left\")\n",
    "    .na.fill({\"a\":0})\n",
    "    .withColumn(\"b\", F.col(\"yes_total\") - F.col(\"a\"))\n",
    "    .withColumn(\"c\", F.col(\"total_pred_yes\") - F.col(\"a\"))\n",
    "    .withColumn(\"d\", F.col(\"total_pairs\") - F.col(\"a\") - F.col(\"b\") - F.col(\"c\"))\n",
    "    .select(\n",
    "        \"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "        F.when(F.col(\"a\")<0,0).otherwise(F.col(\"a\")).cast(\"int\").alias(\"a\"),\n",
    "        F.when(F.col(\"b\")<0,0).otherwise(F.col(\"b\")).cast(\"int\").alias(\"b\"),\n",
    "        F.when(F.col(\"c\")<0,0).otherwise(F.col(\"c\")).cast(\"int\").alias(\"c\"),\n",
    "        F.when(F.col(\"d\")<0,0).otherwise(F.col(\"d\")).cast(\"int\").alias(\"d\"),\n",
    "        \"total_pairs\",\"total_pred_yes\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert to two-row format (comparison yes/no) with columns yes/no → ready for Fisher\n",
    "mat_counts = (\n",
    "    counts\n",
    "    .select(\"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "            F.lit(\"yes\").alias(\"comparison\"),\n",
    "            F.col(\"a\").alias(\"yes\"),\n",
    "            F.col(\"b\").alias(\"no\"))\n",
    "    .unionByName(\n",
    "        counts.select(\"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "                      F.lit(\"no\").alias(\"comparison\"),\n",
    "                      F.col(\"c\").alias(\"yes\"),\n",
    "                      F.col(\"d\").alias(\"no\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Safety: ensure ints and no nulls\n",
    "mat_counts = (\n",
    "    mat_counts.fillna(0)\n",
    "              .withColumn(\"yes\", F.col(\"yes\").cast(\"int\"))\n",
    "              .withColumn(\"no\",  F.col(\"no\").cast(\"int\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LAST ITERATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark session created at 2025-09-17 18:35:42.737737\n",
      "Analysis started on 2025-09-17 at  2025-09-17 18:35:42.737737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 18:35:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/09/17 18:35:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created with:\n",
      "  spark.driver.memory: 24g\n",
      "  spark.executor.memory: 32g\n",
      "  spark.executor.cores: 4\n",
      "  spark.executor.instances: 12\n",
      "  spark.yarn.executor.memoryOverhead: 8g\n",
      "  spark.sql.shuffle.partitions: 192\n",
      "  spark.default.parallelism: 192\n",
      "  spark.sql.adaptive.enabled: true\n",
      "  spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "Spark UI: http://jr-doe-temp1-m.c.open-targets-eu-dev.internal:46761\n",
      "Loaded all base tables.\n",
      "Built newColoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded gwasComplete\n",
      "Built gwasComplete\n",
      "Built resolvedColoc\n",
      "Built temporary DoE datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built analysis_chembl_indication\n",
      "[info] agg_once not found — rebuilding it from test2/benchmark …\n",
      "[info] agg_once rebuilt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# ============================\\n# Denominator = ALL pairs in analysis_chembl_indication (deduped)\\n# Build 2x2 counts using totals, then Fisher via applyInPandas\\n# ============================\\nfrom datetime import date\\nimport pandas as pd\\nimport numpy as np\\nimport pyspark.sql.functions as F\\nfrom pyspark.sql.types import (\\n    StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\\n)\\nfrom scipy.stats import fisher_exact\\nfrom scipy.stats.contingency import odds_ratio\\n\\n# ---- 0) Universe of pairs & phase flags (only de-dup, no other filtering)\\nuniverse = (\\n    analysis_chembl_indication\\n    .select(\"targetId\", \"diseaseId\", \"maxClinPhase\")  # dedupe on these\\n    .distinct()\\n    .join(F.broadcast(negativeTD), on=[\"targetId\",\"diseaseId\"], how=\"left\")\\n    .withColumn(\"PhaseT\", F.when(F.col(\"stopReason\")==\"Negative\", \"yes\").otherwise(\"no\"))\\n    .withColumn(\"Phase>=4\", F.when((F.col(\"maxClinPhase\")==4) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\\n    .withColumn(\"Phase>=3\", F.when((F.col(\"maxClinPhase\")>=3) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\\n    .withColumn(\"Phase>=2\", F.when((F.col(\"maxClinPhase\")>=2) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\\n    .withColumn(\"Phase>=1\", F.when((F.col(\"maxClinPhase\")>=1) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\\n)\\nprint(\\'universe of pairs and phase flags built\\')\\n\\n# Long view of phase flags for universe\\nphases_universe_long = universe.select(\\n    \"targetId\",\"diseaseId\",\\n    F.expr(\"stack(5, \"\\n           \"\\'Phase>=4\\', `Phase>=4`, \"\\n           \"\\'Phase>=3\\', `Phase>=3`, \"\\n           \"\\'Phase>=2\\', `Phase>=2`, \"\\n           \"\\'Phase>=1\\', `Phase>=1`, \"\\n           \"\\'PhaseT\\',  `PhaseT`\"\\n           \")\").alias(\"phase_name\",\"prediction\")\\n)\\nprint(\\'phase_universe_long built\\')\\n\\n# Totals per phase (denominator totals)\\ntotal_pairs_by_phase = (\\n    phases_universe_long\\n    .groupBy(\"phase_name\")\\n    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"total_pairs\"))\\n)\\ntotal_pred_yes_by_phase = (\\n    phases_universe_long\\n    .filter(F.col(\"prediction\")==\"yes\")\\n    .groupBy(\"phase_name\")\\n    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"total_pred_yes\"))\\n)\\nprint(\\'phase_universe_long built\\')\\n\\n# ---- 1) Build analysis_long from agg_once (flags) + phases (prediction)\\n# metrics we’ll analyze\\nmetric_flags = [\"NoneCellYes_flag\", \"NdiagonalYes_flag\", \"hasGenetics_flag\"]\\n\\n# phase flags per (target,disease,maxClinPhase)\\nphase_flags = (\\n    benchmark.select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\")\\n    .dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"])\\n)\\n\\n# stack phases for the records present in agg_once (feature,value specific)\\nphases_long_for_records = (\\n    phase_flags.join(agg_once.select(\"targetId\",\"diseaseId\",\"maxClinPhase\").dropDuplicates(),\\n                     on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"inner\")\\n    .select(\\n        \"targetId\",\"diseaseId\",\"maxClinPhase\",\\n        F.expr(\"stack(5, \"\\n               \"\\'Phase>=4\\', `Phase>=4`, \"\\n               \"\\'Phase>=3\\', `Phase>=3`, \"\\n               \"\\'Phase>=2\\', `Phase>=2`, \"\\n               \"\\'Phase>=1\\', `Phase>=1`, \"\\n               \"\\'PhaseT\\',  `PhaseT`\"\\n               \")\").alias(\"phase_name\",\"prediction\")\\n    )\\n)\\n\\ndef attach_metric(metric_col: str):\\n    # comparison = metric flag yes/no at (target,disease,feature,value)\\n    return (\\n        agg_once.select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"feature\",\"value\",\\n                        F.col(metric_col).alias(\"comparison\"))\\n        .join(phases_long_for_records, on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"inner\")\\n        .withColumn(\"metric\", F.lit(metric_col.replace(\"_flag\",\"\")))  # prettier label\\n    )\\n\\nanalysis_long = attach_metric(metric_flags[0])\\nfor mc in metric_flags[1:]:\\n    analysis_long = analysis_long.unionByName(attach_metric(mc))\\n\\n# ---- 2) Count distinct pairs for 2x2 components using the fixed universe\\n# a = count of pairs with comparison==\\'yes\\' AND prediction==\\'yes\\'\\nyes_yes = (\\n    analysis_long\\n    .filter((F.col(\"comparison\")==\"yes\") & (F.col(\"prediction\")==\"yes\"))\\n    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\\n    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"a\"))\\n)\\n# yes_total = count of pairs with comparison==\\'yes\\' (regardless of prediction)\\nyes_total = (\\n    analysis_long\\n    .filter(F.col(\"comparison\")==\"yes\")\\n    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\\n    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"yes_total\"))\\n)\\n\\n# Assemble b,c,d from totals\\ncounts = (\\n    yes_total\\n    .join(yes_yes, on=[\"metric\",\"feature\",\"value\",\"phase_name\"], how=\"left\")\\n    .join(total_pairs_by_phase, on=\"phase_name\", how=\"left\")\\n    .join(total_pred_yes_by_phase, on=\"phase_name\", how=\"left\")\\n    .na.fill({\"a\":0})\\n    .withColumn(\"b\", F.col(\"yes_total\") - F.col(\"a\"))\\n    .withColumn(\"c\", F.col(\"total_pred_yes\") - F.col(\"a\"))\\n    .withColumn(\"d\", F.col(\"total_pairs\") - F.col(\"a\") - F.col(\"b\") - F.col(\"c\"))\\n    .select(\\n        \"metric\",\"feature\",\"value\",\"phase_name\",\\n        F.when(F.col(\"a\")<0,0).otherwise(F.col(\"a\")).cast(\"int\").alias(\"a\"),\\n        F.when(F.col(\"b\")<0,0).otherwise(F.col(\"b\")).cast(\"int\").alias(\"b\"),\\n        F.when(F.col(\"c\")<0,0).otherwise(F.col(\"c\")).cast(\"int\").alias(\"c\"),\\n        F.when(F.col(\"d\")<0,0).otherwise(F.col(\"d\")).cast(\"int\").alias(\"d\"),\\n        \"total_pairs\",\"total_pred_yes\"\\n    )\\n)\\n\\n# Convert to two-row format (comparison yes/no) with columns yes/no → ready for Fisher\\nmat_counts = (\\n    counts\\n    .select(\"metric\",\"feature\",\"value\",\"phase_name\",\\n            F.lit(\"yes\").alias(\"comparison\"),\\n            F.col(\"a\").alias(\"yes\"),\\n            F.col(\"b\").alias(\"no\"))\\n    .unionByName(\\n        counts.select(\"metric\",\"feature\",\"value\",\"phase_name\",\\n                      F.lit(\"no\").alias(\"comparison\"),\\n                      F.col(\"c\").alias(\"yes\"),\\n                      F.col(\"d\").alias(\"no\"))\\n    )\\n)\\n\\n# Safety: ensure ints and no nulls\\nmat_counts = (\\n    mat_counts.fillna(0)\\n              .withColumn(\"yes\", F.col(\"yes\").cast(\"int\"))\\n              .withColumn(\"no\",  F.col(\"no\").cast(\"int\"))\\n)\\n\\n# ---- 3) Fisher per group with applyInPandas\\nresult_schema = StructType([\\n    StructField(\"group\",        StringType(),  True),\\n    StructField(\"comparison\",   StringType(),  True),\\n    StructField(\"phase\",        StringType(),  True),\\n    StructField(\"oddsRatio\",    DoubleType(),  True),\\n    StructField(\"pValue\",       DoubleType(),  True),\\n    StructField(\"lowerInterval\",DoubleType(),  True),\\n    StructField(\"upperInterval\",DoubleType(),  True),\\n    StructField(\"total\",        StringType(),  True),\\n    StructField(\"values\",       ArrayType(ArrayType(IntegerType())), True),\\n    StructField(\"relSuccess\",   DoubleType(),  True),\\n    StructField(\"rsLower\",      DoubleType(),  True),\\n    StructField(\"rsUpper\",      DoubleType(),  True),\\n    StructField(\"path\",         StringType(),  True),\\n])\\n\\ndef _relative_success(matrix_2x2: np.ndarray):\\n    a, b = matrix_2x2[0,0], matrix_2x2[0,1]\\n    c, d = matrix_2x2[1,0], matrix_2x2[1,1]\\n    rate_yes = a / (a + b) if (a + b) > 0 else 0.0\\n    rate_no  = c / (c + d) if (c + d) > 0 else 0.0\\n    rs = rate_yes - rate_no\\n    import math\\n    se = 0.0\\n    if (a+b) > 0:\\n        se += rate_yes * (1 - rate_yes) / (a + b)\\n    if (c+d) > 0:\\n        se += rate_no  * (1 - rate_no)  / (c + d)\\n    se = math.sqrt(se)\\n    lo, hi = rs - 1.96*se, rs + 1.96*se\\n    return float(rs), float(lo), float(hi)\\n\\ndef fisher_by_group(pdf: pd.DataFrame) -> pd.DataFrame:\\n    sub = pdf[[\"comparison\",\"yes\",\"no\"]].copy()\\n    sub = sub.set_index(\"comparison\").reindex([\"yes\",\"no\"]).fillna(0)\\n    mat = sub[[\"yes\",\"no\"]].to_numpy(dtype=int)\\n\\n    total = int(mat.sum())\\n    if total == 0:\\n        # Return a neutral row to avoid SciPy errors on empty tables\\n        return pd.DataFrame([{\\n            \"group\":        pdf[\"metric\"].iloc[0],\\n            \"comparison\":   f\"{pdf[\\'value\\'].iloc[0]}_only\",\\n            \"phase\":        pdf[\"phase_name\"].iloc[0],\\n            \"oddsRatio\":    float(\"nan\"),\\n            \"pValue\":       float(\"nan\"),\\n            \"lowerInterval\":float(\"nan\"),\\n            \"upperInterval\":float(\"nan\"),\\n            \"total\":        \"0\",\\n            \"values\":       mat.tolist(),\\n            \"relSuccess\":   float(\"nan\"),\\n            \"rsLower\":      float(\"nan\"),\\n            \"rsUpper\":      float(\"nan\"),\\n            \"path\":         \"\",\\n        }])\\n\\n    from scipy.stats import fisher_exact\\n    from scipy.stats.contingency import odds_ratio\\n\\n    or_val, p_val = fisher_exact(mat, alternative=\"two-sided\")\\n    ci = odds_ratio(mat).confidence_interval(0.95)\\n    rs, rs_lo, rs_hi = _relative_success(mat)\\n\\n    return pd.DataFrame([{\\n        \"group\":        pdf[\"metric\"].iloc[0],\\n        \"comparison\":   f\"{pdf[\\'value\\'].iloc[0]}_only\",\\n        \"phase\":        pdf[\"phase_name\"].iloc[0],\\n        \"oddsRatio\":    round(float(or_val), 2),\\n        \"pValue\":       float(p_val),\\n        \"lowerInterval\":round(float(ci[0]), 2),\\n        \"upperInterval\":round(float(ci[1]), 2),\\n        \"total\":        str(total),\\n        \"values\":       mat.tolist(),\\n        \"relSuccess\":   round(float(rs), 2),\\n        \"rsLower\":      round(float(rs_lo), 2),\\n        \"rsUpper\":      round(float(rs_hi), 2),\\n        \"path\":         \"\",\\n    }])\\n\\n# (optional) Arrow for speed\\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\n\\nresults_df = (\\n    mat_counts\\n    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\\n    .applyInPandas(fisher_by_group, schema=result_schema)\\n)\\n\\n# ---- 4) Spreadsheet formatting + annotation + CSV\\nfrom itertools import chain\\nfrom pyspark.sql.functions import create_map\\n\\n# build disdic from agg_once\\ndisdic = {r[\"value\"]: r[\"feature\"] for r in agg_once.select(\"feature\",\"value\").distinct().collect()}\\n\\npatterns = [\"_only\", \"_isRightTissueSignalAgreed\"]\\nregex_pattern = \"(\" + \"|\".join(patterns) + \")\"\\n\\ndf_fmt = (\\n    spreadSheetFormatter(results_df)\\n    .withColumn(\"prefix\", F.regexp_replace(F.col(\"comparison\"), regex_pattern + \".*\", \"\"))\\n    .withColumn(\"suffix\", F.regexp_extract(F.col(\"comparison\"), regex_pattern, 0))\\n)\\n\\nmapping_expr = create_map([F.lit(x) for x in chain(*disdic.items())])\\ndf_annot = df_fmt.withColumn(\"annotation\", mapping_expr.getItem(F.col(\"prefix\")))\\n\\ntoday_date = date.today().isoformat()\\nout_csv = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try.csv\"\\ndf_annot.toPandas().to_csv(out_csv, index=False)\\nprint(f\"Analysis written: {out_csv}\")\\ntoday_date = date.today().isoformat()\\n\\n(out_path, coalesce_n) = (f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try\", 1)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Single-script, loop-free PySpark job (tall/unpivot + single aggregation)\n",
    "\n",
    "import os\n",
    "from datetime import date\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    ")\n",
    "\n",
    "# Your helpers\n",
    "from functions import (\n",
    "    relative_success,\n",
    "    spreadSheetFormatter,\n",
    "    discrepancifier,\n",
    "    temporary_directionOfEffect,\n",
    "    buildColocData,\n",
    "    gwasDataset,\n",
    ")\n",
    "from DoEAssessment import directionOfEffect  # noqa: F401  (kept if you need it later)\n",
    "\n",
    "# -------------------------------\n",
    "# Spark / YARN resource settings (Single-Node Option A)\n",
    "# -------------------------------\n",
    "driver_memory = \"24g\"                 # plenty for planning & small collects\n",
    "\n",
    "executor_cores = 4                    # sweet spot for GC + Python workers\n",
    "num_executors  = 12                   # 12 * 4 = 48 cores for executors; ~16 cores left for driver/OS\n",
    "executor_memory = \"32g\"               # per executor heap\n",
    "executor_memory_overhead = \"8g\"       # ~20% overhead for PySpark/Arrow/off-heap\n",
    "\n",
    "# Totals: (32+8) * 12 = 480 GB executors + 24 GB driver ≈ 504 GB (adjust down if your hard cap is <500 GB)\n",
    "# If you must stay strictly ≤ 500 GB, use executor_memory=\"30g\", overhead=\"6g\"  → (36 * 12) + 24 = 456 + 24 = 480 GB\n",
    "\n",
    "shuffle_partitions   = 192            # ≈ 2–4× total cores (48) → start with 192\n",
    "default_parallelism  = 192\n",
    "\n",
    "# If you later move to a multi-worker cluster, replace the values above.\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MyOptimizedPySparkApp\")\n",
    "    .config(\"spark.master\", \"yarn\")\n",
    "    # core resources\n",
    "    .config(\"spark.driver.memory\", driver_memory)\n",
    "    .config(\"spark.executor.memory\", executor_memory)\n",
    "    .config(\"spark.executor.cores\", executor_cores)\n",
    "    .config(\"spark.executor.instances\", num_executors)\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead)\n",
    "    # shuffle & parallelism\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions)\n",
    "    .config(\"spark.default.parallelism\", default_parallelism)\n",
    "    # adaptive query execution for better skew/partition sizing\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession created with:\")\n",
    "for k in [\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.executor.cores\",\n",
    "    \"spark.executor.instances\",\n",
    "    \"spark.yarn.executor.memoryOverhead\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.default.parallelism\",\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "]:\n",
    "    print(f\"  {k}: {spark.conf.get(k)}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# -------------------------------\n",
    "# Spark / YARN resource settings\n",
    "# -------------------------------\n",
    "driver_memory = \"16g\"\n",
    "executor_memory = \"32g\"\n",
    "executor_cores = \"8\"\n",
    "num_executors = \"16\"\n",
    "executor_memory_overhead = \"8g\"\n",
    "shuffle_partitions = \"150\"\n",
    "default_parallelism = str(int(executor_cores) * int(num_executors) * 2)  # 80\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MyOptimizedPySparkApp\")\n",
    "    .config(\"spark.master\", \"yarn\")\n",
    "    .config(\"spark.driver.memory\", driver_memory)\n",
    "    .config(\"spark.executor.memory\", executor_memory)\n",
    "    .config(\"spark.executor.cores\", executor_cores)\n",
    "    .config(\"spark.executor.instances\", num_executors)\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead)\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions)\n",
    "    .config(\"spark.default.parallelism\", default_parallelism)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession created with:\")\n",
    "for k in [\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.executor.cores\",\n",
    "    \"spark.executor.instances\",\n",
    "    \"spark.yarn.executor.memoryOverhead\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.default.parallelism\",\n",
    "]:\n",
    "    print(f\"  {k}: {spark.conf.get(k)}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "'''\n",
    "# --------------------------------\n",
    "# 0) Load inputs\n",
    "# --------------------------------\n",
    "path_n = \"gs://open-targets-data-releases/25.06/output/\"\n",
    "\n",
    "target = spark.read.parquet(f\"{path_n}target/\")\n",
    "diseases = spark.read.parquet(f\"{path_n}disease/\")\n",
    "evidences = spark.read.parquet(f\"{path_n}evidence\")\n",
    "credible = spark.read.parquet(f\"{path_n}credible_set\")\n",
    "new = spark.read.parquet(f\"{path_n}colocalisation_coloc\")\n",
    "index = spark.read.parquet(f\"{path_n}study/\")\n",
    "variantIndex = spark.read.parquet(f\"{path_n}variant\")\n",
    "biosample = spark.read.parquet(f\"{path_n}biosample\")\n",
    "ecaviar = spark.read.parquet(f\"{path_n}colocalisation_ecaviar\")\n",
    "all_coloc = ecaviar.unionByName(new, allowMissingColumns=True)\n",
    "print(\"Loaded all base tables.\")\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Build coloc + GWAS dataset\n",
    "# --------------------------------\n",
    "newColoc = buildColocData(all_coloc, credible, index)\n",
    "print(\"Built newColoc\")\n",
    "\n",
    "gwasComplete = gwasDataset(evidences, credible)\n",
    "print(\"Built gwasComplete\")\n",
    "\n",
    "resolvedColoc = (\n",
    "    newColoc.withColumnRenamed(\"geneId\", \"targetId\")\n",
    "    .join(\n",
    "        gwasComplete.withColumnRenamed(\"studyLocusId\", \"leftStudyLocusId\"),\n",
    "        on=[\"leftStudyLocusId\", \"targetId\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .join(\n",
    "        diseases.selectExpr(\"id as diseaseId\", \"name\", \"parents\", \"therapeuticAreas\"),\n",
    "        on=\"diseaseId\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"diseaseId\",\n",
    "        F.explode_outer(F.concat(F.array(F.col(\"diseaseId\")), F.col(\"parents\"))),\n",
    "    )\n",
    "    .drop(\"parents\", \"oldDiseaseId\")\n",
    "    .withColumn(\n",
    "        \"colocDoE\",\n",
    "        F.when(\n",
    "            F.col(\"rightStudyType\").isin([\"eqtl\", \"pqtl\", \"tuqtl\", \"sceqtl\", \"sctuqtl\"]),\n",
    "            F.when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"GoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"LoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"LoF_protect\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"GoF_protect\"))\n",
    "        ).when(\n",
    "            F.col(\"rightStudyType\").isin([\"sqtl\", \"scsqtl\"]),\n",
    "            F.when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"LoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"GoF_risk\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0), F.lit(\"GoF_protect\"))\n",
    "            .when((F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0), F.lit(\"LoF_protect\"))\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "print(\"Built resolvedColoc\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Direction of Effect & ChEMBL indication\n",
    "# --------------------------------\n",
    "datasource_filter = [\n",
    "    \"gwas_credible_sets\",\n",
    "    \"gene_burden\",\n",
    "    \"eva\",\n",
    "    \"eva_somatic\",\n",
    "    \"gene2phenotype\",\n",
    "    \"orphanet\",\n",
    "    \"cancer_gene_census\",\n",
    "    \"intogen\",\n",
    "    \"impc\",\n",
    "    \"chembl\",\n",
    "]\n",
    "assessment, evidences, actionType_unused, oncolabel_unused = temporary_directionOfEffect(path_n, datasource_filter)\n",
    "print(\"Built temporary DoE datasets\")\n",
    "\n",
    "# (Optional) Add MoA to ChEMBL paths as in your later code\n",
    "mecact_path = f\"{path_n}drug_mechanism_of_action/\"\n",
    "mecact = spark.read.parquet(mecact_path)\n",
    "actionType = (\n",
    "    mecact.select(\n",
    "        F.explode_outer(\"chemblIds\").alias(\"drugId\"),\n",
    "        \"actionType\",\n",
    "        \"mechanismOfAction\",\n",
    "        \"targets\",\n",
    "    )\n",
    "    .select(\n",
    "        F.explode_outer(\"targets\").alias(\"targetId\"),\n",
    "        \"drugId\",\n",
    "        \"actionType\",\n",
    "        \"mechanismOfAction\",\n",
    "    )\n",
    "    .groupBy(\"targetId\", \"drugId\")\n",
    "    .agg(F.collect_set(\"actionType\").alias(\"actionType2\"))\n",
    "    .withColumn(\"nMoA\", F.size(F.col(\"actionType2\")))\n",
    ")\n",
    "\n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "        .join(actionType, on=[\"targetId\", \"drugId\"], how=\"left\")\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(\"clinicalPhase\").over(Window.partitionBy(\"targetId\", \"diseaseId\")),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\", \"actionType2\")\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    .drop(\"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\")\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    ")\n",
    "print(\"Built analysis_chembl_indication\")\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Benchmark (filtered coloc) + clinical phase flags\n",
    "# --------------------------------\n",
    "resolvedColocFiltered = resolvedColoc.filter((F.col(\"clpp\") >= 0.01) | (F.col(\"h4\") >= 0.8))\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\").count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\")).drop(\"count\")\n",
    ")\n",
    "benchmark = (\n",
    "    resolvedColocFiltered.filter(F.col(\"name\") != \"COVID-19\")\n",
    "    .join(analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\")\n",
    "    .withColumn(\n",
    "        \"AgreeDrug\",\n",
    "        F.when((F.col(\"drugGoF_protect\").isNotNull()) & (F.col(\"colocDoE\") == \"GoF_protect\"), \"yes\")\n",
    "        .when((F.col(\"drugLoF_protect\").isNotNull()) & (F.col(\"colocDoE\") == \"LoF_protect\"), \"yes\")\n",
    "        .otherwise(\"no\"),\n",
    "    )\n",
    "    .join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    ")\n",
    "\n",
    "benchmark = (\n",
    "    benchmark.join(F.broadcast(negativeTD), on=[\"targetId\", \"diseaseId\"], how=\"left\")\n",
    "    .withColumn(\"PhaseT\", F.when(F.col(\"stopReason\") == \"Negative\", \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=4\", F.when((F.col(\"maxClinPhase\") == 4) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=3\", F.when((F.col(\"maxClinPhase\") >= 3) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=2\", F.when((F.col(\"maxClinPhase\") >= 2) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=1\", F.when((F.col(\"maxClinPhase\") >= 1) & (F.col(\"PhaseT\") == \"no\"), \"yes\").otherwise(\"no\"))\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Replace nested loops:\n",
    "#     compute DoE counts once → derive flags → unpivot → single aggregation\n",
    "# --------------------------------\n",
    "doe_cols = [\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "\n",
    "# counts per colocDoE over the grouping you previously used in the loop\n",
    "group_keys = [\n",
    "    \"targetId\", \"diseaseId\", \"maxClinPhase\",\n",
    "    \"actionType2\", \"biosampleName\", \"projectId\", \"rightStudyType\", \"colocalisationMethod\"\n",
    "]\n",
    "\n",
    "doe_counts = (\n",
    "    benchmark.groupBy(*group_keys)\n",
    "    .agg(*[F.sum(F.when(F.col(\"colocDoE\") == c, 1).otherwise(0)).alias(c) for c in doe_cols])\n",
    ")\n",
    "\n",
    "# max name(s) (in case of ties) without arrays of structs\n",
    "greatest_count = F.greatest(*[F.col(c) for c in doe_cols])\n",
    "max_names = F.filter(\n",
    "    F.array(*[F.when(F.col(c) == greatest_count, F.lit(c)) for c in doe_cols]),\n",
    "    lambda x: x.isNotNull()\n",
    ")\n",
    "\n",
    "# presence of drug-side signals (equivalent to *_ch presence in your loop path)\n",
    "has_lof_ch = F.col(\"drugLoF_protect\").isNotNull()\n",
    "has_gof_ch = F.col(\"drugGoF_protect\").isNotNull()\n",
    "\n",
    "test2 = (\n",
    "    benchmark.select(*group_keys, \"drugLoF_protect\", \"drugGoF_protect\")\n",
    "    .join(doe_counts, on=group_keys, how=\"left\")\n",
    "    .withColumn(\"NoneCellYes\",\n",
    "        F.when(has_lof_ch & (~has_gof_ch) & F.array_contains(max_names, F.lit(\"LoF_protect\")), \"yes\")\n",
    "         .when(has_gof_ch & (~has_lof_ch) & F.array_contains(max_names, F.lit(\"GoF_protect\")), \"yes\")\n",
    "         .otherwise(\"no\")\n",
    "    )\n",
    "    .withColumn(\"NdiagonalYes\",\n",
    "        F.when(has_lof_ch & (~has_gof_ch) & (F.array_contains(max_names, F.lit(\"LoF_protect\")) | F.array_contains(max_names, F.lit(\"GoF_risk\"))), \"yes\")\n",
    "         .when(has_gof_ch & (~has_lof_ch) & (F.array_contains(max_names, F.lit(\"GoF_protect\")) | F.array_contains(max_names, F.lit(\"LoF_risk\"))), \"yes\")\n",
    "         .otherwise(\"no\")\n",
    "    )\n",
    "    .withColumn(\"drugCoherency\",\n",
    "        F.when(has_lof_ch & ~has_gof_ch, \"coherent\")\n",
    "         .when(~has_lof_ch & has_gof_ch, \"coherent\")\n",
    "         .when(has_lof_ch & has_gof_ch, \"dispar\")\n",
    "         .otherwise(\"other\")\n",
    "    ).withColumn(\n",
    "    \"hasGenetics2\",\n",
    "    F.when(\n",
    "        reduce(lambda acc, c: acc & F.col(c).isNull(), doe_cols[1:], F.col(doe_cols[0]).isNull()),\n",
    "        F.lit(\"no\")\n",
    "    ).otherwise(F.lit(\"yes\"))\n",
    ")\n",
    "    .withColumn(\"hasGenetics\", F.when(F.col(\"NdiagonalYes\").isNotNull(), \"yes\").otherwise(\"no\")) #### we have to change it\n",
    ")\n",
    "#test2.persist()\n",
    "\n",
    "# ---------- Guard: (re)build agg_once if not defined ----------\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def _build_agg_once_from_test2_and_benchmark(test2, benchmark_df):\n",
    "    # Columns we keep across all longified slices\n",
    "    common_cols = [\n",
    "        \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "        \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\",\n",
    "        \"NoneCellYes\",\"NdiagonalYes\",\"hasGenetics2\"  # note: hasGenetics2 from your test2\n",
    "    ]\n",
    "\n",
    "    # Join phase flags once\n",
    "    phase_flags = (\n",
    "        benchmark_df.select(\n",
    "            \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "            \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\"\n",
    "        ).dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"])\n",
    "    )\n",
    "\n",
    "    t2_with_phase = test2.join(\n",
    "        phase_flags, on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    # actionType2 is ARRAY<STRING> → explode\n",
    "    long_action = (\n",
    "        t2_with_phase\n",
    "        .select(*common_cols, F.explode_outer(\"actionType2\").alias(\"value\"))\n",
    "        .withColumn(\"feature\", F.lit(\"actionType2\"))\n",
    "        .select(*common_cols, \"feature\", \"value\")\n",
    "    )\n",
    "\n",
    "    # helper for scalar columns\n",
    "    def longify_scalar(colname: str):\n",
    "        return (\n",
    "            t2_with_phase\n",
    "            .select(*common_cols, F.col(colname).alias(\"value\"))\n",
    "            .withColumn(\"feature\", F.lit(colname))\n",
    "            .select(*common_cols, \"feature\", \"value\")\n",
    "        )\n",
    "\n",
    "    long_biosample = longify_scalar(\"biosampleName\")\n",
    "    long_project   = longify_scalar(\"projectId\")\n",
    "    long_rstype    = longify_scalar(\"rightStudyType\")\n",
    "    long_colocm    = longify_scalar(\"colocalisationMethod\")\n",
    "\n",
    "    # union into one tall table\n",
    "    long_features = (\n",
    "        long_action\n",
    "        .unionByName(long_biosample)\n",
    "        .unionByName(long_project)\n",
    "        .unionByName(long_rstype)\n",
    "        .unionByName(long_colocm)\n",
    "    ).filter(F.col(\"value\").isNotNull())\n",
    "\n",
    "    # single aggregation to compute flags\n",
    "    agg_once_local = (\n",
    "        long_features\n",
    "        .groupBy(\n",
    "            \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "            \"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\",\n",
    "            \"feature\",\"value\"\n",
    "        )\n",
    "        .agg(\n",
    "            F.max(F.when(F.col(\"NoneCellYes\")==\"yes\", 1).otherwise(0)).alias(\"NoneCellYes\"),\n",
    "            F.max(F.when(F.col(\"NdiagonalYes\")==\"yes\", 1).otherwise(0)).alias(\"NdiagonalYes\"),\n",
    "            F.max(F.when(F.col(\"hasGenetics2\")==\"yes\", 1).otherwise(0)).alias(\"hasGenetics\"),\n",
    "        )\n",
    "        .selectExpr(\n",
    "            \"*\",\n",
    "            \"CASE WHEN NoneCellYes=1 THEN 'yes' ELSE 'no' END as NoneCellYes_flag\",\n",
    "            \"CASE WHEN NdiagonalYes=1 THEN 'yes' ELSE 'no' END as NdiagonalYes_flag\",\n",
    "            \"CASE WHEN hasGenetics=1 THEN 'yes' ELSE 'no' END as hasGenetics_flag\"\n",
    "        )\n",
    "    )\n",
    "    return agg_once_local\n",
    "\n",
    "if 'agg_once' not in globals():\n",
    "    print(\"[info] agg_once not found — rebuilding it from test2/benchmark …\")\n",
    "    agg_once = _build_agg_once_from_test2_and_benchmark(test2, benchmark)\n",
    "    print(\"[info] agg_once rebuilt.\")\n",
    "\n",
    "'''\n",
    "# ============================\n",
    "# Denominator = ALL pairs in analysis_chembl_indication (deduped)\n",
    "# Build 2x2 counts using totals, then Fisher via applyInPandas\n",
    "# ============================\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    ")\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "\n",
    "# ---- 0) Universe of pairs & phase flags (only de-dup, no other filtering)\n",
    "universe = (\n",
    "    analysis_chembl_indication\n",
    "    .select(\"targetId\", \"diseaseId\", \"maxClinPhase\")  # dedupe on these\n",
    "    .distinct()\n",
    "    .join(F.broadcast(negativeTD), on=[\"targetId\",\"diseaseId\"], how=\"left\")\n",
    "    .withColumn(\"PhaseT\", F.when(F.col(\"stopReason\")==\"Negative\", \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=4\", F.when((F.col(\"maxClinPhase\")==4) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=3\", F.when((F.col(\"maxClinPhase\")>=3) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=2\", F.when((F.col(\"maxClinPhase\")>=2) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=1\", F.when((F.col(\"maxClinPhase\")>=1) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    ")\n",
    "print('universe of pairs and phase flags built')\n",
    "\n",
    "# Long view of phase flags for universe\n",
    "phases_universe_long = universe.select(\n",
    "    \"targetId\",\"diseaseId\",\n",
    "    F.expr(\"stack(5, \"\n",
    "           \"'Phase>=4', `Phase>=4`, \"\n",
    "           \"'Phase>=3', `Phase>=3`, \"\n",
    "           \"'Phase>=2', `Phase>=2`, \"\n",
    "           \"'Phase>=1', `Phase>=1`, \"\n",
    "           \"'PhaseT',  `PhaseT`\"\n",
    "           \")\").alias(\"phase_name\",\"prediction\")\n",
    ")\n",
    "print('phase_universe_long built')\n",
    "\n",
    "# Totals per phase (denominator totals)\n",
    "total_pairs_by_phase = (\n",
    "    phases_universe_long\n",
    "    .groupBy(\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"total_pairs\"))\n",
    ")\n",
    "total_pred_yes_by_phase = (\n",
    "    phases_universe_long\n",
    "    .filter(F.col(\"prediction\")==\"yes\")\n",
    "    .groupBy(\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"total_pred_yes\"))\n",
    ")\n",
    "print('phase_universe_long built')\n",
    "\n",
    "# ---- 1) Build analysis_long from agg_once (flags) + phases (prediction)\n",
    "# metrics we’ll analyze\n",
    "metric_flags = [\"NoneCellYes_flag\", \"NdiagonalYes_flag\", \"hasGenetics_flag\"]\n",
    "\n",
    "# phase flags per (target,disease,maxClinPhase)\n",
    "phase_flags = (\n",
    "    benchmark.select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\")\n",
    "    .dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"])\n",
    ")\n",
    "\n",
    "# stack phases for the records present in agg_once (feature,value specific)\n",
    "phases_long_for_records = (\n",
    "    phase_flags.join(agg_once.select(\"targetId\",\"diseaseId\",\"maxClinPhase\").dropDuplicates(),\n",
    "                     on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"inner\")\n",
    "    .select(\n",
    "        \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "        F.expr(\"stack(5, \"\n",
    "               \"'Phase>=4', `Phase>=4`, \"\n",
    "               \"'Phase>=3', `Phase>=3`, \"\n",
    "               \"'Phase>=2', `Phase>=2`, \"\n",
    "               \"'Phase>=1', `Phase>=1`, \"\n",
    "               \"'PhaseT',  `PhaseT`\"\n",
    "               \")\").alias(\"phase_name\",\"prediction\")\n",
    "    )\n",
    ")\n",
    "\n",
    "def attach_metric(metric_col: str):\n",
    "    # comparison = metric flag yes/no at (target,disease,feature,value)\n",
    "    return (\n",
    "        agg_once.select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"feature\",\"value\",\n",
    "                        F.col(metric_col).alias(\"comparison\"))\n",
    "        .join(phases_long_for_records, on=[\"targetId\",\"diseaseId\",\"maxClinPhase\"], how=\"inner\")\n",
    "        .withColumn(\"metric\", F.lit(metric_col.replace(\"_flag\",\"\")))  # prettier label\n",
    "    )\n",
    "\n",
    "analysis_long = attach_metric(metric_flags[0])\n",
    "for mc in metric_flags[1:]:\n",
    "    analysis_long = analysis_long.unionByName(attach_metric(mc))\n",
    "\n",
    "# ---- 2) Count distinct pairs for 2x2 components using the fixed universe\n",
    "# a = count of pairs with comparison=='yes' AND prediction=='yes'\n",
    "yes_yes = (\n",
    "    analysis_long\n",
    "    .filter((F.col(\"comparison\")==\"yes\") & (F.col(\"prediction\")==\"yes\"))\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"a\"))\n",
    ")\n",
    "# yes_total = count of pairs with comparison=='yes' (regardless of prediction)\n",
    "yes_total = (\n",
    "    analysis_long\n",
    "    .filter(F.col(\"comparison\")==\"yes\")\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"yes_total\"))\n",
    ")\n",
    "\n",
    "# Assemble b,c,d from totals\n",
    "counts = (\n",
    "    yes_total\n",
    "    .join(yes_yes, on=[\"metric\",\"feature\",\"value\",\"phase_name\"], how=\"left\")\n",
    "    .join(total_pairs_by_phase, on=\"phase_name\", how=\"left\")\n",
    "    .join(total_pred_yes_by_phase, on=\"phase_name\", how=\"left\")\n",
    "    .na.fill({\"a\":0})\n",
    "    .withColumn(\"b\", F.col(\"yes_total\") - F.col(\"a\"))\n",
    "    .withColumn(\"c\", F.col(\"total_pred_yes\") - F.col(\"a\"))\n",
    "    .withColumn(\"d\", F.col(\"total_pairs\") - F.col(\"a\") - F.col(\"b\") - F.col(\"c\"))\n",
    "    .select(\n",
    "        \"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "        F.when(F.col(\"a\")<0,0).otherwise(F.col(\"a\")).cast(\"int\").alias(\"a\"),\n",
    "        F.when(F.col(\"b\")<0,0).otherwise(F.col(\"b\")).cast(\"int\").alias(\"b\"),\n",
    "        F.when(F.col(\"c\")<0,0).otherwise(F.col(\"c\")).cast(\"int\").alias(\"c\"),\n",
    "        F.when(F.col(\"d\")<0,0).otherwise(F.col(\"d\")).cast(\"int\").alias(\"d\"),\n",
    "        \"total_pairs\",\"total_pred_yes\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert to two-row format (comparison yes/no) with columns yes/no → ready for Fisher\n",
    "mat_counts = (\n",
    "    counts\n",
    "    .select(\"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "            F.lit(\"yes\").alias(\"comparison\"),\n",
    "            F.col(\"a\").alias(\"yes\"),\n",
    "            F.col(\"b\").alias(\"no\"))\n",
    "    .unionByName(\n",
    "        counts.select(\"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "                      F.lit(\"no\").alias(\"comparison\"),\n",
    "                      F.col(\"c\").alias(\"yes\"),\n",
    "                      F.col(\"d\").alias(\"no\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Safety: ensure ints and no nulls\n",
    "mat_counts = (\n",
    "    mat_counts.fillna(0)\n",
    "              .withColumn(\"yes\", F.col(\"yes\").cast(\"int\"))\n",
    "              .withColumn(\"no\",  F.col(\"no\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "# ---- 3) Fisher per group with applyInPandas\n",
    "result_schema = StructType([\n",
    "    StructField(\"group\",        StringType(),  True),\n",
    "    StructField(\"comparison\",   StringType(),  True),\n",
    "    StructField(\"phase\",        StringType(),  True),\n",
    "    StructField(\"oddsRatio\",    DoubleType(),  True),\n",
    "    StructField(\"pValue\",       DoubleType(),  True),\n",
    "    StructField(\"lowerInterval\",DoubleType(),  True),\n",
    "    StructField(\"upperInterval\",DoubleType(),  True),\n",
    "    StructField(\"total\",        StringType(),  True),\n",
    "    StructField(\"values\",       ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"relSuccess\",   DoubleType(),  True),\n",
    "    StructField(\"rsLower\",      DoubleType(),  True),\n",
    "    StructField(\"rsUpper\",      DoubleType(),  True),\n",
    "    StructField(\"path\",         StringType(),  True),\n",
    "])\n",
    "\n",
    "def _relative_success(matrix_2x2: np.ndarray):\n",
    "    a, b = matrix_2x2[0,0], matrix_2x2[0,1]\n",
    "    c, d = matrix_2x2[1,0], matrix_2x2[1,1]\n",
    "    rate_yes = a / (a + b) if (a + b) > 0 else 0.0\n",
    "    rate_no  = c / (c + d) if (c + d) > 0 else 0.0\n",
    "    rs = rate_yes - rate_no\n",
    "    import math\n",
    "    se = 0.0\n",
    "    if (a+b) > 0:\n",
    "        se += rate_yes * (1 - rate_yes) / (a + b)\n",
    "    if (c+d) > 0:\n",
    "        se += rate_no  * (1 - rate_no)  / (c + d)\n",
    "    se = math.sqrt(se)\n",
    "    lo, hi = rs - 1.96*se, rs + 1.96*se\n",
    "    return float(rs), float(lo), float(hi)\n",
    "\n",
    "def fisher_by_group(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    sub = pdf[[\"comparison\",\"yes\",\"no\"]].copy()\n",
    "    sub = sub.set_index(\"comparison\").reindex([\"yes\",\"no\"]).fillna(0)\n",
    "    mat = sub[[\"yes\",\"no\"]].to_numpy(dtype=int)\n",
    "\n",
    "    total = int(mat.sum())\n",
    "    if total == 0:\n",
    "        # Return a neutral row to avoid SciPy errors on empty tables\n",
    "        return pd.DataFrame([{\n",
    "            \"group\":        pdf[\"metric\"].iloc[0],\n",
    "            \"comparison\":   f\"{pdf['value'].iloc[0]}_only\",\n",
    "            \"phase\":        pdf[\"phase_name\"].iloc[0],\n",
    "            \"oddsRatio\":    float(\"nan\"),\n",
    "            \"pValue\":       float(\"nan\"),\n",
    "            \"lowerInterval\":float(\"nan\"),\n",
    "            \"upperInterval\":float(\"nan\"),\n",
    "            \"total\":        \"0\",\n",
    "            \"values\":       mat.tolist(),\n",
    "            \"relSuccess\":   float(\"nan\"),\n",
    "            \"rsLower\":      float(\"nan\"),\n",
    "            \"rsUpper\":      float(\"nan\"),\n",
    "            \"path\":         \"\",\n",
    "        }])\n",
    "\n",
    "    from scipy.stats import fisher_exact\n",
    "    from scipy.stats.contingency import odds_ratio\n",
    "\n",
    "    or_val, p_val = fisher_exact(mat, alternative=\"two-sided\")\n",
    "    ci = odds_ratio(mat).confidence_interval(0.95)\n",
    "    rs, rs_lo, rs_hi = _relative_success(mat)\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"group\":        pdf[\"metric\"].iloc[0],\n",
    "        \"comparison\":   f\"{pdf['value'].iloc[0]}_only\",\n",
    "        \"phase\":        pdf[\"phase_name\"].iloc[0],\n",
    "        \"oddsRatio\":    round(float(or_val), 2),\n",
    "        \"pValue\":       float(p_val),\n",
    "        \"lowerInterval\":round(float(ci[0]), 2),\n",
    "        \"upperInterval\":round(float(ci[1]), 2),\n",
    "        \"total\":        str(total),\n",
    "        \"values\":       mat.tolist(),\n",
    "        \"relSuccess\":   round(float(rs), 2),\n",
    "        \"rsLower\":      round(float(rs_lo), 2),\n",
    "        \"rsUpper\":      round(float(rs_hi), 2),\n",
    "        \"path\":         \"\",\n",
    "    }])\n",
    "\n",
    "# (optional) Arrow for speed\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "results_df = (\n",
    "    mat_counts\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .applyInPandas(fisher_by_group, schema=result_schema)\n",
    ")\n",
    "\n",
    "# ---- 4) Spreadsheet formatting + annotation + CSV\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# build disdic from agg_once\n",
    "disdic = {r[\"value\"]: r[\"feature\"] for r in agg_once.select(\"feature\",\"value\").distinct().collect()}\n",
    "\n",
    "patterns = [\"_only\", \"_isRightTissueSignalAgreed\"]\n",
    "regex_pattern = \"(\" + \"|\".join(patterns) + \")\"\n",
    "\n",
    "df_fmt = (\n",
    "    spreadSheetFormatter(results_df)\n",
    "    .withColumn(\"prefix\", F.regexp_replace(F.col(\"comparison\"), regex_pattern + \".*\", \"\"))\n",
    "    .withColumn(\"suffix\", F.regexp_extract(F.col(\"comparison\"), regex_pattern, 0))\n",
    ")\n",
    "\n",
    "mapping_expr = create_map([F.lit(x) for x in chain(*disdic.items())])\n",
    "df_annot = df_fmt.withColumn(\"annotation\", mapping_expr.getItem(F.col(\"prefix\")))\n",
    "\n",
    "today_date = date.today().isoformat()\n",
    "out_csv = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try.csv\"\n",
    "df_annot.toPandas().to_csv(out_csv, index=False)\n",
    "print(f\"Analysis written: {out_csv}\")\n",
    "today_date = date.today().isoformat()\n",
    "\n",
    "(out_path, coalesce_n) = (f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try\", 1)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 18:40:42 ERROR AsyncEventQueue: Dropping event from queue appStatus. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.\n",
      "25/09/17 18:40:42 WARN AsyncEventQueue: Dropped 1 events from appStatus since the application started.\n",
      "ERROR:root:Exception while sending command.+ 0) / 192][Stage 102:(127 + 3) / 192]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o2521.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 245\u001b[0m\n\u001b[1;32m    239\u001b[0m csv_path     \u001b[38;5;241m=\u001b[39m base \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Stage to parquet (break lineage)\u001b[39;00m\n\u001b[1;32m    242\u001b[0m (\u001b[43mresults_df\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# smallish files, even spread\u001b[39;49;00m\n\u001b[1;32m    244\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 245\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Read back and spill to CSV shards\u001b[39;00m\n\u001b[1;32m    248\u001b[0m prev \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.adaptive.coalescePartitions.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o2521.parquet"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================\n",
    "# Simple, single-node Fisher pipeline\n",
    "# Denominator = ALL (target,disease,maxClinPhase) in analysis_chembl_indication\n",
    "# ============================\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    ")\n",
    "\n",
    "# ---- 0) Universe of pairs & phase flags (dedupe only; no extra filtering)\n",
    "universe = (\n",
    "    analysis_chembl_indication\n",
    "    .select(\"targetId\", \"diseaseId\", \"maxClinPhase\")\n",
    "    .distinct()\n",
    "    .join(negativeTD.select(\"targetId\",\"diseaseId\",\"stopReason\"), [\"targetId\",\"diseaseId\"], \"left\")\n",
    "    .withColumn(\"PhaseT\",  F.when(F.col(\"stopReason\")==\"Negative\", \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=4\",F.when((F.col(\"maxClinPhase\")==4) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=3\",F.when((F.col(\"maxClinPhase\")>=3) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=2\",F.when((F.col(\"maxClinPhase\")>=2) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    "    .withColumn(\"Phase>=1\",F.when((F.col(\"maxClinPhase\")>=1) & (F.col(\"PhaseT\")==\"no\"), \"yes\").otherwise(\"no\"))\n",
    ")\n",
    "\n",
    "# Long view of phase flags for universe (used for denominators)\n",
    "phases_universe_long = universe.select(\n",
    "    \"targetId\",\"diseaseId\",\n",
    "    F.expr(\n",
    "      \"stack(5,\"\n",
    "      \" 'Phase>=4', `Phase>=4`,\"\n",
    "      \" 'Phase>=3', `Phase>=3`,\"\n",
    "      \" 'Phase>=2', `Phase>=2`,\"\n",
    "      \" 'Phase>=1', `Phase>=1`,\"\n",
    "      \" 'PhaseT',  `PhaseT`)\"\n",
    "    ).alias(\"phase_name\",\"prediction\")\n",
    ")\n",
    "\n",
    "total_pairs_by_phase = (\n",
    "    phases_universe_long\n",
    "    .groupBy(\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"total_pairs\"))\n",
    ")\n",
    "total_pred_yes_by_phase = (\n",
    "    phases_universe_long\n",
    "    .filter(F.col(\"prediction\")==\"yes\")\n",
    "    .groupBy(\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"total_pred_yes\"))\n",
    ")\n",
    "\n",
    "# ---- 1) Build analysis_long = (metric flag yes/no) × (phase yes/no)\n",
    "# We assume 'agg_once' exists with flags per (targetId,diseaseId,maxClinPhase,feature,value)\n",
    "metric_flags = [\"NoneCellYes_flag\", \"NdiagonalYes_flag\", \"hasGenetics_flag\"]\n",
    "\n",
    "phase_flags = (\n",
    "    benchmark\n",
    "    .select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"Phase>=4\",\"Phase>=3\",\"Phase>=2\",\"Phase>=1\",\"PhaseT\")\n",
    "    .dropDuplicates([\"targetId\",\"diseaseId\",\"maxClinPhase\"])\n",
    ")\n",
    "\n",
    "phases_long_for_records = (\n",
    "    phase_flags\n",
    "    .join(agg_once.select(\"targetId\",\"diseaseId\",\"maxClinPhase\").dropDuplicates(),\n",
    "          [\"targetId\",\"diseaseId\",\"maxClinPhase\"], \"inner\")\n",
    "    .select(\n",
    "        \"targetId\",\"diseaseId\",\"maxClinPhase\",\n",
    "        F.expr(\n",
    "          \"stack(5,\"\n",
    "          \" 'Phase>=4', `Phase>=4`,\"\n",
    "          \" 'Phase>=3', `Phase>=3`,\"\n",
    "          \" 'Phase>=2', `Phase>=2`,\"\n",
    "          \" 'Phase>=1', `Phase>=1`,\"\n",
    "          \" 'PhaseT',  `PhaseT`)\"\n",
    "        ).alias(\"phase_name\",\"prediction\")\n",
    "    )\n",
    ")\n",
    "\n",
    "def attach_metric(metric_col: str):\n",
    "    return (\n",
    "        agg_once\n",
    "        .select(\"targetId\",\"diseaseId\",\"maxClinPhase\",\"feature\",\"value\",\n",
    "                F.col(metric_col).alias(\"comparison\"))\n",
    "        .join(phases_long_for_records, [\"targetId\",\"diseaseId\",\"maxClinPhase\"], \"inner\")\n",
    "        .withColumn(\"metric\", F.lit(metric_col.replace(\"_flag\",\"\")))\n",
    "    )\n",
    "\n",
    "analysis_long = attach_metric(metric_flags[0])\n",
    "for m in metric_flags[1:]:\n",
    "    analysis_long = analysis_long.unionByName(attach_metric(m))\n",
    "\n",
    "# ---- 2) Build 2×2 counts with the fixed denominators\n",
    "# a = yes/yes, b = yes/no, c = no/yes, d = no/no\n",
    "yes_yes = (\n",
    "    analysis_long\n",
    "    .filter((F.col(\"comparison\")==\"yes\") & (F.col(\"prediction\")==\"yes\"))\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"a\"))\n",
    ")\n",
    "yes_total = (\n",
    "    analysis_long\n",
    "    .filter(F.col(\"comparison\")==\"yes\")\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .agg(F.countDistinct(F.struct(\"targetId\",\"diseaseId\")).alias(\"yes_total\"))\n",
    ")\n",
    "\n",
    "counts = (\n",
    "    yes_total\n",
    "    .join(yes_yes, [\"metric\",\"feature\",\"value\",\"phase_name\"], \"left\")\n",
    "    .join(total_pairs_by_phase, [\"phase_name\"], \"left\")\n",
    "    .join(total_pred_yes_by_phase, [\"phase_name\"], \"left\")\n",
    "    .na.fill({\"a\":0})\n",
    "    .withColumn(\"b\", F.col(\"yes_total\") - F.col(\"a\"))\n",
    "    .withColumn(\"c\", F.col(\"total_pred_yes\") - F.col(\"a\"))\n",
    "    .withColumn(\"d\", F.col(\"total_pairs\") - F.col(\"a\") - F.col(\"b\") - F.col(\"c\"))\n",
    "    .select(\n",
    "        \"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "        F.greatest(F.lit(0), F.col(\"a\")).cast(\"int\").alias(\"a\"),\n",
    "        F.greatest(F.lit(0), F.col(\"b\")).cast(\"int\").alias(\"b\"),\n",
    "        F.greatest(F.lit(0), F.col(\"c\")).cast(\"int\").alias(\"c\"),\n",
    "        F.greatest(F.lit(0), F.col(\"d\")).cast(\"int\").alias(\"d\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert to two-row format (comparison yes/no) for applyInPandas\n",
    "mat_counts = (\n",
    "    counts\n",
    "    .select(\"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "            F.lit(\"yes\").alias(\"comparison\"),\n",
    "            F.col(\"a\").alias(\"yes\"),\n",
    "            F.col(\"b\").alias(\"no\"))\n",
    "    .unionByName(\n",
    "        counts.select(\"metric\",\"feature\",\"value\",\"phase_name\",\n",
    "                      F.lit(\"no\").alias(\"comparison\"),\n",
    "                      F.col(\"c\").alias(\"yes\"),\n",
    "                      F.col(\"d\").alias(\"no\"))\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .withColumn(\"yes\", F.col(\"yes\").cast(\"int\"))\n",
    "    .withColumn(\"no\",  F.col(\"no\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "# ---- 3) Fisher per (metric,feature,value,phase)\n",
    "result_schema = StructType([\n",
    "    StructField(\"group\",        StringType(),  True),\n",
    "    StructField(\"comparison\",   StringType(),  True),\n",
    "    StructField(\"phase\",        StringType(),  True),\n",
    "    StructField(\"oddsRatio\",    DoubleType(),  True),\n",
    "    StructField(\"pValue\",       DoubleType(),  True),\n",
    "    StructField(\"lowerInterval\",DoubleType(),  True),\n",
    "    StructField(\"upperInterval\",DoubleType(),  True),\n",
    "    StructField(\"total\",        StringType(),  True),\n",
    "    StructField(\"values\",       ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"relSuccess\",   DoubleType(),  True),\n",
    "    StructField(\"rsLower\",      DoubleType(),  True),\n",
    "    StructField(\"rsUpper\",      DoubleType(),  True),\n",
    "    StructField(\"path\",         StringType(),  True),\n",
    "])\n",
    "\n",
    "def _relative_success(mat: np.ndarray):\n",
    "    a, b = mat[0,0], mat[0,1]\n",
    "    c, d = mat[1,0], mat[1,1]\n",
    "    rate_yes = a / (a + b) if (a + b) > 0 else 0.0\n",
    "    rate_no  = c / (c + d) if (c + d) > 0 else 0.0\n",
    "    rs = rate_yes - rate_no\n",
    "    # simple Wald CI\n",
    "    import math\n",
    "    se = 0.0\n",
    "    if (a+b) > 0:\n",
    "        se += rate_yes * (1 - rate_yes) / (a + b)\n",
    "    if (c+d) > 0:\n",
    "        se += rate_no  * (1 - rate_no)  / (c + d)\n",
    "    se = math.sqrt(se)\n",
    "    lo, hi = rs - 1.96*se, rs + 1.96*se\n",
    "    return float(rs), float(lo), float(hi)\n",
    "\n",
    "def fisher_by_group(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    sub = pdf[[\"comparison\",\"yes\",\"no\"]].copy()\n",
    "    sub = sub.set_index(\"comparison\").reindex([\"yes\",\"no\"]).fillna(0)\n",
    "    mat = sub[[\"yes\",\"no\"]].to_numpy(dtype=int)\n",
    "    total = int(mat.sum())\n",
    "\n",
    "    if total == 0:\n",
    "        return pd.DataFrame([{\n",
    "            \"group\":      pdf[\"metric\"].iloc[0],\n",
    "            \"comparison\": f\"{pdf['value'].iloc[0]}_only\",\n",
    "            \"phase\":      pdf[\"phase_name\"].iloc[0],\n",
    "            \"oddsRatio\":  float(\"nan\"),\n",
    "            \"pValue\":     float(\"nan\"),\n",
    "            \"lowerInterval\": float(\"nan\"),\n",
    "            \"upperInterval\": float(\"nan\"),\n",
    "            \"total\":      \"0\",\n",
    "            \"values\":     mat.tolist(),\n",
    "            \"relSuccess\": float(\"nan\"),\n",
    "            \"rsLower\":    float(\"nan\"),\n",
    "            \"rsUpper\":    float(\"nan\"),\n",
    "            \"path\":       \"\",\n",
    "        }])\n",
    "\n",
    "    from scipy.stats import fisher_exact\n",
    "    from scipy.stats.contingency import odds_ratio\n",
    "\n",
    "    or_val, p_val = fisher_exact(mat, alternative=\"two-sided\")\n",
    "    ci = odds_ratio(mat).confidence_interval(0.95)\n",
    "    rs, rs_lo, rs_hi = _relative_success(mat)\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"group\":        pdf[\"metric\"].iloc[0],\n",
    "        \"comparison\":   f\"{pdf['value'].iloc[0]}_only\",\n",
    "        \"phase\":        pdf[\"phase_name\"].iloc[0],\n",
    "        \"oddsRatio\":    round(float(or_val), 2),\n",
    "        \"pValue\":       float(p_val),\n",
    "        \"lowerInterval\":round(float(ci[0]), 2),\n",
    "        \"upperInterval\":round(float(ci[1]), 2),\n",
    "        \"total\":        str(total),\n",
    "        \"values\":       mat.tolist(),\n",
    "        \"relSuccess\":   round(float(rs), 2),\n",
    "        \"rsLower\":      round(float(rs_lo), 2),\n",
    "        \"rsUpper\":      round(float(rs_hi), 2),\n",
    "        \"path\":         \"\",\n",
    "    }])\n",
    "\n",
    "# Keep Arrow on; limit batch size for stability on one node\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"20000\")\n",
    "\n",
    "results_df = (\n",
    "    mat_counts\n",
    "    .repartition(96, \"metric\", \"phase_name\")  # moderate parallelism on single node\n",
    "    .groupBy(\"metric\",\"feature\",\"value\",\"phase_name\")\n",
    "    .applyInPandas(fisher_by_group, schema=result_schema)\n",
    ")\n",
    "\n",
    "# ---- 4) Write out (Parquet → CSV shards); no toPandas\n",
    "today_date = date.today().isoformat()\n",
    "base = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_denominator_allPairs\"\n",
    "\n",
    "parquet_path = base + \"_parquet\"\n",
    "csv_path     = base + \"_csv\"\n",
    "\n",
    "# Stage to parquet (break lineage)\n",
    "'''\n",
    "(results_df\n",
    "  .repartition(96, \"group\", \"phase\")   # smallish files, even spread\n",
    "  .write.mode(\"overwrite\")\n",
    "  .parquet(parquet_path))\n",
    "\n",
    "# Read back and spill to CSV shards\n",
    "prev = spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n",
    "(spark.read.parquet(parquet_path)\n",
    "  .repartition(96, \"group\", \"phase\")\n",
    "  .write.mode(\"overwrite\")\n",
    "  .option(\"header\",\"true\")\n",
    "  .option(\"maxRecordsPerFile\",\"250000\")\n",
    "  .csv(csv_path))\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", prev)\n",
    "'''\n",
    "print(f\"Wrote results to:\\n  Parquet: {parquet_path}\\n  CSV:     {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 17:35:09 ERROR AsyncEventQueue: Dropping event from queue appStatus. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.\n",
      "25/09/17 17:35:09 WARN AsyncEventQueue: Dropped 1 events from appStatus since the application started.\n",
      "ERROR:root:Exception while sending command.+ 2) / 192][Stage 125:(182 + 2) / 192]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/pandas/conversion.py\", line 280, in _collect_as_arrow\n",
      "    results = list(batch_stream)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/pandas/serializers.py\", line 69, in load_stream\n",
      "    for batch in self.serializer.load_stream(stream):\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/pandas/serializers.py\", line 111, in load_stream\n",
      "    reader = pa.ipc.open_stream(stream)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/pyarrow/ipc.py\", line 190, in open_stream\n",
      "    return RecordBatchStreamReader(source, options=options,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/pyarrow/ipc.py\", line 52, in __init__\n",
      "    self._open(source, options=options, memory_pool=memory_pool)\n",
      "  File \"pyarrow/ipc.pxi\", line 929, in pyarrow.lib._RecordBatchStreamReader._open\n",
      "  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "/usr/lib/spark/python/pyspark/sql/pandas/conversion.py:198: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\n",
      "  An error occurred while calling o2980.getResult\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o2980.getResult",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/pandas/conversion.py:280\u001b[0m, in \u001b[0;36mPandasConversionMixin._collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/pandas/serializers.py:69\u001b[0m, in \u001b[0;36mArrowCollectSerializer.load_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# load the batches\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/pandas/serializers.py:111\u001b[0m, in \u001b[0;36mArrowStreamSerializer.load_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m reader:\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/pyarrow/ipc.py:190\u001b[0m, in \u001b[0;36mopen_stream\u001b[0;34m(source, options, memory_pool)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mCreate reader for Arrow streaming format.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    A reader for the given source\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRecordBatchStreamReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/pyarrow/ipc.py:52\u001b[0m, in \u001b[0;36mRecordBatchStreamReader.__init__\u001b[0;34m(self, source, options, memory_pool)\u001b[0m\n\u001b[1;32m     51\u001b[0m options \u001b[38;5;241m=\u001b[39m _ensure_default_ipc_read_options(options)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/pyarrow/ipc.pxi:929\u001b[0m, in \u001b[0;36mpyarrow.lib._RecordBatchStreamReader._open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Tried reading schema message, was null or length 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m today_date \u001b[38;5;241m=\u001b[39m date\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39misoformat()\n\u001b[1;32m      2\u001b[0m out_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://ot-team/jroldan/analysis/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf_annot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(out_csv, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis written: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/pandas/conversion.py:131\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\n\u001b[1;32m    130\u001b[0m self_destruct \u001b[38;5;241m=\u001b[39m jconf\u001b[38;5;241m.\u001b[39marrowPySparkSelfDestructEnabled()\n\u001b[0;32m--> 131\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_as_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_destruct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    133\u001b[0m     table \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(batches)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/pandas/conversion.py:284\u001b[0m, in \u001b[0;36mPandasConversionMixin._collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_spark_exception():\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;66;03m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m         \u001b[43mjsocket_auth_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Separate RecordBatches from batch order indices in results\u001b[39;00m\n\u001b[1;32m    287\u001b[0m batches \u001b[38;5;241m=\u001b[39m results[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o2980.getResult"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "today_date = date.today().isoformat()\n",
    "out_csv = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try.csv\"\n",
    "df_annot.toPandas().to_csv(out_csv, index=False)\n",
    "print(f\"Analysis written: {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing df_annot to write\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 17:23:09 ERROR AsyncEventQueue: Dropping event from queue appStatus. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.\n",
      "25/09/17 17:23:09 WARN AsyncEventQueue: Dropped 1 events from appStatus since the application started.\n",
      "ERROR:root:KeyboardInterrupt while sending command.28][Stage 137:(126 + -1) / 128]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m (out_path, coalesce_n) \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://ot-team/jroldan/analysis/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreparing df_annot to write\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m (\u001b[43mdf_annot\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoalesce_n\u001b[49m\u001b[43m)\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# 1 file if feasible; increase if OOM on shuffle\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrote CSV shards to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "(out_path, coalesce_n) = (f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try\", 1)\n",
    "print('preparing df_annot to write')\n",
    "(df_annot\n",
    "  .coalesce(coalesce_n)                 # 1 file if feasible; increase if OOM on shuffle\n",
    "  .write.mode(\"overwrite\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(out_path))\n",
    "\n",
    "print(f\"Wrote CSV shards to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 317:> (9 + 1) / 80][Stage 318:(113 + 1) / 174][Stage 319:> (8 + 1) / 80]]]\r"
     ]
    }
   ],
   "source": [
    "prev = spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n",
    "\n",
    "\n",
    "out_path    = f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_try\"\n",
    "parquet_path = out_path + \"_parquet\"\n",
    "\n",
    "writer_parts = 64   # try 48/64/80 depending on size\n",
    "\n",
    "# Stage 1: write Parquet\n",
    "(df_annot\n",
    "  #.repartition(writer_parts, \"metric\", \"phase\")   # pick keys you have; omit if not present\n",
    "  .write.mode(\"overwrite\")\n",
    "  .parquet(parquet_path))\n",
    "\n",
    "# Free lineage\n",
    "df_annot.unpersist()\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Stage 2: read Parquet back and write CSV shards (no coalesce(1)!)\n",
    "(spark.read.parquet(parquet_path)\n",
    "  .repartition(writer_parts, \"metric\", \"phase\")\n",
    "  .write.mode(\"overwrite\")\n",
    "  .option(\"header\",\"true\")\n",
    "  .option(\"maxRecordsPerFile\",\"200000\")   # smaller chunks → lower peak memory\n",
    "  .option(\"compression\",\"gzip\")           # optional; reduces IO/space\n",
    "  .csv(out_path))\n",
    "\n",
    "# Restore AQE setting\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", prev)\n",
    "\n",
    "print(f\"Wrote CSV shards under: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### compare with the code we already had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark session created at 2025-09-17 20:24:13.866899\n",
      "Analysis started on 2025-09-17 at  2025-09-17 20:24:13.866899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 20:24:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint groups\n",
      "loaded files\n",
      "loaded newColoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded gwasComplete\n",
      "loaded resolvedColloc\n",
      "run temporary direction of effect\n",
      "built drugApproved dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load comparisons_df_iterative function\n",
      "created full_data and lists\n",
      "loaded rightTissue dataset\n",
      "built negativeTD dataset\n",
      "built bench2 dataset\n",
      "looping for variables_study\n",
      "built benchmark dataset\n",
      "Collecting combined distinct values from the cluster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final disdic: {'HipSci': 'projectId', 'van_de_Bunt_2015': 'projectId', 'GTEx': 'projectId', 'Lepik_2017': 'projectId', 'Bossini-Castillo_2019': 'projectId', 'ROSMAP': 'projectId', 'BLUEPRINT': 'projectId', 'TwinsUK': 'projectId', 'FUSION': 'projectId', 'Cytoimmgen': 'projectId', 'Gilchrist_2021': 'projectId', 'PhLiPS': 'projectId', 'Fairfax_2014': 'projectId', 'BrainSeq': 'projectId', 'GEUVADIS': 'projectId', 'Kim-Hellmuth_2017': 'projectId', 'Schmiedel_2018': 'projectId', 'Peng_2018': 'projectId', 'CEDAR': 'projectId', 'Nathan_2022': 'projectId', 'UKB_PPP_EUR': 'projectId', 'Quach_2016': 'projectId', 'iPSCORE': 'projectId', 'Jerber_2021': 'projectId', 'Alasoo_2018': 'projectId', 'Perez_2022': 'projectId', 'CommonMind': 'projectId', 'CAP': 'projectId', 'Walker_2019': 'projectId', 'GENCORD': 'projectId', 'Nedelec_2016': 'projectId', 'Steinberg_2020': 'projectId', 'OneK1K': 'projectId', 'Fairfax_2012': 'projectId', 'Aygun_2021': 'projectId', 'Schwartzentruber_2018': 'projectId', 'Kasela_2017': 'projectId', 'PISA': 'projectId', 'Braineac2': 'projectId', 'Randolph_2021': 'projectId', 'Naranbhai_2015': 'projectId', 'Sun_2018': 'projectId', 'Young_2019': 'projectId'}\n",
      "Pre-computing 'hasboth' column...\n",
      "Processing pivot for: projectId\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from array import ArrayType\n",
    "from functions import (\n",
    "    relative_success,\n",
    "    spreadSheetFormatter,\n",
    "    discrepancifier,\n",
    "    temporary_directionOfEffect,\n",
    "    buildColocData,\n",
    "    gwasDataset,\n",
    ")\n",
    "# from stoppedTrials import terminated_td\n",
    "from DoEAssessment import directionOfEffect\n",
    "# from membraneTargets import target_membrane\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    DecimalType,\n",
    "    StringType,\n",
    "    FloatType,\n",
    ")\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\n",
    "    \"spark.sql.shuffle.partitions\", \"400\"\n",
    ")  # Default is 200, increase if needed\n",
    "\n",
    "print('joint groups')\n",
    "path_n='gs://open-targets-data-releases/25.06/output/'\n",
    "\n",
    "target = spark.read.parquet(f\"{path_n}target/\")\n",
    "\n",
    "diseases = spark.read.parquet(f\"{path_n}disease/\")\n",
    "\n",
    "evidences = spark.read.parquet(f\"{path_n}evidence\")\n",
    "\n",
    "credible = spark.read.parquet(f\"{path_n}credible_set\")\n",
    "\n",
    "new = spark.read.parquet(f\"{path_n}colocalisation_coloc\") \n",
    "\n",
    "index=spark.read.parquet(f\"{path_n}study/\")\n",
    "\n",
    "variantIndex = spark.read.parquet(f\"{path_n}variant\")\n",
    "\n",
    "biosample = spark.read.parquet(f\"{path_n}biosample\")\n",
    "\n",
    "ecaviar=spark.read.parquet(f\"{path_n}colocalisation_ecaviar\")\n",
    "\n",
    "all_coloc=ecaviar.unionByName(new, allowMissingColumns=True)\n",
    "\n",
    "print(\"loaded files\")\n",
    "\n",
    "#### FIRST MODULE: BUILDING COLOC \n",
    "newColoc=buildColocData(all_coloc,credible,index)\n",
    "\n",
    "print(\"loaded newColoc\")\n",
    "\n",
    "### SECOND MODULE: PROCESS EVIDENCES TO AVOID EXCESS OF COLUMNS \n",
    "gwasComplete = gwasDataset(evidences,credible)\n",
    "\n",
    "#### THIRD MODULE: INCLUDE COLOC IN THE \n",
    "resolvedColoc = (\n",
    "    (\n",
    "        newColoc.withColumnRenamed(\"geneId\", \"targetId\")\n",
    "        .join(\n",
    "            gwasComplete.withColumnRenamed(\"studyLocusId\", \"leftStudyLocusId\"),\n",
    "            on=[\"leftStudyLocusId\", \"targetId\"],\n",
    "            how=\"right\",\n",
    "        )\n",
    "\n",
    "        .join(  ### propagated using parent terms\n",
    "            diseases.selectExpr(\n",
    "                \"id as diseaseId\", \"name\", \"parents\", \"therapeuticAreas\"\n",
    "            ),\n",
    "            on=\"diseaseId\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"diseaseId\",\n",
    "            F.explode_outer(F.concat(F.array(F.col(\"diseaseId\")), F.col(\"parents\"))),\n",
    "        )\n",
    "        .drop(\"parents\", \"oldDiseaseId\")\n",
    "      \n",
    "    ).withColumn(\n",
    "        \"colocDoE\",\n",
    "        F.when(\n",
    "            F.col(\"rightStudyType\").isin(\n",
    "                [\"eqtl\", \"pqtl\", \"tuqtl\", \"sceqtl\", \"sctuqtl\"]\n",
    "            ),\n",
    "            F.when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"GoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"LoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"LoF_protect\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"GoF_protect\"),\n",
    "            ),\n",
    "        ).when(\n",
    "            F.col(\"rightStudyType\").isin(\n",
    "                [\"sqtl\", \"scsqtl\"]\n",
    "            ),  ### opposite directionality than sqtl\n",
    "            F.when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"LoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"GoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"GoF_protect\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"LoF_protect\"),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    # .persist()\n",
    ")\n",
    "print(\"loaded resolvedColloc\")\n",
    "\n",
    "datasource_filter = [\n",
    "#   \"ot_genetics_portal\",\n",
    "    \"gwas_credible_sets\",\n",
    "    \"gene_burden\",\n",
    "    \"eva\",\n",
    "    \"eva_somatic\",\n",
    "    \"gene2phenotype\",\n",
    "    \"orphanet\",\n",
    "    \"cancer_gene_census\",\n",
    "    \"intogen\",\n",
    "    \"impc\",\n",
    "    \"chembl\",\n",
    "]\n",
    "\n",
    "assessment, evidences, actionType, oncolabel = temporary_directionOfEffect(\n",
    "    path_n, datasource_filter\n",
    ")\n",
    "\n",
    "print(\"run temporary direction of effect\")\n",
    "\n",
    "\n",
    "print(\"built drugApproved dataset\")\n",
    "\n",
    "\n",
    "#### FOURTH MODULE BUILDING CHEMBL ASSOCIATIONS - HERE TAKE CARE WITH FILTERING STEP \n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter((F.col(\"datasourceId\") == \"chembl\"))\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(F.col(\"clinicalPhase\")).over(\n",
    "                Window.partitionBy(\"targetId\", \"diseaseId\")\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\")\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    #.filter(F.col(\"coherencyDiagonal\") == \"coherent\")\n",
    "    .drop(\n",
    "        \"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\"\n",
    "    )\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    "    # .persist()\n",
    ")\n",
    "\n",
    "####2 Define agregation function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def convertTuple(tup):\n",
    "    st = \",\".join(map(str, tup))\n",
    "    return st\n",
    "\n",
    "\n",
    "#####3 run in a function\n",
    "def aggregations_original(\n",
    "    df,\n",
    "    data,\n",
    "    listado,\n",
    "    comparisonColumn,\n",
    "    comparisonType,\n",
    "    predictionColumn,\n",
    "    predictionType,\n",
    "    today_date,\n",
    "):\n",
    "    wComparison = Window.partitionBy(comparisonColumn)\n",
    "    wPrediction = Window.partitionBy(predictionColumn)\n",
    "    wPredictionComparison = Window.partitionBy(comparisonColumn, predictionColumn)\n",
    "    results = []\n",
    "    # uniqIds = df.select(\"targetId\", \"diseaseId\").distinct().count()\n",
    "    out = (\n",
    "        df.withColumn(\"comparisonType\", F.lit(comparisonType))\n",
    "        .withColumn(\"dataset\", F.lit(data))\n",
    "        .withColumn(\"predictionType\", F.lit(predictionType))\n",
    "        # .withColumn(\"total\", F.lit(uniqIds))\n",
    "        .withColumn(\"a\", F.count(\"targetId\").over(wPredictionComparison))\n",
    "        .withColumn(\"comparisonColumn\", F.lit(comparisonColumn))\n",
    "        .withColumn(\"predictionColumnValue\", F.lit(predictionColumn))\n",
    "        .withColumn(\n",
    "            \"predictionTotal\",\n",
    "            F.count(\"targetId\").over(wPrediction),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"comparisonTotal\",\n",
    "            F.count(\"targetId\").over(wComparison),\n",
    "        )\n",
    "        .select(\n",
    "            F.col(predictionColumn).alias(\"prediction\"),\n",
    "            F.col(comparisonColumn).alias(\"comparison\"),\n",
    "            \"dataset\",\n",
    "            \"comparisonColumn\",\n",
    "            \"predictionColumnValue\",\n",
    "            \"comparisonType\",\n",
    "            \"predictionType\",\n",
    "            \"a\",\n",
    "            \"predictionTotal\",\n",
    "            \"comparisonTotal\",\n",
    "        )\n",
    "        .filter(F.col(\"prediction\").isNotNull())\n",
    "        .filter(F.col(\"comparison\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    '''\n",
    "    out.write.mode(\"overwrite\").parquet(\n",
    "        \"gs://ot-team/jroldan/\"\n",
    "        + str(\n",
    "            today_date\n",
    "            + \"_\"\n",
    "            + \"analysis/\"\n",
    "            + data\n",
    "            # + \"_propagated\"\n",
    "            + \"/\"\n",
    "            + comparisonColumn\n",
    "            + \"_\"\n",
    "            + comparisonType\n",
    "            + \"_\"\n",
    "            + predictionColumn\n",
    "            + \".parquet\"\n",
    "        )\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    listado.append(\n",
    "        \"gs://ot-team/jroldan/\"\n",
    "        + str(\n",
    "            today_date\n",
    "            + \"_\"\n",
    "            + \"analysis/\"\n",
    "            + data\n",
    "            # + \"_propagated\"\n",
    "            + \"/\"\n",
    "            + comparisonColumn\n",
    "            + \"_\"\n",
    "            + comparisonType\n",
    "            + \"_\"\n",
    "            + predictionColumn\n",
    "            + \".parquet\"\n",
    "        )\n",
    "    )\n",
    "    path = \"gs://ot-team/jroldan/\" + str(\n",
    "        today_date\n",
    "        + \"_\"\n",
    "        + \"analysis/\"\n",
    "        + data\n",
    "        # + \"_propagated\"\n",
    "        + \"/\"\n",
    "        + comparisonColumn\n",
    "        + \"_\"\n",
    "        + comparisonType\n",
    "        + \"_\"\n",
    "        + predictionColumn\n",
    "        + \".parquet\"\n",
    "    )\n",
    "    print(path)\n",
    "    \n",
    "    ### making analysis\n",
    "    array1 = np.delete(\n",
    "        out.join(full_data, on=[\"prediction\", \"comparison\"], how=\"outer\")\n",
    "        .groupBy(\"comparison\")\n",
    "        .pivot(\"prediction\")\n",
    "        .agg(F.first(\"a\"))\n",
    "        .sort(F.col(\"comparison\").desc())\n",
    "        .select(\"comparison\", \"yes\", \"no\")\n",
    "        .fillna(0)\n",
    "        .toPandas()\n",
    "        .to_numpy(),\n",
    "        [0],\n",
    "        1,\n",
    "    )\n",
    "    total = np.sum(array1)\n",
    "    res_npPhaseX = np.array(array1, dtype=int)\n",
    "    resX = convertTuple(fisher_exact(res_npPhaseX, alternative=\"two-sided\"))\n",
    "    resx_CI = convertTuple(\n",
    "        odds_ratio(res_npPhaseX).confidence_interval(confidence_level=0.95)\n",
    "    )\n",
    "\n",
    "    result_st.append(resX)\n",
    "    result_ci.append(resx_CI)\n",
    "    (rs_result, rs_ci) = relative_success(array1)\n",
    "    results.extend(\n",
    "        [\n",
    "            comparisonType,\n",
    "            comparisonColumn,\n",
    "            predictionColumn,\n",
    "            round(float(resX.split(\",\")[0]), 2),\n",
    "            float(resX.split(\",\")[1]),\n",
    "            round(float(resx_CI.split(\",\")[0]), 2),\n",
    "            round(float(resx_CI.split(\",\")[1]), 2),\n",
    "            str(total),\n",
    "            np.array(res_npPhaseX).tolist(),\n",
    "            round(float(rs_result), 2),\n",
    "            round(float(rs_ci[0]), 2),\n",
    "            round(float(rs_ci[1]), 2),\n",
    "            # studies,\n",
    "            # tissues,\n",
    "            path,\n",
    "        ]\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "#### 3 Loop over different datasets (as they will have different rows and columns)\n",
    "\n",
    "\n",
    "def comparisons_df_iterative(elements):\n",
    "    # toAnalysis = [(key, value) for key, value in disdic.items() if value == projectId]\n",
    "    toAnalysis = [(col, \"predictor\") for col in elements]\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"comparison\", StringType(), True),\n",
    "            StructField(\"comparisonType\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    comparisons = spark.createDataFrame(toAnalysis, schema=schema)\n",
    "    ### include all the columns as predictor\n",
    "\n",
    "    predictions = spark.createDataFrame(\n",
    "        data=[\n",
    "            (\"Phase>=4\", \"clinical\"),\n",
    "            ('Phase>=3','clinical'),\n",
    "            ('Phase>=2','clinical'),\n",
    "            ('Phase>=1','clinical'),\n",
    "            (\"PhaseT\", \"clinical\"),\n",
    "        ]\n",
    "    )\n",
    "    return comparisons.join(predictions, how=\"full\").collect()\n",
    "\n",
    "\n",
    "print(\"load comparisons_df_iterative function\")\n",
    "\n",
    "\n",
    "full_data = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\"yes\", \"yes\"),\n",
    "        (\"yes\", \"no\"),\n",
    "        (\"no\", \"yes\"),\n",
    "        (\"no\", \"no\"),\n",
    "    ],\n",
    "    schema=StructType(\n",
    "        [\n",
    "            StructField(\"prediction\", StringType(), True),\n",
    "            StructField(\"comparison\", StringType(), True),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\"created full_data and lists\")\n",
    "\n",
    "#rightTissue = spark.read.csv(\n",
    "#    'gs://ot-team/jroldan/analysis/20250526_rightTissue.csv',\n",
    "#    header=True,\n",
    "#).drop(\"_c0\")\n",
    "\n",
    "print(\"loaded rightTissue dataset\")\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\")\n",
    "    .count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\"))\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "print(\"built negativeTD dataset\")\n",
    "\n",
    "print(\"built bench2 dataset\")\n",
    "\n",
    "###### cut from here\n",
    "print(\"looping for variables_study\")\n",
    "\n",
    "#### new part with chatgpt -- TEST\n",
    "\n",
    "## QUESTIONS TO ANSWER:\n",
    "# HAVE ECAVIAR >=0.8\n",
    "# HAVE COLOC \n",
    "# HAVE COLOC >= 0.8\n",
    "# HAVE COLOC + ECAVIAR >= 0.01\n",
    "# HAVE COLOC >= 0.8 + ECAVIAR >= 0.01\n",
    "# RIGHT JOING WITH CHEMBL \n",
    "\n",
    "### FIFTH MODULE: BUILDING BENCHMARK OF THE DATASET TO EXTRACT EHE ANALYSIS \n",
    "\n",
    "resolvedColocFiltered = resolvedColoc.filter((F.col('clpp')>=0.01) | (F.col('h4')>=0.8))\n",
    "benchmark = (\n",
    "    (\n",
    "        resolvedColocFiltered.filter(F.col(\"betaGwas\") < 0).filter(\n",
    "        F.col(\"name\") != \"COVID-19\"\n",
    "    )\n",
    "        .join(  ### select just GWAS giving protection\n",
    "            analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\"  ### RIGHT SIDE\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"AgreeDrug\",\n",
    "            F.when(\n",
    "                (F.col(\"drugGoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"GoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"drugLoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"LoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "    )  #### remove COVID-19 associations\n",
    ").join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    "\n",
    "#bench2 = benchmark.join(\n",
    "#    rightTissue, on=[\"name\", \"bioSampleName\"], how=\"left\"\n",
    "#).withColumn(\n",
    "#    \"rightTissue\",\n",
    "#    F.when(F.col(\"rightTissue1\") == \"yes\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "#)\n",
    "\n",
    "print(\"built benchmark dataset\")\n",
    "\n",
    "## write the benchmark \n",
    "#name='benchmark'\n",
    "#output_partitioned_path = f\"gs://ot-team/jroldan/analysis/parquetFiles/{name}\"\n",
    "#benchmark.write.mode(\"overwrite\").parquet(output_partitioned_path)\n",
    "#print(f'written {name}')\n",
    "#### Analysis\n",
    "\n",
    "#### 1 Build a dictionary with the distinct values as key and column names as value\n",
    "#variables_study = [\"projectId\", \"biosampleName\", \"rightStudyType\", \"colocDoE\",\"colocalisationMethod\"]\n",
    "variables_study = [\"projectId\"]\n",
    "\n",
    "# List to hold temporary DataFrames\n",
    "temp_dfs_for_union = []\n",
    "\n",
    "# Iterate over the column names to prepare DataFrames for union\n",
    "for col_name in variables_study:\n",
    "    # Select the current column, alias it to 'distinct_value' for consistent schema\n",
    "    # Filter out nulls, then get distinct values\n",
    "    # Add a literal column with the original 'col_name'\n",
    "    df_temp = (\n",
    "        benchmark.select(F.col(col_name).alias(\"distinct_value\"))\n",
    "        .filter(F.col(\"distinct_value\").isNotNull()) # Exclude None (null) values\n",
    "        .distinct()\n",
    "        .withColumn(\"column_name\", F.lit(col_name))\n",
    "    )\n",
    "    temp_dfs_for_union.append(df_temp)\n",
    "\n",
    "disdic = {}\n",
    "\n",
    "if temp_dfs_for_union:\n",
    "    # Union all the temporary DataFrames.\n",
    "    # unionByName is crucial to handle potential schema differences (e.g., if columns have same name but different types)\n",
    "    # and ensures columns are matched by name.\n",
    "    combined_distinct_values_df = temp_dfs_for_union[0]\n",
    "    for i in range(1, len(temp_dfs_for_union)):\n",
    "        combined_distinct_values_df = combined_distinct_values_df.unionByName(temp_dfs_for_union[i])\n",
    "\n",
    "    # Now, collect the combined distinct values.\n",
    "    # This is a single collect operation on the aggregated DataFrame.\n",
    "    print(\"Collecting combined distinct values from the cluster...\")\n",
    "    collected_rows = combined_distinct_values_df.collect()\n",
    "\n",
    "    # Populate the dictionary from the collected rows\n",
    "    for row in collected_rows:\n",
    "        disdic[row.distinct_value] = row.column_name\n",
    "else:\n",
    "    print(\"variables_study list is empty, disdic will be empty.\")\n",
    "\n",
    "\n",
    "print(\"\\nFinal disdic:\", disdic)\n",
    "\n",
    "# Assuming 'spark' session, 'benchmark' DataFrame, 'negativeTD' DataFrame, and 'disdic' dictionary are defined\n",
    "\n",
    "# --- Step 1: Pre-compute 'hasboth' ONCE ---\n",
    "# This is a shuffle, but only happens once.\n",
    "print(\"Pre-computing 'hasboth' column...\")\n",
    "window_target_disease_only = Window.partitionBy('targetId', 'diseaseId')\n",
    "benchmark_processed = benchmark.withColumn(\n",
    "    'hasboth',\n",
    "    F.size(F.collect_set('colocalisationMethod').over(window_target_disease_only))\n",
    ")\n",
    "\n",
    "# You might consider caching this intermediate result if 'benchmark' is very large\n",
    "# and you have enough memory, to avoid re-reading from source if possible.\n",
    "# benchmark_processed.cache() # or .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "# benchmark_processed.count() # Force computation if you cache\n",
    "\n",
    "pivoted_dfs = {}\n",
    "\n",
    "# --- Step 2: Loop for each variable_study column ---\n",
    "for col_name in variables_study:\n",
    "    print(f\"Processing pivot for: {col_name}\")\n",
    "\n",
    "    # Define window specs for the current iteration, including 'col_name' in partition\n",
    "    # (This shuffle is still per iteration, but unavoidable if 'resolvedAgreeDrug' depends on 'col_name' values)\n",
    "    current_col_window_spec_qtl = Window.partitionBy(\"targetId\", \"diseaseId\", col_name).orderBy(F.col(\"qtlPValueExponent\").asc())\n",
    "    current_col_pvalue_order_window = Window.partitionBy(\"targetId\", \"diseaseId\", col_name).orderBy(F.col('colocalisationMethod').asc(), F.col(\"qtlPValueExponent\").asc())\n",
    "\n",
    "    # Calculate 'resolvedAgreeDrug' for the current 'col_name'\n",
    "    # This involves a shuffle per iteration.\n",
    "    temp_df_with_resolved = benchmark_processed.withColumn('resolvedAgreeDrug',\n",
    "        F.when(F.col('hasboth') > 1,\n",
    "            F.first(F.col('AgreeDrug'), ignorenulls=True).over(current_col_pvalue_order_window)\n",
    "        ).otherwise(F.first(F.col('AgreeDrug'), ignorenulls=True).over(current_col_window_spec_qtl))\n",
    "    )\n",
    "\n",
    "    # --- Step 3: Perform the pivot and join ---\n",
    "    # This is an expensive operation (shuffle, potential wide dataframe)\n",
    "    pivoted_df = (\n",
    "        temp_df_with_resolved\n",
    "        .groupBy(\n",
    "            \"targetId\",\n",
    "            \"diseaseId\",\n",
    "            \"maxClinPhase\",\n",
    "        )\n",
    "        .pivot(col_name) # Pivoting on values of the 'col_name' column\n",
    "        .agg(F.collect_set(\"resolvedAgreeDrug\"))\n",
    "        .join(negativeTD, on=[\"targetId\", \"diseaseId\"], how=\"left\") # Ensure negativeTD is broadcast if small\n",
    "    )\n",
    "\n",
    "    # --- Step 4: Add derived columns (these are generally cheap) ---\n",
    "    for phase in [1, 2, 3, 4]:\n",
    "        pivoted_df = pivoted_df.withColumn(\n",
    "            f\"Phase>={phase}\",\n",
    "            F.when(F.col(\"maxClinPhase\") >= phase, F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "\n",
    "    pivoted_df = pivoted_df.withColumn(\n",
    "        \"PhaseT\",\n",
    "        F.when(F.col(\"stopReason\") == \"Negative\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"Phase>=4\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") == 4) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"Phase>=3\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 3) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"Phase>=2\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 2) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"Phase>=1\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 1) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    )\n",
    "\n",
    "    # Add _only columns dynamically based on disdic values matching current column\n",
    "    matching_keys = [key for key, val in disdic.items() if val == col_name]\n",
    "\n",
    "    for key in matching_keys:\n",
    "        # F.col(key) assumes 'key' refers to a column that exists in pivoted_df after the pivot.\n",
    "        pivoted_df = pivoted_df.withColumn(\n",
    "            f\"{key}_only\",\n",
    "            F.when(F.array_contains(F.col(key), \"yes\"), F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "\n",
    "### making columns for the \n",
    "\n",
    "    # --- Step 5: Store result. Consider writing to GCS to break lineage if memory is an issue ---\n",
    "    # This is highly recommended if 'variables_study' is very large.\n",
    "    # Write to Parquet for efficient storage and schema preservation.\n",
    "    # output_path = f\"gs://your-bucket/temp_pivoted_results/{col_name}\"\n",
    "    # print(f\"Writing results for {col_name} to {output_path}\")\n",
    "    # pivoted_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    # pivoted_dfs[col_name] = spark.read.parquet(output_path) # Read back if needed later\n",
    "    # output_partitioned_path = f\"gs://ot-team/jroldan/analysis/parquetFiles/pivoted_df_{col_name}\"\n",
    "    # pivoted_df.write.mode(\"overwrite\").parquet(output_partitioned_path)\n",
    "    # print(f\"DataFrame successfully written and partitioned to {output_partitioned_path}\")\n",
    "    # If not writing to GCS, just store the DF in memory (be cautious for large number of DFs)\n",
    "\n",
    "    pivoted_dfs[col_name] = pivoted_df\n",
    "\n",
    "##### PROJECTID\n",
    "project_keys=[f\"{k}_only\" for k,v in disdic.items() if v == 'projectId']\n",
    "main=['GTEx_only', 'UKB_PPP_EUR_only']\n",
    "#stimulated=['Alasoo_2018_only','Cytoimmgen_only','Fairfax_2014_only','Kim-Hellmuth_2017_only','Nathan_2022_only','Nedelec_2016_only','Quach_2016_only','Randolph_2021_only','Schmiedel_2018_only']\n",
    "cellLine=['CAP_only','HipSci_only','iPSCORE_only','Jerber_2021_only','PhLiPS_only','Schwartzentruber_2018_only','TwinsUK_only']\n",
    "stimulated=['Schmiedel_2018_only','Bossini-Castillo_2019_only','Alasoo_2018_only','Cytoimmgen_only','Gilchrist_2021_only','CAP_only','Quach_2016_only','Randolph_2021_only','Sun_2018_only','Nedelec_2016_only','Kim-Hellmuth_2017_only']\n",
    "others=[item for item in project_keys if item not in main]\n",
    "nonStimulated=[item for item in project_keys if item not in stimulated]\n",
    "otherCellLine=[item for item in project_keys if item not in cellLine]\n",
    "\n",
    "# First condition: any \"yes\" in list1\n",
    "condition1 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), others[1:], F.col(others[0]) == \"yes\")\n",
    "# estimulated\n",
    "condition2 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), stimulated[1:], F.col(stimulated[0]) == \"yes\")\n",
    "## non estimulated:\n",
    "condition3 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), nonStimulated[1:], F.col(nonStimulated[0]) == \"yes\")\n",
    "# cellLine\n",
    "condition4 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), cellLine[1:], F.col(cellLine[0]) == \"yes\")\n",
    "# non cellline\n",
    "condition5 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), otherCellLine[1:], F.col(otherCellLine[0]) == \"yes\")\n",
    "# non cellline\n",
    "condition6 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), main[1:], F.col(main[0]) == \"yes\")\n",
    "\n",
    "# Add both columns\n",
    "pivoted_dfs['projectId'] = pivoted_dfs['projectId'].withColumn(\"othersProjectId_only\", F.when(condition1, \"yes\").otherwise(\"no\")) \n",
    "pivoted_dfs['projectId'] = pivoted_dfs['projectId'].withColumn(\"estimulated_only\", F.when(condition2, \"yes\").otherwise(\"no\")) \n",
    "pivoted_dfs['projectId'] = pivoted_dfs['projectId'].withColumn(\"nonStimulated_only\", F.when(condition3, \"yes\").otherwise(\"no\")) \n",
    "pivoted_dfs['projectId'] = pivoted_dfs['projectId'].withColumn(\"cellLine\", F.when(condition4, \"yes\").otherwise(\"no\")) \n",
    "pivoted_dfs['projectId'] = pivoted_dfs['projectId'].withColumn(\"nonCellLine\", F.when(condition5, \"yes\").otherwise(\"no\")) \n",
    "pivoted_dfs['projectId'] = pivoted_dfs['projectId'].withColumn(\"GTExUKB\", F.when(condition6, \"yes\").otherwise(\"no\")) \n",
    "\n",
    "\n",
    "###append to dictionary\n",
    "\n",
    "disdic.update({'othersProjectId': 'projectId','Stimulated': 'projectId','cellLine': 'projectId', 'othersBiosampleName_only': 'biosampleName', 'otherRightStudyType':'rightStudyType'})\n",
    "\n",
    "\n",
    "result = []\n",
    "result_st = []\n",
    "result_ci = []\n",
    "array2 = []\n",
    "listado = []\n",
    "result_all = []\n",
    "today_date = str(date.today())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_aggregate = ['NoneCellYes', 'NdiagonalYes','hasGenetics'] # The values you want to collect in the pivoted cells\n",
    "all_pivoted_dfs = {}\n",
    "\n",
    "doe_columns=[\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "diagonal_lof=['LoF_protect','GoF_risk']\n",
    "diagonal_gof=['LoF_risk','GoF_protect']\n",
    "\n",
    "conditions = [\n",
    "    F.when(F.col(c) == F.col(\"maxDoE\"), F.lit(c)).otherwise(F.lit(None)) for c in doe_columns\n",
    "    ]\n",
    "\n",
    "# --- Nested Loops for Dynamic Pivoting ---\n",
    "for agg_col_name in columns_to_aggregate:\n",
    "    for pivot_col_name in columns_to_pivot_on:\n",
    "        print(f\"\\n--- Creating DataFrame for Aggregation: '{agg_col_name}' and Pivot: '{pivot_col_name}' ---\")\n",
    "        current_col_pvalue_order_window = Window.partitionBy(\"targetId\", \"diseaseId\", \"maxClinPhase\", pivot_col_name).orderBy(F.col('colocalisationMethod').asc(), F.col(\"qtlPValueExponent\").asc())\n",
    "        test2=discrepancifier(benchmark.withColumn('actionType2', F.concat_ws(\",\", F.col(\"actionType2\"))).withColumn('qtlColocDoE',F.first('colocDoE').over(current_col_pvalue_order_window)).groupBy(\n",
    "        \"targetId\", \"diseaseId\", \"maxClinPhase\", \"drugLoF_protect\", \"drugGoF_protect\",pivot_col_name)\n",
    "        .pivot(\"colocDoE\")\n",
    "        .count()\n",
    "        .withColumnRenamed('drugLoF_protect', 'LoF_protect_ch')\n",
    "        .withColumnRenamed('drugGoF_protect', 'GoF_protect_ch')).withColumn( ## .filter(F.col('coherencyDiagonal')!='noEvid')\n",
    "    \"arrayN\", F.array(*[F.col(c) for c in doe_columns])\n",
    "    ).withColumn(\n",
    "        \"maxDoE\", F.array_max(F.col(\"arrayN\"))\n",
    "    ).withColumn(\"maxDoE_names\", F.array(*conditions)\n",
    "    ).withColumn(\"maxDoE_names\", F.expr(\"filter(maxDoE_names, x -> x is not null)\")\n",
    "    ).withColumn(\n",
    "        \"NoneCellYes\",\n",
    "        F.when((F.col(\"LoF_protect_ch\").isNotNull() & (F.col('GoF_protect_ch').isNull())) & (F.array_contains(F.col(\"maxDoE_names\"), F.lit(\"LoF_protect\")))==True, F.lit('yes'))\n",
    "        .when((F.col(\"GoF_protect_ch\").isNotNull() & (F.col('LoF_protect_ch').isNull())) & (F.array_contains(F.col(\"maxDoE_names\"), F.lit(\"GoF_protect\")))==True, F.lit('yes')\n",
    "            ).otherwise(F.lit('no'))  # If the value is null, return null # Otherwise, check if name is in array\n",
    "    ).withColumn(\n",
    "        \"NdiagonalYes\",\n",
    "        F.when((F.col(\"LoF_protect_ch\").isNotNull() & (F.col('GoF_protect_ch').isNull())) & \n",
    "            (F.size(F.array_intersect(F.col(\"maxDoE_names\"), F.array([F.lit(x) for x in diagonal_lof]))) > 0),\n",
    "            F.lit(\"yes\")\n",
    "        ).when((F.col(\"GoF_protect_ch\").isNotNull() & (F.col('LoF_protect_ch').isNull())) & \n",
    "            (F.size(F.array_intersect(F.col(\"maxDoE_names\"), F.array([F.lit(x) for x in diagonal_gof]))) > 0),\n",
    "            F.lit(\"yes\")\n",
    "        ).otherwise(F.lit('no'))\n",
    "    ).withColumn(\n",
    "        \"drugCoherency\",\n",
    "        F.when(\n",
    "            (F.col(\"LoF_protect_ch\").isNotNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNull()), F.lit(\"coherent\")\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"LoF_protect_ch\").isNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNotNull()), F.lit(\"coherent\")\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"LoF_protect_ch\").isNotNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNotNull()), F.lit(\"dispar\")\n",
    "        )\n",
    "        .otherwise(F.lit(\"other\")),\n",
    "    ).join(negativeTD, on=[\"targetId\", \"diseaseId\"], how=\"left\").withColumn(\n",
    "        \"PhaseT\",\n",
    "        F.when(F.col(\"stopReason\") == \"Negative\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase4Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") == 4) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase3Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 3) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase2Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 2) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase1Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 1) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"hasGenetics\",\n",
    "        F.when(F.col(\"coherencyDiagonal\") != \"noEvid\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 154:==================================================>    (62 + 5) / 67]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+----------------+------------+-----------------+----------+--------------+--------------------------+----+--------------------+--------------------+----+----+----+----+----+-----------+-------------+---------------------+--------------+----------------------+-----------------+----------+---------+--------------+---------+------------+----------+-------------------------+-------------+------------------+----+-----+--------+-------+---------+--------+--------------+----+----------------+--------+------------+---------------+---------------+---------+-------------+-------+-----------------+\n",
      "|biosampleId|       targetId|    diseaseId|leftStudyLocusId|rightStudyId|rightStudyLocusId|chromosome|rightStudyType|numberColocalisingVariants|clpp|colocalisationMethod|betaRatioSignAverage|  h0|  h1|  h2|  h3|  h4|leftStudyId|leftVariantId|credibleLeftStudyType|rightVariantId|credibleRightStudyType|qtlPValueExponent|isTransQtl|projectId|indexStudyType|condition|datasourceId|datatypeId|diseaseFromSourceMappedId|resourceScore|targetFromSourceId|  id|score|sourceId|studyId|variantId|betaGwas|pValueExponent|name|therapeuticAreas|colocDoE|maxClinPhase|drugGoF_protect|drugLoF_protect|AgreeDrug|biosampleName|hasboth|resolvedAgreeDrug|\n",
      "+-----------+---------------+-------------+----------------+------------+-----------------+----------+--------------+--------------------------+----+--------------------+--------------------+----+----+----+----+----+-----------+-------------+---------------------+--------------+----------------------+-----------------+----------+---------+--------------+---------+------------+----------+-------------------------+-------------+------------------+----+-----+--------+-------+---------+--------+--------------+----+----------------+--------+------------+---------------+---------------+---------+-------------+-------+-----------------+\n",
      "|       NULL|ENSG00000007314|  EFO_0000555|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         2.0|           NULL|              3|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000007314|  EFO_1000249|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         3.0|           NULL|              1|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000008018|  EFO_1000453|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         2.0|           NULL|              1|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000012504|MONDO_0019052|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         4.0|              1|           NULL|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000012779|MONDO_0004235|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         3.0|           NULL|              4|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000014138|  EFO_1001945|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         3.0|           NULL|              4|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000023228|  EFO_1000657|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         2.0|           NULL|              3|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000062822|  EFO_0004991|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         2.0|           NULL|              1|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000065883|  EFO_0001378|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         1.0|           NULL|              1|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000067191|  EFO_0000319|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         4.0|           NULL|              2|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000070886|  EFO_1000489|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         2.0|           NULL|              1|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000077782|MONDO_0008315|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         2.0|           NULL|              6|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000080293|  EFO_0001073|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         2.0|              1|           NULL|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000081248|MONDO_0004992|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         1.0|           NULL|              1|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000082175|  EFO_1000796|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         2.0|              1|           NULL|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000082482|  EFO_1000637|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         3.0|           NULL|           NULL|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000082556|  EFO_1000906|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         2.0|           NULL|              1|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000082556|MONDO_0004992|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         2.0|              1|           NULL|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000087085|MONDO_0005301|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         3.0|           NULL|              1|       no|         NULL|      0|               no|\n",
      "|       NULL|ENSG00000087586|MONDO_0002516|            NULL|        NULL|             NULL|      NULL|          NULL|                      NULL|NULL|                NULL|                NULL|NULL|NULL|NULL|NULL|NULL|       NULL|         NULL|                 NULL|          NULL|                  NULL|             NULL|      NULL|     NULL|          NULL|     NULL|        NULL|      NULL|                     NULL|         NULL|              NULL|NULL| NULL|    NULL|   NULL|     NULL|    NULL|          NULL|NULL|            NULL|    NULL|         1.0|           NULL|              1|       no|         NULL|      0|               no|\n",
      "+-----------+---------------+-------------+----------------+------------+-----------------+----------+--------------+--------------------------+----+--------------------+--------------------+----+----+----+----+----+-----------+-------------+---------------------+--------------+----------------------+-----------------+----------+---------+--------------+---------+------------+----------+-------------------------+-------------+------------------+----+-----+--------+-------+---------+--------+--------------+----+----------------+--------+------------+---------------+---------------+---------+-------------+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_df_with_resolved.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[targetId: string, diseaseId: string, maxClinPhase: double, null: array<string>, Alasoo_2018: array<string>, Aygun_2021: array<string>, BLUEPRINT: array<string>, Bossini-Castillo_2019: array<string>, BrainSeq: array<string>, Braineac2: array<string>, CAP: array<string>, CEDAR: array<string>, CommonMind: array<string>, Cytoimmgen: array<string>, FUSION: array<string>, Fairfax_2012: array<string>, Fairfax_2014: array<string>, GENCORD: array<string>, GEUVADIS: array<string>, GTEx: array<string>, Gilchrist_2021: array<string>, HipSci: array<string>, Jerber_2021: array<string>, Kasela_2017: array<string>, Kim-Hellmuth_2017: array<string>, Lepik_2017: array<string>, Naranbhai_2015: array<string>, Nathan_2022: array<string>, Nedelec_2016: array<string>, OneK1K: array<string>, PISA: array<string>, Peng_2018: array<string>, Perez_2022: array<string>, PhLiPS: array<string>, Quach_2016: array<string>, ROSMAP: array<string>, Randolph_2021: array<string>, Schmiedel_2018: array<string>, Schwartzentruber_2018: array<string>, Steinberg_2020: array<string>, Sun_2018: array<string>, TwinsUK: array<string>, UKB_PPP_EUR: array<string>, Walker_2019: array<string>, Young_2019: array<string>, iPSCORE: array<string>, van_de_Bunt_2015: array<string>, stopReason: string, Phase>=1: string, Phase>=2: string, Phase>=3: string, Phase>=4: string, PhaseT: string, HipSci_only: string, van_de_Bunt_2015_only: string, GTEx_only: string, Lepik_2017_only: string, Bossini-Castillo_2019_only: string, ROSMAP_only: string, BLUEPRINT_only: string, TwinsUK_only: string, FUSION_only: string, Cytoimmgen_only: string, Gilchrist_2021_only: string, PhLiPS_only: string, Fairfax_2014_only: string, BrainSeq_only: string, GEUVADIS_only: string, Kim-Hellmuth_2017_only: string, Schmiedel_2018_only: string, Peng_2018_only: string, CEDAR_only: string, Nathan_2022_only: string, UKB_PPP_EUR_only: string, Quach_2016_only: string, iPSCORE_only: string, Jerber_2021_only: string, Alasoo_2018_only: string, Perez_2022_only: string, CommonMind_only: string, CAP_only: string, Walker_2019_only: string, GENCORD_only: string, Nedelec_2016_only: string, Steinberg_2020_only: string, OneK1K_only: string, Fairfax_2012_only: string, Aygun_2021_only: string, Schwartzentruber_2018_only: string, Kasela_2017_only: string, PISA_only: string, Braineac2_only: string, Randolph_2021_only: string, Naranbhai_2015_only: string, Sun_2018_only: string, Young_2019_only: string, othersProjectId_only: string, estimulated_only: string, nonStimulated_only: string, cellLine: string, nonCellLine: string, GTExUKB: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_dfs['projectId'].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:====================================================>(399 + 1) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+------------+----+-----------+----------+---------+---------------------+--------+---------+---+-----+----------+----------+------+------------+------------+-------+--------+----+--------------+------+-----------+-----------+-----------------+----------+--------------+-----------+------------+------+----+---------+----------+------+----------+------+-------------+--------------+---------------------+--------------+--------+-------+-----------+-----------+----------+-------+----------------+----------+--------+--------+--------+--------+------+-----------+---------------------+---------+---------------+--------------------------+-----------+--------------+------------+-----------+---------------+-------------------+-----------+-----------------+-------------+-------------+----------------------+-------------------+--------------+----------+----------------+----------------+---------------+------------+----------------+----------------+---------------+---------------+--------+----------------+------------+-----------------+-------------------+-----------+-----------------+---------------+--------------------------+----------------+---------+--------------+------------------+-------------------+-------------+---------------+--------------------+----------------+------------------+--------+-----------+-------+\n",
      "|       targetId|    diseaseId|maxClinPhase|null|Alasoo_2018|Aygun_2021|BLUEPRINT|Bossini-Castillo_2019|BrainSeq|Braineac2|CAP|CEDAR|CommonMind|Cytoimmgen|FUSION|Fairfax_2012|Fairfax_2014|GENCORD|GEUVADIS|GTEx|Gilchrist_2021|HipSci|Jerber_2021|Kasela_2017|Kim-Hellmuth_2017|Lepik_2017|Naranbhai_2015|Nathan_2022|Nedelec_2016|OneK1K|PISA|Peng_2018|Perez_2022|PhLiPS|Quach_2016|ROSMAP|Randolph_2021|Schmiedel_2018|Schwartzentruber_2018|Steinberg_2020|Sun_2018|TwinsUK|UKB_PPP_EUR|Walker_2019|Young_2019|iPSCORE|van_de_Bunt_2015|stopReason|Phase>=1|Phase>=2|Phase>=3|Phase>=4|PhaseT|HipSci_only|van_de_Bunt_2015_only|GTEx_only|Lepik_2017_only|Bossini-Castillo_2019_only|ROSMAP_only|BLUEPRINT_only|TwinsUK_only|FUSION_only|Cytoimmgen_only|Gilchrist_2021_only|PhLiPS_only|Fairfax_2014_only|BrainSeq_only|GEUVADIS_only|Kim-Hellmuth_2017_only|Schmiedel_2018_only|Peng_2018_only|CEDAR_only|Nathan_2022_only|UKB_PPP_EUR_only|Quach_2016_only|iPSCORE_only|Jerber_2021_only|Alasoo_2018_only|Perez_2022_only|CommonMind_only|CAP_only|Walker_2019_only|GENCORD_only|Nedelec_2016_only|Steinberg_2020_only|OneK1K_only|Fairfax_2012_only|Aygun_2021_only|Schwartzentruber_2018_only|Kasela_2017_only|PISA_only|Braineac2_only|Randolph_2021_only|Naranbhai_2015_only|Sun_2018_only|Young_2019_only|othersProjectId_only|estimulated_only|nonStimulated_only|cellLine|nonCellLine|GTExUKB|\n",
      "+---------------+-------------+------------+----+-----------+----------+---------+---------------------+--------+---------+---+-----+----------+----------+------+------------+------------+-------+--------+----+--------------+------+-----------+-----------+-----------------+----------+--------------+-----------+------------+------+----+---------+----------+------+----------+------+-------------+--------------+---------------------+--------------+--------+-------+-----------+-----------+----------+-------+----------------+----------+--------+--------+--------+--------+------+-----------+---------------------+---------+---------------+--------------------------+-----------+--------------+------------+-----------+---------------+-------------------+-----------+-----------------+-------------+-------------+----------------------+-------------------+--------------+----------+----------------+----------------+---------------+------------+----------------+----------------+---------------+---------------+--------+----------------+------------+-----------------+-------------------+-----------+-----------------+---------------+--------------------------+----------------+---------+--------------+------------------+-------------------+-------------+---------------+--------------------+----------------+------------------+--------+-----------+-------+\n",
      "|ENSG00000007314|  EFO_0000555|         2.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000007314|  EFO_1000249|         3.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|     yes|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000008018|  EFO_1000453|         2.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000012504|MONDO_0019052|         4.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|     yes|     yes|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000012779|MONDO_0004235|         3.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|  Negative|      no|      no|      no|      no|   yes|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000014138|  EFO_1001945|         3.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|     yes|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000023228|  EFO_1000657|         2.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000062822|  EFO_0004991|         2.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000065883|  EFO_0001378|         1.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|      no|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000067191|  EFO_0000319|         4.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|     yes|     yes|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000070886|  EFO_1000489|         2.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|  Negative|      no|      no|      no|      no|   yes|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000077782|MONDO_0008315|         2.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000080293|  EFO_0001073|         2.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000081248|MONDO_0004992|         1.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|      no|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000082175|  EFO_1000796|         2.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000082482|  EFO_1000637|         3.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|     yes|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000082556|  EFO_1000906|         2.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000082556|MONDO_0004992|         2.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000087085|MONDO_0005301|         3.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|     yes|     yes|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "|ENSG00000087586|MONDO_0002516|         1.0|[no]|         []|        []|       []|                   []|      []|       []| []|   []|        []|        []|    []|          []|          []|     []|      []|  []|            []|    []|         []|         []|               []|        []|            []|         []|          []|    []|  []|       []|        []|    []|        []|    []|           []|            []|                   []|            []|      []|     []|         []|         []|        []|     []|              []|      NULL|     yes|      no|      no|      no|    no|         no|                   no|       no|             no|                        no|         no|            no|          no|         no|             no|                 no|         no|               no|           no|           no|                    no|                 no|            no|        no|              no|              no|             no|          no|              no|              no|             no|             no|      no|              no|          no|               no|                 no|         no|               no|             no|                        no|              no|       no|            no|                no|                 no|           no|             no|                  no|              no|                no|      no|         no|     no|\n",
      "+---------------+-------------+------------+----+-----------+----------+---------+---------------------+--------+---------+---+-----+----------+----------+------+------------+------------+-------+--------+----+--------------+------+-----------+-----------+-----------------+----------+--------------+-----------+------------+------+----+---------+----------+------+----------+------+-------------+--------------+---------------------+--------------+--------+-------+-----------+-----------+----------+-------+----------------+----------+--------+--------+--------+--------+------+-----------+---------------------+---------+---------------+--------------------------+-----------+--------------+------------+-----------+---------------+-------------------+-----------+-----------------+-------------+-------------+----------------------+-------------------+--------------+----------+----------------+----------------+---------------+------------+----------------+----------------+---------------+---------------+--------+----------------+------------+-----------------+-------------------+-----------+-----------------+---------------+--------------------------+----------------+---------+--------------+------------------+-------------------+-------------+---------------+--------------------+----------------+------------------+--------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivoted_dfs['projectId'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with projectId\n",
      "There are  <class 'filter'> columns to analyse with phases\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/othersProjectId_only_predictor_Phase>=4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/othersProjectId_only_predictor_Phase>=3.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/othersProjectId_only_predictor_Phase>=2.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/othersProjectId_only_predictor_Phase>=1.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/othersProjectId_only_predictor_PhaseT.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/estimulated_only_predictor_Phase>=4.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/estimulated_only_predictor_Phase>=3.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/estimulated_only_predictor_Phase>=2.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/estimulated_only_predictor_Phase>=1.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/estimulated_only_predictor_PhaseT.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/nonStimulated_only_predictor_Phase>=4.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/nonStimulated_only_predictor_Phase>=3.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/nonStimulated_only_predictor_Phase>=2.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/nonStimulated_only_predictor_Phase>=1.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/nonStimulated_only_predictor_PhaseT.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/cellLine_predictor_Phase>=4.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/cellLine_predictor_Phase>=3.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/cellLine_predictor_Phase>=2.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/cellLine_predictor_Phase>=1.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/cellLine_predictor_PhaseT.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/nonCellLine_predictor_Phase>=4.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/nonCellLine_predictor_Phase>=3.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/nonCellLine_predictor_Phase>=2.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/nonCellLine_predictor_Phase>=1.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/nonCellLine_predictor_PhaseT.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/GTExUKB_predictor_Phase>=4.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/GTExUKB_predictor_Phase>=3.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/GTExUKB_predictor_Phase>=2.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/GTExUKB_predictor_Phase>=1.parquet\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/propagated/GTExUKB_predictor_PhaseT.parquet\n",
      "df unpersisted\n",
      "importing functions\n",
      "imported functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe written \n",
      " Analysis finished\n"
     ]
    }
   ],
   "source": [
    "pivoted_dfs['projectId'].unpersist()\n",
    "##### PROJECT ID ###### \n",
    "print('working with projectId')\n",
    "pivoted_dfs['projectId'].persist()\n",
    "#unique_values = benchmark.select('projectId').filter(F.col('projectId').isNotNull()).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "#filter = len(pivoted_dfs['projectId'].drop(*unique_values).columns[10:])\n",
    "print('There are ', filter, 'columns to analyse with phases')\n",
    "rows = comparisons_df_iterative(pivoted_dfs['projectId'].columns[-6:])\n",
    "\n",
    "# If needed, now process the rest\n",
    "for row in rows:\n",
    "    results = aggregations_original(\n",
    "        pivoted_dfs['projectId'], \"propagated\", listado, *row, today_date\n",
    "    )\n",
    "    result_all.append(results)\n",
    "\n",
    "pivoted_dfs['projectId'].unpersist()\n",
    "print('df unpersisted')\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"group\", StringType(), True),\n",
    "        StructField(\"comparison\", StringType(), True),\n",
    "        StructField(\"phase\", StringType(), True),\n",
    "        StructField(\"oddsRatio\", DoubleType(), True),\n",
    "        StructField(\"pValue\", DoubleType(), True),\n",
    "        StructField(\"lowerInterval\", DoubleType(), True),\n",
    "        StructField(\"upperInterval\", DoubleType(), True),\n",
    "        StructField(\"total\", StringType(), True),\n",
    "        StructField(\"values\", ArrayType(ArrayType(IntegerType())), True),\n",
    "        StructField(\"relSuccess\", DoubleType(), True),\n",
    "        StructField(\"rsLower\", DoubleType(), True),\n",
    "        StructField(\"rsUpper\", DoubleType(), True),\n",
    "        StructField(\"path\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "import re\n",
    "\n",
    "# Define the list of patterns to search for\n",
    "patterns = [\n",
    "    \"_only\",\n",
    "    #\"_tissue\",\n",
    "    #\"_isSignalFromRightTissue\",\n",
    "    \"_isRightTissueSignalAgreed\",\n",
    "]\n",
    "# Create a regex pattern to match any of the substrings\n",
    "regex_pattern = \"(\" + \"|\".join(map(re.escape, patterns)) + \")\"\n",
    "\n",
    "# Convert list of lists to DataFrame\n",
    "df = (\n",
    "    spreadSheetFormatter(spark.createDataFrame(result_all, schema=schema))\n",
    "    .withColumn(\n",
    "        \"prefix\",\n",
    "        F.regexp_replace(\n",
    "            F.col(\"comparison\"), regex_pattern + \".*\", \"\"\n",
    "        ),  # Extract part before the pattern\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"suffix\",\n",
    "        F.regexp_extract(\n",
    "            F.col(\"comparison\"), regex_pattern, 0\n",
    "        ),  # Extract the pattern itself\n",
    "    )\n",
    ")\n",
    "### annotate projectId, tissue, qtl type and doe type:\n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr=create_map([F.lit(x) for x in chain(*disdic.items())])\n",
    "\n",
    "df_annot=df.withColumn('annotation',mapping_expr.getItem(F.col('prefix')))\n",
    "\n",
    "df_annot.toPandas().to_csv(\n",
    "    f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phases_jointProjects_rightJoin.csv\"\n",
    ")\n",
    "\n",
    "print(\"dataframe written \\n Analysis finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRY WITH THE DRUG DATASET TO SEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark session created at 2025-09-17 20:45:50.891922\n",
      "Analysis started on 2025-09-17 at  2025-09-17 20:45:50.891922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 20:45:55 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/09/17 20:45:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully with the following configurations:\n",
      "  spark.driver.memory: 24g\n",
      "  spark.executor.memory: 32g\n",
      "  spark.executor.cores: 4\n",
      "  spark.executor.instances: 12\n",
      "  spark.yarn.executor.memoryOverhead: 8g\n",
      "  spark.sql.shuffle.partitions: 192\n",
      "  spark.default.parallelism: 192\n",
      "Spark UI available at: http://jr-doe-temp1-m.c.open-targets-eu-dev.internal:35471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded files\n",
      "loaded newColoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded gwasComplete\n",
      "loaded resolvedColloc\n",
      "run temporary direction of effect\n",
      "built drugApproved dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load comparisons_df_iterative function\n",
      "created full_data and lists\n",
      "loaded rightTissue dataset\n",
      "built negativeTD dataset\n",
      "built bench2 dataset\n",
      "looping for variables_study\n",
      "entering the big loops\n",
      "\n",
      "--- Creating DataFrame for Aggregation: 'NoneCellYes' and Pivot: 'projectId' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified ArrayType columns for conversion: ['null', 'Alasoo_2018', 'Aygun_2021', 'BLUEPRINT', 'Bossini-Castillo_2019', 'BrainSeq', 'Braineac2', 'CAP', 'CEDAR', 'CommonMind', 'Cytoimmgen', 'FUSION', 'Fairfax_2012', 'Fairfax_2014', 'GENCORD', 'GEUVADIS', 'GTEx', 'Gilchrist_2021', 'HipSci', 'Jerber_2021', 'Kasela_2017', 'Kim-Hellmuth_2017', 'Lepik_2017', 'Naranbhai_2015', 'Nathan_2022', 'Nedelec_2016', 'OneK1K', 'PISA', 'Peng_2018', 'Perez_2022', 'PhLiPS', 'Quach_2016', 'ROSMAP', 'Randolph_2021', 'Schmiedel_2018', 'Schwartzentruber_2018', 'Steinberg_2020', 'Sun_2018', 'TwinsUK', 'UKB_PPP_EUR', 'Walker_2019', 'Young_2019', 'iPSCORE', 'van_de_Bunt_2015']\n",
      "\n",
      "--- Creating DataFrame for Aggregation: 'NdiagonalYes' and Pivot: 'projectId' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 20:48:41 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified ArrayType columns for conversion: ['null', 'Alasoo_2018', 'Aygun_2021', 'BLUEPRINT', 'Bossini-Castillo_2019', 'BrainSeq', 'Braineac2', 'CAP', 'CEDAR', 'CommonMind', 'Cytoimmgen', 'FUSION', 'Fairfax_2012', 'Fairfax_2014', 'GENCORD', 'GEUVADIS', 'GTEx', 'Gilchrist_2021', 'HipSci', 'Jerber_2021', 'Kasela_2017', 'Kim-Hellmuth_2017', 'Lepik_2017', 'Naranbhai_2015', 'Nathan_2022', 'Nedelec_2016', 'OneK1K', 'PISA', 'Peng_2018', 'Perez_2022', 'PhLiPS', 'Quach_2016', 'ROSMAP', 'Randolph_2021', 'Schmiedel_2018', 'Schwartzentruber_2018', 'Steinberg_2020', 'Sun_2018', 'TwinsUK', 'UKB_PPP_EUR', 'Walker_2019', 'Young_2019', 'iPSCORE', 'van_de_Bunt_2015']\n",
      "\n",
      "--- Creating DataFrame for Aggregation: 'hasGenetics' and Pivot: 'projectId' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 20:49:15 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified ArrayType columns for conversion: ['null', 'Alasoo_2018', 'Aygun_2021', 'BLUEPRINT', 'Bossini-Castillo_2019', 'BrainSeq', 'Braineac2', 'CAP', 'CEDAR', 'CommonMind', 'Cytoimmgen', 'FUSION', 'Fairfax_2012', 'Fairfax_2014', 'GENCORD', 'GEUVADIS', 'GTEx', 'Gilchrist_2021', 'HipSci', 'Jerber_2021', 'Kasela_2017', 'Kim-Hellmuth_2017', 'Lepik_2017', 'Naranbhai_2015', 'Nathan_2022', 'Nedelec_2016', 'OneK1K', 'PISA', 'Peng_2018', 'Perez_2022', 'PhLiPS', 'Quach_2016', 'ROSMAP', 'Randolph_2021', 'Schmiedel_2018', 'Schwartzentruber_2018', 'Steinberg_2020', 'Sun_2018', 'TwinsUK', 'UKB_PPP_EUR', 'Walker_2019', 'Young_2019', 'iPSCORE', 'van_de_Bunt_2015']\n",
      "\n",
      "--- All generated DataFrames are stored in 'all_pivoted_dfs' dictionary ---\n",
      "Keys available: dict_keys(['df_pivot_nonecellyes_by_projectid', 'df_pivot_ndiagonalyes_by_projectid', 'df_pivot_hasgenetics_by_projectid'])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from array import ArrayType\n",
    "from functions import (\n",
    "    relative_success,\n",
    "    spreadSheetFormatter,\n",
    "    discrepancifier,\n",
    "    temporary_directionOfEffect,\n",
    "    buildColocData,\n",
    "    gwasDataset,\n",
    ")\n",
    "# from stoppedTrials import terminated_td\n",
    "from DoEAssessment import directionOfEffect\n",
    "# from membraneTargets import target_membrane\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    DecimalType,\n",
    "    StringType,\n",
    "    FloatType,\n",
    ")\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# --- Build the SparkSession ---\n",
    "# Use the .config() method to set these parameters before calling .getOrCreate()\n",
    "# This ensures Spark requests the correct resources from YARN at the start.\n",
    "driver_memory = \"24g\"                 # plenty for planning & small collects\n",
    "executor_cores = 4                    # sweet spot for GC + Python workers\n",
    "num_executors  = 12                   # 12 * 4 = 48 cores for executors; ~16 cores left for driver/OS\n",
    "executor_memory = \"32g\"               # per executor heap\n",
    "executor_memory_overhead = \"8g\"       # ~20% overhead for PySpark/Arrow/off-heap\n",
    "# Totals: (32+8) * 12 = 480 GB executors + 24 GB driver ≈ 504 GB (adjust down if your hard cap is <500 GB)\n",
    "# If you must stay strictly ≤ 500 GB, use executor_memory=\"30g\", overhead=\"6g\"  → (36 * 12) + 24 = 456 + 24 = 480 GB\n",
    "\n",
    "shuffle_partitions   = 192            # ≈ 2–4× total cores (48) → start with 192\n",
    "default_parallelism  = 192\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyOptimizedPySparkApp\") \\\n",
    "    .config(\"spark.master\", \"yarn\") \\\n",
    "    .config(\"spark.driver.memory\", driver_memory) \\\n",
    "    .config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .config(\"spark.executor.cores\", executor_cores) \\\n",
    "    .config(\"spark.executor.instances\", num_executors) \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions) \\\n",
    "    .config(\"spark.default.parallelism\", default_parallelism) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"SparkSession created successfully with the following configurations:\")\n",
    "print(f\"  spark.driver.memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"  spark.executor.memory: {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"  spark.executor.cores: {spark.conf.get('spark.executor.cores')}\")\n",
    "print(f\"  spark.executor.instances: {spark.conf.get('spark.executor.instances')}\")\n",
    "print(f\"  spark.yarn.executor.memoryOverhead: {spark.conf.get('spark.yarn.executor.memoryOverhead')}\")\n",
    "print(f\"  spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"  spark.default.parallelism: {spark.conf.get('spark.default.parallelism')}\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# --- Your PySpark Code Here ---\n",
    "# Now you can proceed with your data loading and processing.\n",
    "# Example:\n",
    "# df = spark.read.parquet(\"hdfs:///user/your_user/your_large_data.parquet\")\n",
    "# print(f\"Number of rows in DataFrame: {df.count()}\")\n",
    "# df.groupBy(\"some_column\").agg({\"another_column\": \"sum\"}).show()\n",
    "\n",
    "# Remember to stop the SparkSession when you are done\n",
    "# spark.stop()\n",
    "\n",
    "path_n='gs://open-targets-data-releases/25.06/output/'\n",
    "\n",
    "target = spark.read.parquet(f\"{path_n}target/\")\n",
    "\n",
    "diseases = spark.read.parquet(f\"{path_n}disease/\")\n",
    "\n",
    "evidences = spark.read.parquet(f\"{path_n}evidence\")\n",
    "\n",
    "credible = spark.read.parquet(f\"{path_n}credible_set\")\n",
    "\n",
    "new = spark.read.parquet(f\"{path_n}colocalisation_coloc\") \n",
    "\n",
    "index=spark.read.parquet(f\"{path_n}study/\")\n",
    "\n",
    "variantIndex = spark.read.parquet(f\"{path_n}variant\")\n",
    "\n",
    "biosample = spark.read.parquet(f\"{path_n}biosample\")\n",
    "\n",
    "ecaviar=spark.read.parquet(f\"{path_n}colocalisation_ecaviar\")\n",
    "\n",
    "all_coloc=ecaviar.unionByName(new, allowMissingColumns=True)\n",
    "\n",
    "print(\"loaded files\")\n",
    "\n",
    "#### FIRST MODULE: BUILDING COLOC \n",
    "newColoc=buildColocData(all_coloc,credible,index)\n",
    "\n",
    "print(\"loaded newColoc\")\n",
    "\n",
    "### SECOND MODULE: PROCESS EVIDENCES TO AVOID EXCESS OF COLUMNS \n",
    "gwasComplete = gwasDataset(evidences,credible)\n",
    "\n",
    "#### THIRD MODULE: INCLUDE COLOC IN THE \n",
    "resolvedColoc = (\n",
    "    (\n",
    "        newColoc.withColumnRenamed(\"geneId\", \"targetId\")\n",
    "        .join(\n",
    "            gwasComplete.withColumnRenamed(\"studyLocusId\", \"leftStudyLocusId\"),\n",
    "            on=[\"leftStudyLocusId\", \"targetId\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .join(  ### propagated using parent terms\n",
    "            diseases.selectExpr(\n",
    "                \"id as diseaseId\", \"name\", \"parents\", \"therapeuticAreas\"\n",
    "            ),\n",
    "            on=\"diseaseId\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"diseaseId\",\n",
    "            F.explode_outer(F.concat(F.array(F.col(\"diseaseId\")), F.col(\"parents\"))),\n",
    "        )\n",
    "        .drop(\"parents\", \"oldDiseaseId\")\n",
    "    ).withColumn(\n",
    "        \"colocDoE\",\n",
    "        F.when(\n",
    "            F.col(\"rightStudyType\").isin(\n",
    "                [\"eqtl\", \"pqtl\", \"tuqtl\", \"sceqtl\", \"sctuqtl\"]\n",
    "            ),\n",
    "            F.when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"GoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"LoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"LoF_protect\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"GoF_protect\"),\n",
    "            ),\n",
    "        ).when(\n",
    "            F.col(\"rightStudyType\").isin(\n",
    "                [\"sqtl\", \"scsqtl\"]\n",
    "            ),  ### opposite directionality than sqtl\n",
    "            F.when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"LoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"GoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"GoF_protect\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"LoF_protect\"),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    # .persist()\n",
    ")\n",
    "print(\"loaded resolvedColloc\")\n",
    "\n",
    "datasource_filter = [\n",
    "#   \"ot_genetics_portal\",\n",
    "    \"gwas_credible_sets\",\n",
    "    \"gene_burden\",\n",
    "    \"eva\",\n",
    "    \"eva_somatic\",\n",
    "    \"gene2phenotype\",\n",
    "    \"orphanet\",\n",
    "    \"cancer_gene_census\",\n",
    "    \"intogen\",\n",
    "    \"impc\",\n",
    "    \"chembl\",\n",
    "]\n",
    "\n",
    "assessment, evidences, actionType, oncolabel = temporary_directionOfEffect(\n",
    "    path_n, datasource_filter\n",
    ")\n",
    "\n",
    "print(\"run temporary direction of effect\")\n",
    "\n",
    "\n",
    "print(\"built drugApproved dataset\")\n",
    "\n",
    "\n",
    "#### FOURTH MODULE BUILDING CHEMBL ASSOCIATIONS - HERE TAKE CARE WITH FILTERING STEP \n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter((F.col(\"datasourceId\") == \"chembl\"))\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(F.col(\"clinicalPhase\")).over(\n",
    "                Window.partitionBy(\"targetId\", \"diseaseId\")\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\")\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    #.filter(F.col(\"coherencyDiagonal\") == \"coherent\")\n",
    "    .drop(\n",
    "        \"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\"\n",
    "    )\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    "    # .persist()\n",
    ")\n",
    "\n",
    "####2 Define agregation function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def convertTuple(tup):\n",
    "    st = \",\".join(map(str, tup))\n",
    "    return st\n",
    "\n",
    "\n",
    "#####3 run in a function\n",
    "def aggregations_original(\n",
    "    df,\n",
    "    data,\n",
    "    listado,\n",
    "    comparisonColumn,\n",
    "    comparisonType,\n",
    "    predictionColumn,\n",
    "    predictionType,\n",
    "    today_date,\n",
    "):\n",
    "    wComparison = Window.partitionBy(comparisonColumn)\n",
    "    wPrediction = Window.partitionBy(predictionColumn)\n",
    "    wPredictionComparison = Window.partitionBy(comparisonColumn, predictionColumn)\n",
    "    results = []\n",
    "    # uniqIds = df.select(\"targetId\", \"diseaseId\").distinct().count()\n",
    "    out = (\n",
    "        df.withColumn(\"comparisonType\", F.lit(comparisonType))\n",
    "        .withColumn(\"dataset\", F.lit(data))\n",
    "        .withColumn(\"predictionType\", F.lit(predictionType))\n",
    "        # .withColumn(\"total\", F.lit(uniqIds))\n",
    "        .withColumn(\"a\", F.count(\"targetId\").over(wPredictionComparison))\n",
    "        .withColumn(\"comparisonColumn\", F.lit(comparisonColumn))\n",
    "        .withColumn(\"predictionColumnValue\", F.lit(predictionColumn))\n",
    "        .withColumn(\n",
    "            \"predictionTotal\",\n",
    "            F.count(\"targetId\").over(wPrediction),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"comparisonTotal\",\n",
    "            F.count(\"targetId\").over(wComparison),\n",
    "        )\n",
    "        .select(\n",
    "            F.col(predictionColumn).alias(\"prediction\"),\n",
    "            F.col(comparisonColumn).alias(\"comparison\"),\n",
    "            \"dataset\",\n",
    "            \"comparisonColumn\",\n",
    "            \"predictionColumnValue\",\n",
    "            \"comparisonType\",\n",
    "            \"predictionType\",\n",
    "            \"a\",\n",
    "            \"predictionTotal\",\n",
    "            \"comparisonTotal\",\n",
    "        )\n",
    "        .filter(F.col(\"prediction\").isNotNull())\n",
    "        .filter(F.col(\"comparison\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    '''\n",
    "    out.write.mode(\"overwrite\").parquet(\n",
    "        \"gs://ot-team/jroldan/\"\n",
    "        + str(\n",
    "            today_date\n",
    "            + \"_\"\n",
    "            + \"analysis/\"\n",
    "            + data\n",
    "            # + \"_propagated\"\n",
    "            + \"/\"\n",
    "            + comparisonColumn\n",
    "            + \"_\"\n",
    "            + comparisonType\n",
    "            + \"_\"\n",
    "            + predictionColumn\n",
    "            + \".parquet\"\n",
    "        )\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    listado.append(\n",
    "        \"gs://ot-team/jroldan/\"\n",
    "        + str(\n",
    "            today_date\n",
    "            + \"_\"\n",
    "            + \"analysis/\"\n",
    "            + data\n",
    "            # + \"_propagated\"\n",
    "            + \"/\"\n",
    "            + comparisonColumn\n",
    "            + \"_\"\n",
    "            + comparisonType\n",
    "            + \"_\"\n",
    "            + predictionColumn\n",
    "            + \".parquet\"\n",
    "        )\n",
    "    )\n",
    "    path = \"gs://ot-team/jroldan/\" + str(\n",
    "        today_date\n",
    "        + \"_\"\n",
    "        + \"analysis/\"\n",
    "        + data\n",
    "        # + \"_propagated\"\n",
    "        + \"/\"\n",
    "        + comparisonColumn\n",
    "        + \"_\"\n",
    "        + comparisonType\n",
    "        + \"_\"\n",
    "        + predictionColumn\n",
    "        + \".parquet\"\n",
    "    )\n",
    "    print(path)\n",
    "    \n",
    "    ### making analysis\n",
    "    array1 = np.delete(\n",
    "        out.join(full_data, on=[\"prediction\", \"comparison\"], how=\"outer\")\n",
    "        .groupBy(\"comparison\")\n",
    "        .pivot(\"prediction\")\n",
    "        .agg(F.first(\"a\"))\n",
    "        .sort(F.col(\"comparison\").desc())\n",
    "        .select(\"comparison\", \"yes\", \"no\")\n",
    "        .fillna(0)\n",
    "        .toPandas()\n",
    "        .to_numpy(),\n",
    "        [0],\n",
    "        1,\n",
    "    )\n",
    "    total = np.sum(array1)\n",
    "    res_npPhaseX = np.array(array1, dtype=int)\n",
    "    resX = convertTuple(fisher_exact(res_npPhaseX, alternative=\"two-sided\"))\n",
    "    resx_CI = convertTuple(\n",
    "        odds_ratio(res_npPhaseX).confidence_interval(confidence_level=0.95)\n",
    "    )\n",
    "\n",
    "    result_st.append(resX)\n",
    "    result_ci.append(resx_CI)\n",
    "    (rs_result, rs_ci) = relative_success(array1)\n",
    "    results.extend(\n",
    "        [\n",
    "            comparisonType,\n",
    "            comparisonColumn,\n",
    "            predictionColumn,\n",
    "            round(float(resX.split(\",\")[0]), 2),\n",
    "            float(resX.split(\",\")[1]),\n",
    "            round(float(resx_CI.split(\",\")[0]), 2),\n",
    "            round(float(resx_CI.split(\",\")[1]), 2),\n",
    "            str(total),\n",
    "            np.array(res_npPhaseX).tolist(),\n",
    "            round(float(rs_result), 2),\n",
    "            round(float(rs_ci[0]), 2),\n",
    "            round(float(rs_ci[1]), 2),\n",
    "            # studies,\n",
    "            # tissues,\n",
    "            path,\n",
    "        ]\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "#### 3 Loop over different datasets (as they will have different rows and columns)\n",
    "\n",
    "\n",
    "def comparisons_df_iterative(elements):\n",
    "    #toAnalysis = [(key, value) for key, value in disdic.items() if value == projectId]\n",
    "    toAnalysis = [(col, \"predictor\") for col in elements]\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"comparison\", StringType(), True),\n",
    "            StructField(\"comparisonType\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    comparisons = spark.createDataFrame(toAnalysis, schema=schema)\n",
    "    ### include all the columns as predictor\n",
    "\n",
    "    predictions = spark.createDataFrame(\n",
    "        data=[\n",
    "            (\"Phase>=4\", \"clinical\"),\n",
    "            ('Phase>=3','clinical'),\n",
    "            ('Phase>=2','clinical'),\n",
    "            ('Phase>=1','clinical'),\n",
    "            (\"PhaseT\", \"clinical\"),\n",
    "        ]\n",
    "    )\n",
    "    return comparisons.join(predictions, how=\"full\").collect()\n",
    "\n",
    "\n",
    "print(\"load comparisons_df_iterative function\")\n",
    "\n",
    "\n",
    "full_data = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\"yes\", \"yes\"),\n",
    "        (\"yes\", \"no\"),\n",
    "        (\"no\", \"yes\"),\n",
    "        (\"no\", \"no\"),\n",
    "    ],\n",
    "    schema=StructType(\n",
    "        [\n",
    "            StructField(\"prediction\", StringType(), True),\n",
    "            StructField(\"comparison\", StringType(), True),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\"created full_data and lists\")\n",
    "\n",
    "#rightTissue = spark.read.csv(\n",
    "#    'gs://ot-team/jroldan/analysis/20250526_rightTissue.csv',\n",
    "#    header=True,\n",
    "#).drop(\"_c0\")\n",
    "\n",
    "print(\"loaded rightTissue dataset\")\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\")\n",
    "    .count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\"))\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "print(\"built negativeTD dataset\")\n",
    "\n",
    "print(\"built bench2 dataset\")\n",
    "\n",
    "###### cut from here\n",
    "print(\"looping for variables_study\")\n",
    "\n",
    "#### new part with chatgpt -- TEST\n",
    "\n",
    "## QUESTIONS TO ANSWER:\n",
    "# HAVE ECAVIAR >=0.8\n",
    "# HAVE COLOC \n",
    "# HAVE COLOC >= 0.8\n",
    "# HAVE COLOC + ECAVIAR >= 0.01\n",
    "# HAVE COLOC >= 0.8 + ECAVIAR >= 0.01\n",
    "# RIGHT JOING WITH CHEMBL \n",
    "\n",
    "### FIFTH MODULE: BUILDING BENCHMARK OF THE DATASET TO EXTRACT EHE ANALYSIS \n",
    "\n",
    "resolvedColocFiltered = resolvedColoc.filter((F.col('clpp')>=0.01) | (F.col('h4')>=0.8))\n",
    "benchmark = (\n",
    "    (\n",
    "        resolvedColocFiltered.filter( ## .filter(F.col(\"betaGwas\") < 0)\n",
    "        F.col(\"name\") != \"COVID-19\"\n",
    "    )\n",
    "        .join(  ### select just GWAS giving protection\n",
    "            analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\"  ### RIGHT SIDE\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"AgreeDrug\",\n",
    "            F.when(\n",
    "                (F.col(\"drugGoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"GoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"drugLoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"LoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "    )  #### remove COVID-19 associations\n",
    ").join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    "\n",
    "\n",
    "### drug mechanism of action\n",
    "mecact_path = f\"{path_n}drug_mechanism_of_action/\" #  mechanismOfAction == old version\n",
    "mecact = spark.read.parquet(mecact_path)\n",
    "\n",
    "inhibitors = [\n",
    "    \"RNAI INHIBITOR\",\n",
    "    \"NEGATIVE MODULATOR\",\n",
    "    \"NEGATIVE ALLOSTERIC MODULATOR\",\n",
    "    \"ANTAGONIST\",\n",
    "    \"ANTISENSE INHIBITOR\",\n",
    "    \"BLOCKER\",\n",
    "    \"INHIBITOR\",\n",
    "    \"DEGRADER\",\n",
    "    \"INVERSE AGONIST\",\n",
    "    \"ALLOSTERIC ANTAGONIST\",\n",
    "    \"DISRUPTING AGENT\",\n",
    "]\n",
    "\n",
    "activators = [\n",
    "    \"PARTIAL AGONIST\",\n",
    "    \"ACTIVATOR\",\n",
    "    \"POSITIVE ALLOSTERIC MODULATOR\",\n",
    "    \"POSITIVE MODULATOR\",\n",
    "    \"AGONIST\",\n",
    "    \"SEQUESTERING AGENT\",  ## lost at 31.01.2025\n",
    "    \"STABILISER\",\n",
    "    # \"EXOGENOUS GENE\", ## added 24.06.2025\n",
    "    # \"EXOGENOUS PROTEIN\" ## added 24.06.2025\n",
    "]\n",
    "\n",
    "\n",
    "actionType = (\n",
    "        mecact.select(\n",
    "            F.explode_outer(\"chemblIds\").alias(\"drugId\"),\n",
    "            \"actionType\",\n",
    "            \"mechanismOfAction\",\n",
    "            \"targets\",\n",
    "        )\n",
    "        .select(\n",
    "            F.explode_outer(\"targets\").alias(\"targetId\"),\n",
    "            \"drugId\",\n",
    "            \"actionType\",\n",
    "            \"mechanismOfAction\",\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"drugId\")\n",
    "        .agg(F.collect_set(\"actionType\").alias(\"actionType2\"))\n",
    "    ).withColumn('nMoA', F.size(F.col('actionType2')))\n",
    "\n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter((F.col(\"datasourceId\") == \"chembl\")).join(actionType, on=['targetId','drugId'], how='left')\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(F.col(\"clinicalPhase\")).over(\n",
    "                Window.partitionBy(\"targetId\", \"diseaseId\")\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\",'actionType2')\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    #.filter(F.col(\"coherencyDiagonal\") == \"coherent\")\n",
    "    .drop(\n",
    "        \"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\"\n",
    "    )\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    ")\n",
    "\n",
    "benchmark = (\n",
    "    (\n",
    "        resolvedColocFiltered.filter( ## .filter(F.col(\"betaGwas\") < 0)\n",
    "        F.col(\"name\") != \"COVID-19\"\n",
    "    )\n",
    "        .join(  ### select just GWAS giving protection\n",
    "            analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\"  ### RIGHT SIDE\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"AgreeDrug\",\n",
    "            F.when(\n",
    "                (F.col(\"drugGoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"GoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"drugLoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"LoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "    )  #### remove COVID-19 associations\n",
    ").join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\")\n",
    "    .count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\"))\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "### create disdic dictionary\n",
    "disdic={}\n",
    "\n",
    "# --- Configuration for your iterative pivoting ---\n",
    "group_by_columns = ['targetId', 'diseaseId','phase4Clean','phase3Clean','phase2Clean','phase1Clean','PhaseT']\n",
    "#columns_to_pivot_on = ['actionType2', 'biosampleName', 'projectId', 'rightStudyType','colocalisationMethod']\n",
    "columns_to_pivot_on = ['projectId']\n",
    "columns_to_aggregate = ['NoneCellYes', 'NdiagonalYes','hasGenetics'] # The values you want to collect in the pivoted cells\n",
    "all_pivoted_dfs = {}\n",
    "\n",
    "doe_columns=[\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "diagonal_lof=['LoF_protect','GoF_risk']\n",
    "diagonal_gof=['LoF_risk','GoF_protect']\n",
    "\n",
    "conditions = [\n",
    "    F.when(F.col(c) == F.col(\"maxDoE\"), F.lit(c)).otherwise(F.lit(None)) for c in doe_columns\n",
    "    ]\n",
    "print('entering the big loops')\n",
    "# --- Nested Loops for Dynamic Pivoting ---\n",
    "for agg_col_name in columns_to_aggregate:\n",
    "    for pivot_col_name in columns_to_pivot_on:\n",
    "        print(f\"\\n--- Creating DataFrame for Aggregation: '{agg_col_name}' and Pivot: '{pivot_col_name}' ---\")\n",
    "        current_col_pvalue_order_window = Window.partitionBy(\"targetId\", \"diseaseId\", \"maxClinPhase\", pivot_col_name).orderBy(F.col('colocalisationMethod').asc(), F.col(\"qtlPValueExponent\").asc())\n",
    "        test2=discrepancifier(benchmark.withColumn('actionType2', F.concat_ws(\",\", F.col(\"actionType2\"))).withColumn('qtlColocDoE',F.first('colocDoE').over(current_col_pvalue_order_window)).groupBy(\n",
    "        \"targetId\", \"diseaseId\", \"maxClinPhase\", \"drugLoF_protect\", \"drugGoF_protect\",pivot_col_name)\n",
    "        .pivot(\"colocDoE\")\n",
    "        .count()\n",
    "        .withColumnRenamed('drugLoF_protect', 'LoF_protect_ch')\n",
    "        .withColumnRenamed('drugGoF_protect', 'GoF_protect_ch')).withColumn( ## .filter(F.col('coherencyDiagonal')!='noEvid')\n",
    "    \"arrayN\", F.array(*[F.col(c) for c in doe_columns])\n",
    "    ).withColumn(\n",
    "        \"maxDoE\", F.array_max(F.col(\"arrayN\"))\n",
    "    ).withColumn(\"maxDoE_names\", F.array(*conditions)\n",
    "    ).withColumn(\"maxDoE_names\", F.expr(\"filter(maxDoE_names, x -> x is not null)\")\n",
    "    ).withColumn(\n",
    "        \"NoneCellYes\",\n",
    "        F.when((F.col(\"LoF_protect_ch\").isNotNull() & (F.col('GoF_protect_ch').isNull())) & (F.array_contains(F.col(\"maxDoE_names\"), F.lit(\"LoF_protect\")))==True, F.lit('yes'))\n",
    "        .when((F.col(\"GoF_protect_ch\").isNotNull() & (F.col('LoF_protect_ch').isNull())) & (F.array_contains(F.col(\"maxDoE_names\"), F.lit(\"GoF_protect\")))==True, F.lit('yes')\n",
    "            ).otherwise(F.lit('no'))  # If the value is null, return null # Otherwise, check if name is in array\n",
    "    ).withColumn(\n",
    "        \"NdiagonalYes\",\n",
    "        F.when((F.col(\"LoF_protect_ch\").isNotNull() & (F.col('GoF_protect_ch').isNull())) & \n",
    "            (F.size(F.array_intersect(F.col(\"maxDoE_names\"), F.array([F.lit(x) for x in diagonal_lof]))) > 0),\n",
    "            F.lit(\"yes\")\n",
    "        ).when((F.col(\"GoF_protect_ch\").isNotNull() & (F.col('LoF_protect_ch').isNull())) & \n",
    "            (F.size(F.array_intersect(F.col(\"maxDoE_names\"), F.array([F.lit(x) for x in diagonal_gof]))) > 0),\n",
    "            F.lit(\"yes\")\n",
    "        ).otherwise(F.lit('no'))\n",
    "    ).withColumn(\n",
    "        \"drugCoherency\",\n",
    "        F.when(\n",
    "            (F.col(\"LoF_protect_ch\").isNotNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNull()), F.lit(\"coherent\")\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"LoF_protect_ch\").isNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNotNull()), F.lit(\"coherent\")\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"LoF_protect_ch\").isNotNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNotNull()), F.lit(\"dispar\")\n",
    "        )\n",
    "        .otherwise(F.lit(\"other\")),\n",
    "    ).join(negativeTD, on=[\"targetId\", \"diseaseId\"], how=\"left\").withColumn(\n",
    "        \"PhaseT\",\n",
    "        F.when(F.col(\"stopReason\") == \"Negative\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase4Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") == 4) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase3Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 3) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase2Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 2) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase1Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 1) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"hasGenetics\",\n",
    "        F.when(F.col(\"coherencyDiagonal\") != \"noEvid\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    )\n",
    "        # 1. Get distinct values for the pivot column (essential for pivot())\n",
    "        # This brings a small amount of data to the driver, but is necessary for the pivot schema.\n",
    "        #distinct_pivot_values = [row[0] for row in test2.select(pivot_col_name).distinct().collect()]\n",
    "        # print(f\"Distinct values for '{pivot_col_name}': {distinct_pivot_values}\")\n",
    "\n",
    "        # 2. Perform the groupBy, pivot, and aggregate operations\n",
    "        # The .pivot() function requires the list of distinct values for better performance\n",
    "        # and correct schema inference.\n",
    "        pivoted_df = (\n",
    "            test2.groupBy(*group_by_columns)\n",
    "            .pivot(pivot_col_name) # Provide distinct values distinct_pivot_values\n",
    "            .agg(F.collect_set(F.col(agg_col_name))) # Collect all values into a set\n",
    "            .fillna(0) # Fill cells that have no data with an empty list instead of null\n",
    "        )\n",
    "        # 3. Add items to dictionary to map the columns:\n",
    "        # filter out None and 'null':\n",
    "        datasetColumns=pivoted_df.columns\n",
    "        filtered = [x for x in datasetColumns if x is not None and x != 'null']\n",
    "        # using list comprehension\n",
    "        for item in filtered:\n",
    "            disdic[item] = pivot_col_name\n",
    "\n",
    "        # 3. Add the 'data' literal column dynamically\n",
    "        # This column indicates which aggregation column was used.\n",
    "        #pivoted_df = pivoted_df.withColumn('data', F.lit(f'Drug_{agg_col_name}'))\n",
    "\n",
    "        array_columns_to_convert = [\n",
    "            field.name for field in pivoted_df.schema.fields\n",
    "            if isinstance(field.dataType, ArrayType)\n",
    "        ]\n",
    "        print(f\"Identified ArrayType columns for conversion: {array_columns_to_convert}\")\n",
    "\n",
    "        # 4. Apply the conversion logic to each identified array column\n",
    "        df_after_conversion = pivoted_df # Start with the pivoted_df\n",
    "        for col_to_convert in array_columns_to_convert:\n",
    "            df_after_conversion = df_after_conversion.withColumn(\n",
    "                col_to_convert,\n",
    "                F.when(F.col(col_to_convert).isNull(), F.lit('no'))          # Handle NULLs (from pivot for no data)\n",
    "                .when(F.size(F.col(col_to_convert)) == 0, F.lit('no'))       # Empty array -> 'no'\n",
    "                .when(F.array_contains(F.col(col_to_convert), F.lit('yes')), F.lit('yes')) # Contains 'yes' -> 'yes'\n",
    "                .when(F.array_contains(F.col(col_to_convert), F.lit('no')), F.lit('no'))   # Contains 'no' -> 'no'\n",
    "                .otherwise(F.lit('no')) # Fallback for unexpected array content (e.g., ['other'], ['yes','no'])\n",
    "            )\n",
    "\n",
    "        # 4. Generate a unique name for this DataFrame and store it\n",
    "        df_key = f\"df_pivot_{agg_col_name.lower()}_by_{pivot_col_name.lower()}\"\n",
    "        all_pivoted_dfs[df_key] = df_after_conversion.withColumnRenamed( 'phase4Clean','Phase>=4'\n",
    "        ).withColumnRenamed('phase3Clean','Phase>=3'\n",
    "        ).withColumnRenamed('phase2Clean','Phase>=2'\n",
    "        ).withColumnRenamed('phase1Clean','Phase>=1')\n",
    "\n",
    "\n",
    "# --- Accessing your generated DataFrames ---\n",
    "print(\"\\n--- All generated DataFrames are stored in 'all_pivoted_dfs' dictionary ---\")\n",
    "print(\"Keys available:\", all_pivoted_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'df_pivot_nonecellyes_by_projectid': DataFrame[targetId: string, diseaseId: string, Phase>=4: string, Phase>=3: string, Phase>=2: string, Phase>=1: string, PhaseT: string, null: string, Alasoo_2018: string, Aygun_2021: string, BLUEPRINT: string, Bossini-Castillo_2019: string, BrainSeq: string, Braineac2: string, CAP: string, CEDAR: string, CommonMind: string, Cytoimmgen: string, FUSION: string, Fairfax_2012: string, Fairfax_2014: string, GENCORD: string, GEUVADIS: string, GTEx: string, Gilchrist_2021: string, HipSci: string, Jerber_2021: string, Kasela_2017: string, Kim-Hellmuth_2017: string, Lepik_2017: string, Naranbhai_2015: string, Nathan_2022: string, Nedelec_2016: string, OneK1K: string, PISA: string, Peng_2018: string, Perez_2022: string, PhLiPS: string, Quach_2016: string, ROSMAP: string, Randolph_2021: string, Schmiedel_2018: string, Schwartzentruber_2018: string, Steinberg_2020: string, Sun_2018: string, TwinsUK: string, UKB_PPP_EUR: string, Walker_2019: string, Young_2019: string, iPSCORE: string, van_de_Bunt_2015: string],\n",
       " 'df_pivot_ndiagonalyes_by_projectid': DataFrame[targetId: string, diseaseId: string, Phase>=4: string, Phase>=3: string, Phase>=2: string, Phase>=1: string, PhaseT: string, null: string, Alasoo_2018: string, Aygun_2021: string, BLUEPRINT: string, Bossini-Castillo_2019: string, BrainSeq: string, Braineac2: string, CAP: string, CEDAR: string, CommonMind: string, Cytoimmgen: string, FUSION: string, Fairfax_2012: string, Fairfax_2014: string, GENCORD: string, GEUVADIS: string, GTEx: string, Gilchrist_2021: string, HipSci: string, Jerber_2021: string, Kasela_2017: string, Kim-Hellmuth_2017: string, Lepik_2017: string, Naranbhai_2015: string, Nathan_2022: string, Nedelec_2016: string, OneK1K: string, PISA: string, Peng_2018: string, Perez_2022: string, PhLiPS: string, Quach_2016: string, ROSMAP: string, Randolph_2021: string, Schmiedel_2018: string, Schwartzentruber_2018: string, Steinberg_2020: string, Sun_2018: string, TwinsUK: string, UKB_PPP_EUR: string, Walker_2019: string, Young_2019: string, iPSCORE: string, van_de_Bunt_2015: string],\n",
       " 'df_pivot_hasgenetics_by_projectid': DataFrame[targetId: string, diseaseId: string, Phase>=4: string, Phase>=3: string, Phase>=2: string, Phase>=1: string, PhaseT: string, null: string, Alasoo_2018: string, Aygun_2021: string, BLUEPRINT: string, Bossini-Castillo_2019: string, BrainSeq: string, Braineac2: string, CAP: string, CEDAR: string, CommonMind: string, Cytoimmgen: string, FUSION: string, Fairfax_2012: string, Fairfax_2014: string, GENCORD: string, GEUVADIS: string, GTEx: string, Gilchrist_2021: string, HipSci: string, Jerber_2021: string, Kasela_2017: string, Kim-Hellmuth_2017: string, Lepik_2017: string, Naranbhai_2015: string, Nathan_2022: string, Nedelec_2016: string, OneK1K: string, PISA: string, Peng_2018: string, Perez_2022: string, PhLiPS: string, Quach_2016: string, ROSMAP: string, Randolph_2021: string, Schmiedel_2018: string, Schwartzentruber_2018: string, Steinberg_2020: string, Sun_2018: string, TwinsUK: string, UKB_PPP_EUR: string, Walker_2019: string, Young_2019: string, iPSCORE: string, van_de_Bunt_2015: string]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pivoted_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['othersProjectId_only',\n",
       " 'estimulated_only',\n",
       " 'nonStimulated_only',\n",
       " 'cellLine',\n",
       " 'nonCellLine',\n",
       " 'GTExUKB']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].drop('null').columns[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with df_pivot_nonecellyes_by_projectid\n",
      "There are  6 columns to analyse with phases\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/stimulated_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/stimulated_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/stimulated_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/stimulated_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/stimulated_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonStimulated_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonStimulated_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonStimulated_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonStimulated_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonStimulated_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "df unpersisted\n",
      "working with df_pivot_ndiagonalyes_by_projectid\n",
      "There are  6 columns to analyse with phases\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/stimulated_only_predictor_Phase>=4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/stimulated_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/stimulated_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/stimulated_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/stimulated_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonStimulated_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonStimulated_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonStimulated_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonStimulated_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonStimulated_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "df unpersisted\n",
      "working with df_pivot_hasgenetics_by_projectid\n",
      "There are  6 columns to analyse with phases\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/stimulated_only_predictor_Phase>=4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/stimulated_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/stimulated_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/stimulated_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/stimulated_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonStimulated_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonStimulated_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonStimulated_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonStimulated_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonStimulated_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-17_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "df unpersisted\n",
      "importing functions\n",
      "imported functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe written \n",
      " Analysis finished\n"
     ]
    }
   ],
   "source": [
    "############## HYBRID ##############\n",
    "####################################\n",
    "def strip_only(lst):\n",
    "    return [x.removesuffix(\"_only\") for x in lst]  # Python 3.9+\n",
    "    # or: return [x[:-5] if x.endswith(\"_only\") else x for x in lst]\n",
    "\n",
    "\n",
    "##### PROJECTID\n",
    "project_keys=[f\"{k}_only\" for k,v in disdic.items() if v == 'projectId']\n",
    "main=['GTEx_only', 'UKB_PPP_EUR_only']\n",
    "#stimulated=['Alasoo_2018_only','Cytoimmgen_only','Fairfax_2014_only','Kim-Hellmuth_2017_only','Nathan_2022_only','Nedelec_2016_only','Quach_2016_only','Randolph_2021_only','Schmiedel_2018_only']\n",
    "#cellLine=['CAP_only','HipSci_only','iPSCORE_only','Jerber_2021_only','PhLiPS_only','Schwartzentruber_2018_only','TwinsUK_only']\n",
    "\n",
    "derivedCellLine=['TwinsUK_only','PhLiPS_only','CAP_only','GENCORD_only','Sun_2018_only','Nedelec_2016_only']\n",
    "canonicalCellLine=['Alasoo_2018_only','Jerber_2021_only','GEUVADIS_only','iPSCORE_only','Aygun_2021_only','Schwartzentruber_2018_only']\n",
    "stimulated=['Schmiedel_2018_only','Bossini-Castillo_2019_only','Alasoo_2018_only','Cytoimmgen_only','Gilchrist_2021_only','CAP_only','Quach_2016_only','Randolph_2021_only','Sun_2018_only','Nedelec_2016_only','Kim-Hellmuth_2017_only']\n",
    "\n",
    "# Apply\n",
    "main = strip_only(main)\n",
    "canonicalCellLine = strip_only(canonicalCellLine)\n",
    "derivedCellLine = strip_only(derivedCellLine)\n",
    "stimulated = strip_only(stimulated)\n",
    "\n",
    "others=[item for item in strip_only(project_keys) if item not in main]\n",
    "nonStimulated=[item for item in strip_only(project_keys) if item not in stimulated]\n",
    "nonCanonicalCellLine = [item for item in strip_only(project_keys) if item not in canonicalCellLine]\n",
    "nonDerivedCellLine = [item for item in strip_only(project_keys) if item not in derivedCellLine]\n",
    "\n",
    "#otherCellLine=[item for item in strip_only(project_keys) if item not in cellLine]\n",
    "\n",
    "\n",
    "def _or_yes(df, cols):\n",
    "    \"\"\"Return a Column that is TRUE if any of the given columns == 'yes'.\n",
    "       Ignores columns not present in df. If none present, returns FALSE.\n",
    "    \"\"\"\n",
    "    present = [c for c in cols if c in df.columns]\n",
    "    if not present:\n",
    "        return F.lit(False)\n",
    "    # (col == 'yes') OR (col == 'yes') OR ...\n",
    "    exprs = [F.col(c) == \"yes\" for c in present]\n",
    "    return reduce(lambda a, b: a | b, exprs)\n",
    "\n",
    "def add_project_group_flags(df, main, canonicalCellLine, derivedCellLine,stimulated):\n",
    "    # project keys = every *_only column in this DF\n",
    "    project_keys = [c for c in df.columns if c.endswith(\"_only\")]\n",
    "\n",
    "    # Derived buckets\n",
    "    others=[item for item in strip_only(project_keys) if item not in main]\n",
    "    nonStimulated=[item for item in strip_only(project_keys) if item not in stimulated]\n",
    "    nonCanonicalCellLine = [item for item in strip_only(project_keys) if item not in canonicalCellLine]\n",
    "    nonDerivedCellLine = [item for item in strip_only(project_keys) if item not in derivedCellLine]\n",
    "\n",
    "    # Conditions\n",
    "    condition1 = _or_yes(df, others)  \n",
    "    condition1 = _or_yes(df, main)  \n",
    "    condition2 = _or_yes(df, stimulated)    \n",
    "    condition3 = _or_yes(df, nonStimulated)  \n",
    "    condition4 = _or_yes(df, canonicalCellLine)     \n",
    "    condition5 = _or_yes(df, nonCanonicalCellLine)  \n",
    "    condition7 = _or_yes(df, derivedCellLine)  \n",
    "    condition8 = _or_yes(df, nonDerivedCellLine)         \n",
    "\n",
    "    # Add columns (write 'yes'/'no')\n",
    "    return (\n",
    "        df.withColumn(\"othersProjectId_only\", F.when(condition1, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"GTExUKB_only\",     F.when(condition2, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"stimulated_only\",   F.when(condition3, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"nonStimulated\",             F.when(condition4, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"canonicalCellLine\",          F.when(condition5, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"nonCanonicalCellLine\",              F.when(condition6, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"derivedCellLine\",          F.when(condition7, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"nonDerivedCellLine\",              F.when(condition8, \"yes\").otherwise(\"no\"))\n",
    "    )\n",
    "\n",
    "# --- Apply to the dict entry you mentioned ---\n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = add_project_group_flags(\n",
    "    all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'],main=main, canonicalCellLine=canonicalCellLine, derivedCellLine=derivedCellLine,stimulated=stimulated)\n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = add_project_group_flags(\n",
    "    all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'], main, canonicalCellLine, derivedCellLine,stimulated)\n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = add_project_group_flags(\n",
    "    all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'], main, canonicalCellLine, derivedCellLine,stimulated)\n",
    "# If you wanted to apply to every DF in the dict (only if they all share *_only columns):\n",
    "# for k, df in all_pivoted_dfs.items():\n",
    "#     all_pivoted_dfs[k] = add_project_group_flags(df, main, stimulated, cellLine)\n",
    "\n",
    "###append to dictionary\n",
    "\n",
    "disdic.update({'othersProjectId': 'projectId','Stimulated': 'projectId','cellLine': 'projectId', 'othersBiosampleName_only': 'biosampleName', 'otherRightStudyType':'rightStudyType'})\n",
    "\n",
    "###################################\n",
    "###################################\n",
    "result = []\n",
    "result_st = []\n",
    "result_ci = []\n",
    "array2 = []\n",
    "listado = []\n",
    "result_all = []\n",
    "today_date = str(date.today())\n",
    "\n",
    "for key,df in all_pivoted_dfs.items():\n",
    "\n",
    "    print(f'working with {key}')\n",
    "    parts = key.split('_by_') ### take the part of key belonging to column name\n",
    "    column_name = parts[1] ### take the last part which is column name\n",
    "    all_pivoted_dfs[key].persist()\n",
    "    #unique_values = all_pivoted_dfs[key].drop('null').columns[7:]\n",
    "    unique_values = all_pivoted_dfs[key].drop('null').columns[-6:] ### just the interesting columns for us \n",
    "    filtered_unique_values = [x for x in unique_values if x is not None and x != 'null']\n",
    "    print('There are ', len(filtered_unique_values), 'columns to analyse with phases')\n",
    "    rows = comparisons_df_iterative(filtered_unique_values)\n",
    "\n",
    "    # If needed, now process the rest\n",
    "    for row in rows:\n",
    "        print('performing', row)\n",
    "        results = aggregations_original(\n",
    "            all_pivoted_dfs[key], key, listado, *row, today_date\n",
    "        )\n",
    "        result_all.append(results)\n",
    "        print('results appended')\n",
    "    all_pivoted_dfs[key].unpersist()\n",
    "    print('df unpersisted')\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"group\", StringType(), True),\n",
    "        StructField(\"comparison\", StringType(), True),\n",
    "        StructField(\"phase\", StringType(), True),\n",
    "        StructField(\"oddsRatio\", DoubleType(), True),\n",
    "        StructField(\"pValue\", DoubleType(), True),\n",
    "        StructField(\"lowerInterval\", DoubleType(), True),\n",
    "        StructField(\"upperInterval\", DoubleType(), True),\n",
    "        StructField(\"total\", StringType(), True),\n",
    "        StructField(\"values\", ArrayType(ArrayType(IntegerType())), True),\n",
    "        StructField(\"relSuccess\", DoubleType(), True),\n",
    "        StructField(\"rsLower\", DoubleType(), True),\n",
    "        StructField(\"rsUpper\", DoubleType(), True),\n",
    "        StructField(\"path\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "import re\n",
    "\n",
    "# Define the list of patterns to search for\n",
    "patterns = [\n",
    "    \"_only\",\n",
    "    #\"_tissue\",\n",
    "    #\"_isSignalFromRightTissue\",\n",
    "    \"_isRightTissueSignalAgreed\",\n",
    "]\n",
    "# Create a regex pattern to match any of the substrings\n",
    "regex_pattern = \"(\" + \"|\".join(map(re.escape, patterns)) + \")\"\n",
    "\n",
    "# Convert list of lists to DataFrame\n",
    "df = (\n",
    "    spreadSheetFormatter(spark.createDataFrame(result_all, schema=schema))\n",
    "    .withColumn(\n",
    "        \"prefix\",\n",
    "        F.regexp_replace(\n",
    "            F.col(\"comparison\"), regex_pattern + \".*\", \"\"\n",
    "        ),  # Extract part before the pattern\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"suffix\",\n",
    "        F.regexp_extract(\n",
    "            F.col(\"comparison\"), regex_pattern, 0\n",
    "        ),  # Extract the pattern itself\n",
    "    )\n",
    ")\n",
    "\n",
    "### annotate projectId, tissue, qtl type and doe type:\n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr=create_map([F.lit(x) for x in chain(*disdic.items())])\n",
    "\n",
    "df_annot=df.withColumn('annotation',mapping_expr.getItem(F.col('prefix')))\n",
    "\n",
    "df_annot.toPandas().to_csv(\n",
    "    f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_AllPhases.csv\"\n",
    ")\n",
    "\n",
    "print(\"dataframe written \\n Analysis finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark session created at 2025-09-18 05:40:10.768588\n",
      "Analysis started on 2025-09-18 at  2025-09-18 05:40:10.768588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 05:40:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/09/18 05:40:17 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully with the following configurations:\n",
      "  spark.driver.memory: 24g\n",
      "  spark.executor.memory: 32g\n",
      "  spark.executor.cores: 4\n",
      "  spark.executor.instances: 12\n",
      "  spark.yarn.executor.memoryOverhead: 8g\n",
      "  spark.sql.shuffle.partitions: 192\n",
      "  spark.default.parallelism: 192\n",
      "Spark UI available at: http://jr-doe-temp1-m.c.open-targets-eu-dev.internal:45511\n",
      "loaded files\n",
      "loaded newColoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded gwasComplete\n",
      "loaded resolvedColloc\n",
      "run temporary direction of effect\n",
      "built drugApproved dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load comparisons_df_iterative function\n",
      "created full_data and lists\n",
      "loaded rightTissue dataset\n",
      "built negativeTD dataset\n",
      "built bench2 dataset\n",
      "looping for variables_study\n",
      "entering the big loops\n",
      "\n",
      "--- Creating DataFrame for Aggregation: 'NoneCellYes' and Pivot: 'projectId' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified ArrayType columns for conversion: ['null', 'Alasoo_2018', 'Aygun_2021', 'BLUEPRINT', 'Bossini-Castillo_2019', 'BrainSeq', 'Braineac2', 'CAP', 'CEDAR', 'CommonMind', 'Cytoimmgen', 'FUSION', 'Fairfax_2012', 'Fairfax_2014', 'GENCORD', 'GEUVADIS', 'GTEx', 'Gilchrist_2021', 'HipSci', 'Jerber_2021', 'Kasela_2017', 'Kim-Hellmuth_2017', 'Lepik_2017', 'Naranbhai_2015', 'Nathan_2022', 'Nedelec_2016', 'OneK1K', 'PISA', 'Peng_2018', 'Perez_2022', 'PhLiPS', 'Quach_2016', 'ROSMAP', 'Randolph_2021', 'Schmiedel_2018', 'Schwartzentruber_2018', 'Steinberg_2020', 'Sun_2018', 'TwinsUK', 'UKB_PPP_EUR', 'Walker_2019', 'Young_2019', 'iPSCORE', 'van_de_Bunt_2015']\n",
      "\n",
      "--- Creating DataFrame for Aggregation: 'NdiagonalYes' and Pivot: 'projectId' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 05:44:18 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified ArrayType columns for conversion: ['null', 'Alasoo_2018', 'Aygun_2021', 'BLUEPRINT', 'Bossini-Castillo_2019', 'BrainSeq', 'Braineac2', 'CAP', 'CEDAR', 'CommonMind', 'Cytoimmgen', 'FUSION', 'Fairfax_2012', 'Fairfax_2014', 'GENCORD', 'GEUVADIS', 'GTEx', 'Gilchrist_2021', 'HipSci', 'Jerber_2021', 'Kasela_2017', 'Kim-Hellmuth_2017', 'Lepik_2017', 'Naranbhai_2015', 'Nathan_2022', 'Nedelec_2016', 'OneK1K', 'PISA', 'Peng_2018', 'Perez_2022', 'PhLiPS', 'Quach_2016', 'ROSMAP', 'Randolph_2021', 'Schmiedel_2018', 'Schwartzentruber_2018', 'Steinberg_2020', 'Sun_2018', 'TwinsUK', 'UKB_PPP_EUR', 'Walker_2019', 'Young_2019', 'iPSCORE', 'van_de_Bunt_2015']\n",
      "\n",
      "--- Creating DataFrame for Aggregation: 'hasGenetics' and Pivot: 'projectId' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 05:44:53 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified ArrayType columns for conversion: ['null', 'Alasoo_2018', 'Aygun_2021', 'BLUEPRINT', 'Bossini-Castillo_2019', 'BrainSeq', 'Braineac2', 'CAP', 'CEDAR', 'CommonMind', 'Cytoimmgen', 'FUSION', 'Fairfax_2012', 'Fairfax_2014', 'GENCORD', 'GEUVADIS', 'GTEx', 'Gilchrist_2021', 'HipSci', 'Jerber_2021', 'Kasela_2017', 'Kim-Hellmuth_2017', 'Lepik_2017', 'Naranbhai_2015', 'Nathan_2022', 'Nedelec_2016', 'OneK1K', 'PISA', 'Peng_2018', 'Perez_2022', 'PhLiPS', 'Quach_2016', 'ROSMAP', 'Randolph_2021', 'Schmiedel_2018', 'Schwartzentruber_2018', 'Steinberg_2020', 'Sun_2018', 'TwinsUK', 'UKB_PPP_EUR', 'Walker_2019', 'Young_2019', 'iPSCORE', 'van_de_Bunt_2015']\n",
      "\n",
      "--- All generated DataFrames are stored in 'all_pivoted_dfs' dictionary ---\n",
      "Keys available: dict_keys(['df_pivot_nonecellyes_by_projectid', 'df_pivot_ndiagonalyes_by_projectid', 'df_pivot_hasgenetics_by_projectid'])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from array import ArrayType\n",
    "from functions import (\n",
    "    relative_success,\n",
    "    spreadSheetFormatter,\n",
    "    discrepancifier,\n",
    "    temporary_directionOfEffect,\n",
    "    buildColocData,\n",
    "    gwasDataset,\n",
    ")\n",
    "# from stoppedTrials import terminated_td\n",
    "from DoEAssessment import directionOfEffect\n",
    "# from membraneTargets import target_membrane\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    DecimalType,\n",
    "    StringType,\n",
    "    FloatType,\n",
    ")\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# --- Build the SparkSession ---\n",
    "# Use the .config() method to set these parameters before calling .getOrCreate()\n",
    "# This ensures Spark requests the correct resources from YARN at the start.\n",
    "driver_memory = \"24g\"                 # plenty for planning & small collects\n",
    "executor_cores = 4                    # sweet spot for GC + Python workers\n",
    "num_executors  = 12                   # 12 * 4 = 48 cores for executors; ~16 cores left for driver/OS\n",
    "executor_memory = \"32g\"               # per executor heap\n",
    "executor_memory_overhead = \"8g\"       # ~20% overhead for PySpark/Arrow/off-heap\n",
    "# Totals: (32+8) * 12 = 480 GB executors + 24 GB driver ≈ 504 GB (adjust down if your hard cap is <500 GB)\n",
    "# If you must stay strictly ≤ 500 GB, use executor_memory=\"30g\", overhead=\"6g\"  → (36 * 12) + 24 = 456 + 24 = 480 GB\n",
    "\n",
    "shuffle_partitions   = 192            # ≈ 2–4× total cores (48) → start with 192\n",
    "default_parallelism  = 192\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyOptimizedPySparkApp\") \\\n",
    "    .config(\"spark.master\", \"yarn\") \\\n",
    "    .config(\"spark.driver.memory\", driver_memory) \\\n",
    "    .config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .config(\"spark.executor.cores\", executor_cores) \\\n",
    "    .config(\"spark.executor.instances\", num_executors) \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions) \\\n",
    "    .config(\"spark.default.parallelism\", default_parallelism) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"SparkSession created successfully with the following configurations:\")\n",
    "print(f\"  spark.driver.memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"  spark.executor.memory: {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"  spark.executor.cores: {spark.conf.get('spark.executor.cores')}\")\n",
    "print(f\"  spark.executor.instances: {spark.conf.get('spark.executor.instances')}\")\n",
    "print(f\"  spark.yarn.executor.memoryOverhead: {spark.conf.get('spark.yarn.executor.memoryOverhead')}\")\n",
    "print(f\"  spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"  spark.default.parallelism: {spark.conf.get('spark.default.parallelism')}\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# --- Your PySpark Code Here ---\n",
    "# Now you can proceed with your data loading and processing.\n",
    "# Example:\n",
    "# df = spark.read.parquet(\"hdfs:///user/your_user/your_large_data.parquet\")\n",
    "# print(f\"Number of rows in DataFrame: {df.count()}\")\n",
    "# df.groupBy(\"some_column\").agg({\"another_column\": \"sum\"}).show()\n",
    "\n",
    "# Remember to stop the SparkSession when you are done\n",
    "# spark.stop()\n",
    "\n",
    "path_n='gs://open-targets-data-releases/25.06/output/'\n",
    "\n",
    "target = spark.read.parquet(f\"{path_n}target/\")\n",
    "\n",
    "diseases = spark.read.parquet(f\"{path_n}disease/\")\n",
    "\n",
    "evidences = spark.read.parquet(f\"{path_n}evidence\")\n",
    "\n",
    "credible = spark.read.parquet(f\"{path_n}credible_set\")\n",
    "\n",
    "new = spark.read.parquet(f\"{path_n}colocalisation_coloc\") \n",
    "\n",
    "index=spark.read.parquet(f\"{path_n}study/\")\n",
    "\n",
    "variantIndex = spark.read.parquet(f\"{path_n}variant\")\n",
    "\n",
    "biosample = spark.read.parquet(f\"{path_n}biosample\")\n",
    "\n",
    "ecaviar=spark.read.parquet(f\"{path_n}colocalisation_ecaviar\")\n",
    "\n",
    "all_coloc=ecaviar.unionByName(new, allowMissingColumns=True)\n",
    "\n",
    "print(\"loaded files\")\n",
    "\n",
    "#### FIRST MODULE: BUILDING COLOC \n",
    "newColoc=buildColocData(all_coloc,credible,index)\n",
    "\n",
    "print(\"loaded newColoc\")\n",
    "\n",
    "### SECOND MODULE: PROCESS EVIDENCES TO AVOID EXCESS OF COLUMNS \n",
    "gwasComplete = gwasDataset(evidences,credible)\n",
    "\n",
    "#### THIRD MODULE: INCLUDE COLOC IN THE \n",
    "resolvedColoc = (\n",
    "    (\n",
    "        newColoc.withColumnRenamed(\"geneId\", \"targetId\")\n",
    "        .join(\n",
    "            gwasComplete.withColumnRenamed(\"studyLocusId\", \"leftStudyLocusId\"),\n",
    "            on=[\"leftStudyLocusId\", \"targetId\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .join(  ### propagated using parent terms\n",
    "            diseases.selectExpr(\n",
    "                \"id as diseaseId\", \"name\", \"parents\", \"therapeuticAreas\"\n",
    "            ),\n",
    "            on=\"diseaseId\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"diseaseId\",\n",
    "            F.explode_outer(F.concat(F.array(F.col(\"diseaseId\")), F.col(\"parents\"))),\n",
    "        )\n",
    "        .drop(\"parents\", \"oldDiseaseId\")\n",
    "    ).withColumn(\n",
    "        \"colocDoE\",\n",
    "        F.when(\n",
    "            F.col(\"rightStudyType\").isin(\n",
    "                [\"eqtl\", \"pqtl\", \"tuqtl\", \"sceqtl\", \"sctuqtl\"]\n",
    "            ),\n",
    "            F.when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"GoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"LoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"LoF_protect\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"GoF_protect\"),\n",
    "            ),\n",
    "        ).when(\n",
    "            F.col(\"rightStudyType\").isin(\n",
    "                [\"sqtl\", \"scsqtl\"]\n",
    "            ),  ### opposite directionality than sqtl\n",
    "            F.when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"LoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"GoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"GoF_protect\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"LoF_protect\"),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    # .persist()\n",
    ")\n",
    "print(\"loaded resolvedColloc\")\n",
    "\n",
    "datasource_filter = [\n",
    "#   \"ot_genetics_portal\",\n",
    "    \"gwas_credible_sets\",\n",
    "    \"gene_burden\",\n",
    "    \"eva\",\n",
    "    \"eva_somatic\",\n",
    "    \"gene2phenotype\",\n",
    "    \"orphanet\",\n",
    "    \"cancer_gene_census\",\n",
    "    \"intogen\",\n",
    "    \"impc\",\n",
    "    \"chembl\",\n",
    "]\n",
    "\n",
    "assessment, evidences, actionType, oncolabel = temporary_directionOfEffect(\n",
    "    path_n, datasource_filter\n",
    ")\n",
    "\n",
    "print(\"run temporary direction of effect\")\n",
    "\n",
    "\n",
    "print(\"built drugApproved dataset\")\n",
    "\n",
    "\n",
    "#### FOURTH MODULE BUILDING CHEMBL ASSOCIATIONS - HERE TAKE CARE WITH FILTERING STEP \n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter((F.col(\"datasourceId\") == \"chembl\"))\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(F.col(\"clinicalPhase\")).over(\n",
    "                Window.partitionBy(\"targetId\", \"diseaseId\")\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\")\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    #.filter(F.col(\"coherencyDiagonal\") == \"coherent\")\n",
    "    .drop(\n",
    "        \"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\"\n",
    "    )\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    "    # .persist()\n",
    ")\n",
    "\n",
    "####2 Define agregation function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def convertTuple(tup):\n",
    "    st = \",\".join(map(str, tup))\n",
    "    return st\n",
    "\n",
    "\n",
    "#####3 run in a function\n",
    "def aggregations_original(\n",
    "    df,\n",
    "    data,\n",
    "    listado,\n",
    "    comparisonColumn,\n",
    "    comparisonType,\n",
    "    predictionColumn,\n",
    "    predictionType,\n",
    "    today_date,\n",
    "):\n",
    "    wComparison = Window.partitionBy(comparisonColumn)\n",
    "    wPrediction = Window.partitionBy(predictionColumn)\n",
    "    wPredictionComparison = Window.partitionBy(comparisonColumn, predictionColumn)\n",
    "    results = []\n",
    "    # uniqIds = df.select(\"targetId\", \"diseaseId\").distinct().count()\n",
    "    out = (\n",
    "        df.withColumn(\"comparisonType\", F.lit(comparisonType))\n",
    "        .withColumn(\"dataset\", F.lit(data))\n",
    "        .withColumn(\"predictionType\", F.lit(predictionType))\n",
    "        # .withColumn(\"total\", F.lit(uniqIds))\n",
    "        .withColumn(\"a\", F.count(\"targetId\").over(wPredictionComparison))\n",
    "        .withColumn(\"comparisonColumn\", F.lit(comparisonColumn))\n",
    "        .withColumn(\"predictionColumnValue\", F.lit(predictionColumn))\n",
    "        .withColumn(\n",
    "            \"predictionTotal\",\n",
    "            F.count(\"targetId\").over(wPrediction),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"comparisonTotal\",\n",
    "            F.count(\"targetId\").over(wComparison),\n",
    "        )\n",
    "        .select(\n",
    "            F.col(predictionColumn).alias(\"prediction\"),\n",
    "            F.col(comparisonColumn).alias(\"comparison\"),\n",
    "            \"dataset\",\n",
    "            \"comparisonColumn\",\n",
    "            \"predictionColumnValue\",\n",
    "            \"comparisonType\",\n",
    "            \"predictionType\",\n",
    "            \"a\",\n",
    "            \"predictionTotal\",\n",
    "            \"comparisonTotal\",\n",
    "        )\n",
    "        .filter(F.col(\"prediction\").isNotNull())\n",
    "        .filter(F.col(\"comparison\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    '''\n",
    "    out.write.mode(\"overwrite\").parquet(\n",
    "        \"gs://ot-team/jroldan/\"\n",
    "        + str(\n",
    "            today_date\n",
    "            + \"_\"\n",
    "            + \"analysis/\"\n",
    "            + data\n",
    "            # + \"_propagated\"\n",
    "            + \"/\"\n",
    "            + comparisonColumn\n",
    "            + \"_\"\n",
    "            + comparisonType\n",
    "            + \"_\"\n",
    "            + predictionColumn\n",
    "            + \".parquet\"\n",
    "        )\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    listado.append(\n",
    "        \"gs://ot-team/jroldan/\"\n",
    "        + str(\n",
    "            today_date\n",
    "            + \"_\"\n",
    "            + \"analysis/\"\n",
    "            + data\n",
    "            # + \"_propagated\"\n",
    "            + \"/\"\n",
    "            + comparisonColumn\n",
    "            + \"_\"\n",
    "            + comparisonType\n",
    "            + \"_\"\n",
    "            + predictionColumn\n",
    "            + \".parquet\"\n",
    "        )\n",
    "    )\n",
    "    path = \"gs://ot-team/jroldan/\" + str(\n",
    "        today_date\n",
    "        + \"_\"\n",
    "        + \"analysis/\"\n",
    "        + data\n",
    "        # + \"_propagated\"\n",
    "        + \"/\"\n",
    "        + comparisonColumn\n",
    "        + \"_\"\n",
    "        + comparisonType\n",
    "        + \"_\"\n",
    "        + predictionColumn\n",
    "        + \".parquet\"\n",
    "    )\n",
    "    print(path)\n",
    "    \n",
    "    ### making analysis\n",
    "    array1 = np.delete(\n",
    "        out.join(full_data, on=[\"prediction\", \"comparison\"], how=\"outer\")\n",
    "        .groupBy(\"comparison\")\n",
    "        .pivot(\"prediction\")\n",
    "        .agg(F.first(\"a\"))\n",
    "        .sort(F.col(\"comparison\").desc())\n",
    "        .select(\"comparison\", \"yes\", \"no\")\n",
    "        .fillna(0)\n",
    "        .toPandas()\n",
    "        .to_numpy(),\n",
    "        [0],\n",
    "        1,\n",
    "    )\n",
    "    total = np.sum(array1)\n",
    "    res_npPhaseX = np.array(array1, dtype=int)\n",
    "    resX = convertTuple(fisher_exact(res_npPhaseX, alternative=\"two-sided\"))\n",
    "    resx_CI = convertTuple(\n",
    "        odds_ratio(res_npPhaseX).confidence_interval(confidence_level=0.95)\n",
    "    )\n",
    "\n",
    "    result_st.append(resX)\n",
    "    result_ci.append(resx_CI)\n",
    "    (rs_result, rs_ci) = relative_success(array1)\n",
    "    results.extend(\n",
    "        [\n",
    "            comparisonType,\n",
    "            comparisonColumn,\n",
    "            predictionColumn,\n",
    "            round(float(resX.split(\",\")[0]), 2),\n",
    "            float(resX.split(\",\")[1]),\n",
    "            round(float(resx_CI.split(\",\")[0]), 2),\n",
    "            round(float(resx_CI.split(\",\")[1]), 2),\n",
    "            str(total),\n",
    "            np.array(res_npPhaseX).tolist(),\n",
    "            round(float(rs_result), 2),\n",
    "            round(float(rs_ci[0]), 2),\n",
    "            round(float(rs_ci[1]), 2),\n",
    "            # studies,\n",
    "            # tissues,\n",
    "            path,\n",
    "        ]\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "#### 3 Loop over different datasets (as they will have different rows and columns)\n",
    "\n",
    "\n",
    "def comparisons_df_iterative(elements):\n",
    "    #toAnalysis = [(key, value) for key, value in disdic.items() if value == projectId]\n",
    "    toAnalysis = [(col, \"predictor\") for col in elements]\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"comparison\", StringType(), True),\n",
    "            StructField(\"comparisonType\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    comparisons = spark.createDataFrame(toAnalysis, schema=schema)\n",
    "    ### include all the columns as predictor\n",
    "\n",
    "    predictions = spark.createDataFrame(\n",
    "        data=[\n",
    "            (\"Phase>=4\", \"clinical\"),\n",
    "            ('Phase>=3','clinical'),\n",
    "            ('Phase>=2','clinical'),\n",
    "            ('Phase>=1','clinical'),\n",
    "            (\"PhaseT\", \"clinical\"),\n",
    "        ]\n",
    "    )\n",
    "    return comparisons.join(predictions, how=\"full\").collect()\n",
    "\n",
    "\n",
    "print(\"load comparisons_df_iterative function\")\n",
    "\n",
    "\n",
    "full_data = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\"yes\", \"yes\"),\n",
    "        (\"yes\", \"no\"),\n",
    "        (\"no\", \"yes\"),\n",
    "        (\"no\", \"no\"),\n",
    "    ],\n",
    "    schema=StructType(\n",
    "        [\n",
    "            StructField(\"prediction\", StringType(), True),\n",
    "            StructField(\"comparison\", StringType(), True),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\"created full_data and lists\")\n",
    "\n",
    "#rightTissue = spark.read.csv(\n",
    "#    'gs://ot-team/jroldan/analysis/20250526_rightTissue.csv',\n",
    "#    header=True,\n",
    "#).drop(\"_c0\")\n",
    "\n",
    "print(\"loaded rightTissue dataset\")\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\")\n",
    "    .count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\"))\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "print(\"built negativeTD dataset\")\n",
    "\n",
    "print(\"built bench2 dataset\")\n",
    "\n",
    "###### cut from here\n",
    "print(\"looping for variables_study\")\n",
    "\n",
    "#### new part with chatgpt -- TEST\n",
    "\n",
    "## QUESTIONS TO ANSWER:\n",
    "# HAVE ECAVIAR >=0.8\n",
    "# HAVE COLOC \n",
    "# HAVE COLOC >= 0.8\n",
    "# HAVE COLOC + ECAVIAR >= 0.01\n",
    "# HAVE COLOC >= 0.8 + ECAVIAR >= 0.01\n",
    "# RIGHT JOING WITH CHEMBL \n",
    "\n",
    "### FIFTH MODULE: BUILDING BENCHMARK OF THE DATASET TO EXTRACT EHE ANALYSIS \n",
    "\n",
    "resolvedColocFiltered = resolvedColoc.filter((F.col('clpp')>=0.01) | (F.col('h4')>=0.8))\n",
    "benchmark = (\n",
    "    (\n",
    "        resolvedColocFiltered.filter( ## .filter(F.col(\"betaGwas\") < 0)\n",
    "        F.col(\"name\") != \"COVID-19\"\n",
    "    )\n",
    "        .join(  ### select just GWAS giving protection\n",
    "            analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\"  ### RIGHT SIDE\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"AgreeDrug\",\n",
    "            F.when(\n",
    "                (F.col(\"drugGoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"GoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"drugLoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"LoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "    )  #### remove COVID-19 associations\n",
    ").join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    "\n",
    "\n",
    "### drug mechanism of action\n",
    "mecact_path = f\"{path_n}drug_mechanism_of_action/\" #  mechanismOfAction == old version\n",
    "mecact = spark.read.parquet(mecact_path)\n",
    "\n",
    "inhibitors = [\n",
    "    \"RNAI INHIBITOR\",\n",
    "    \"NEGATIVE MODULATOR\",\n",
    "    \"NEGATIVE ALLOSTERIC MODULATOR\",\n",
    "    \"ANTAGONIST\",\n",
    "    \"ANTISENSE INHIBITOR\",\n",
    "    \"BLOCKER\",\n",
    "    \"INHIBITOR\",\n",
    "    \"DEGRADER\",\n",
    "    \"INVERSE AGONIST\",\n",
    "    \"ALLOSTERIC ANTAGONIST\",\n",
    "    \"DISRUPTING AGENT\",\n",
    "]\n",
    "\n",
    "activators = [\n",
    "    \"PARTIAL AGONIST\",\n",
    "    \"ACTIVATOR\",\n",
    "    \"POSITIVE ALLOSTERIC MODULATOR\",\n",
    "    \"POSITIVE MODULATOR\",\n",
    "    \"AGONIST\",\n",
    "    \"SEQUESTERING AGENT\",  ## lost at 31.01.2025\n",
    "    \"STABILISER\",\n",
    "    # \"EXOGENOUS GENE\", ## added 24.06.2025\n",
    "    # \"EXOGENOUS PROTEIN\" ## added 24.06.2025\n",
    "]\n",
    "\n",
    "\n",
    "actionType = (\n",
    "        mecact.select(\n",
    "            F.explode_outer(\"chemblIds\").alias(\"drugId\"),\n",
    "            \"actionType\",\n",
    "            \"mechanismOfAction\",\n",
    "            \"targets\",\n",
    "        )\n",
    "        .select(\n",
    "            F.explode_outer(\"targets\").alias(\"targetId\"),\n",
    "            \"drugId\",\n",
    "            \"actionType\",\n",
    "            \"mechanismOfAction\",\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"drugId\")\n",
    "        .agg(F.collect_set(\"actionType\").alias(\"actionType2\"))\n",
    "    ).withColumn('nMoA', F.size(F.col('actionType2')))\n",
    "\n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter((F.col(\"datasourceId\") == \"chembl\")).join(actionType, on=['targetId','drugId'], how='left')\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(F.col(\"clinicalPhase\")).over(\n",
    "                Window.partitionBy(\"targetId\", \"diseaseId\")\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\",'actionType2')\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    #.filter(F.col(\"coherencyDiagonal\") == \"coherent\")\n",
    "    .drop(\n",
    "        \"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\"\n",
    "    )\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    ")\n",
    "\n",
    "benchmark = (\n",
    "    (\n",
    "        resolvedColocFiltered.filter( ## .filter(F.col(\"betaGwas\") < 0)\n",
    "        F.col(\"name\") != \"COVID-19\"\n",
    "    )\n",
    "        .join(  ### select just GWAS giving protection\n",
    "            analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\"  ### RIGHT SIDE\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"AgreeDrug\",\n",
    "            F.when(\n",
    "                (F.col(\"drugGoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"GoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"drugLoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"LoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "    )  #### remove COVID-19 associations\n",
    ").join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\")\n",
    "    .count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\"))\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "### create disdic dictionary\n",
    "disdic={}\n",
    "\n",
    "# --- Configuration for your iterative pivoting ---\n",
    "group_by_columns = ['targetId', 'diseaseId','phase4Clean','phase3Clean','phase2Clean','phase1Clean','PhaseT']\n",
    "#columns_to_pivot_on = ['actionType2', 'biosampleName', 'projectId', 'rightStudyType','colocalisationMethod']\n",
    "columns_to_pivot_on = ['projectId']\n",
    "columns_to_aggregate = ['NoneCellYes', 'NdiagonalYes','hasGenetics'] # The values you want to collect in the pivoted cells\n",
    "all_pivoted_dfs = {}\n",
    "\n",
    "doe_columns=[\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "diagonal_lof=['LoF_protect','GoF_risk']\n",
    "diagonal_gof=['LoF_risk','GoF_protect']\n",
    "\n",
    "conditions = [\n",
    "    F.when(F.col(c) == F.col(\"maxDoE\"), F.lit(c)).otherwise(F.lit(None)) for c in doe_columns\n",
    "    ]\n",
    "print('entering the big loops')\n",
    "# --- Nested Loops for Dynamic Pivoting ---\n",
    "for agg_col_name in columns_to_aggregate:\n",
    "    for pivot_col_name in columns_to_pivot_on:\n",
    "        print(f\"\\n--- Creating DataFrame for Aggregation: '{agg_col_name}' and Pivot: '{pivot_col_name}' ---\")\n",
    "        current_col_pvalue_order_window = Window.partitionBy(\"targetId\", \"diseaseId\", \"maxClinPhase\", pivot_col_name).orderBy(F.col('colocalisationMethod').asc(), F.col(\"qtlPValueExponent\").asc())\n",
    "        test2=discrepancifier(benchmark.withColumn('actionType2', F.concat_ws(\",\", F.col(\"actionType2\"))).withColumn('qtlColocDoE',F.first('colocDoE').over(current_col_pvalue_order_window)).groupBy(\n",
    "        \"targetId\", \"diseaseId\", \"maxClinPhase\", \"drugLoF_protect\", \"drugGoF_protect\",pivot_col_name)\n",
    "        .pivot(\"colocDoE\")\n",
    "        .count()\n",
    "        .withColumnRenamed('drugLoF_protect', 'LoF_protect_ch')\n",
    "        .withColumnRenamed('drugGoF_protect', 'GoF_protect_ch')).withColumn( ## .filter(F.col('coherencyDiagonal')!='noEvid')\n",
    "    \"arrayN\", F.array(*[F.col(c) for c in doe_columns])\n",
    "    ).withColumn(\n",
    "        \"maxDoE\", F.array_max(F.col(\"arrayN\"))\n",
    "    ).withColumn(\"maxDoE_names\", F.array(*conditions)\n",
    "    ).withColumn(\"maxDoE_names\", F.expr(\"filter(maxDoE_names, x -> x is not null)\")\n",
    "    ).withColumn(\n",
    "        \"NoneCellYes\",\n",
    "        F.when((F.col(\"LoF_protect_ch\").isNotNull() & (F.col('GoF_protect_ch').isNull())) & (F.array_contains(F.col(\"maxDoE_names\"), F.lit(\"LoF_protect\")))==True, F.lit('yes'))\n",
    "        .when((F.col(\"GoF_protect_ch\").isNotNull() & (F.col('LoF_protect_ch').isNull())) & (F.array_contains(F.col(\"maxDoE_names\"), F.lit(\"GoF_protect\")))==True, F.lit('yes')\n",
    "            ).otherwise(F.lit('no'))  # If the value is null, return null # Otherwise, check if name is in array\n",
    "    ).withColumn(\n",
    "        \"NdiagonalYes\",\n",
    "        F.when((F.col(\"LoF_protect_ch\").isNotNull() & (F.col('GoF_protect_ch').isNull())) & \n",
    "            (F.size(F.array_intersect(F.col(\"maxDoE_names\"), F.array([F.lit(x) for x in diagonal_lof]))) > 0),\n",
    "            F.lit(\"yes\")\n",
    "        ).when((F.col(\"GoF_protect_ch\").isNotNull() & (F.col('LoF_protect_ch').isNull())) & \n",
    "            (F.size(F.array_intersect(F.col(\"maxDoE_names\"), F.array([F.lit(x) for x in diagonal_gof]))) > 0),\n",
    "            F.lit(\"yes\")\n",
    "        ).otherwise(F.lit('no'))\n",
    "    ).withColumn(\n",
    "        \"drugCoherency\",\n",
    "        F.when(\n",
    "            (F.col(\"LoF_protect_ch\").isNotNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNull()), F.lit(\"coherent\")\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"LoF_protect_ch\").isNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNotNull()), F.lit(\"coherent\")\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"LoF_protect_ch\").isNotNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNotNull()), F.lit(\"dispar\")\n",
    "        )\n",
    "        .otherwise(F.lit(\"other\")),\n",
    "    ).join(negativeTD, on=[\"targetId\", \"diseaseId\"], how=\"left\").withColumn(\n",
    "        \"PhaseT\",\n",
    "        F.when(F.col(\"stopReason\") == \"Negative\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase4Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") == 4) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase3Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 3) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase2Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 2) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase1Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 1) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"hasGenetics\",\n",
    "        F.when(F.col(\"coherencyDiagonal\") != \"noEvid\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    )\n",
    "        # 1. Get distinct values for the pivot column (essential for pivot())\n",
    "        # This brings a small amount of data to the driver, but is necessary for the pivot schema.\n",
    "        #distinct_pivot_values = [row[0] for row in test2.select(pivot_col_name).distinct().collect()]\n",
    "        # print(f\"Distinct values for '{pivot_col_name}': {distinct_pivot_values}\")\n",
    "\n",
    "        # 2. Perform the groupBy, pivot, and aggregate operations\n",
    "        # The .pivot() function requires the list of distinct values for better performance\n",
    "        # and correct schema inference.\n",
    "        pivoted_df = (\n",
    "            test2.groupBy(*group_by_columns)\n",
    "            .pivot(pivot_col_name) # Provide distinct values distinct_pivot_values\n",
    "            .agg(F.collect_set(F.col(agg_col_name))) # Collect all values into a set\n",
    "            .fillna(0) # Fill cells that have no data with an empty list instead of null\n",
    "        )\n",
    "        # 3. Add items to dictionary to map the columns:\n",
    "        # filter out None and 'null':\n",
    "        datasetColumns=pivoted_df.columns\n",
    "        filtered = [x for x in datasetColumns if x is not None and x != 'null']\n",
    "        # using list comprehension\n",
    "        for item in filtered:\n",
    "            disdic[item] = pivot_col_name\n",
    "\n",
    "        # 3. Add the 'data' literal column dynamically\n",
    "        # This column indicates which aggregation column was used.\n",
    "        #pivoted_df = pivoted_df.withColumn('data', F.lit(f'Drug_{agg_col_name}'))\n",
    "\n",
    "        array_columns_to_convert = [\n",
    "            field.name for field in pivoted_df.schema.fields\n",
    "            if isinstance(field.dataType, ArrayType)\n",
    "        ]\n",
    "        print(f\"Identified ArrayType columns for conversion: {array_columns_to_convert}\")\n",
    "\n",
    "        # 4. Apply the conversion logic to each identified array column\n",
    "        df_after_conversion = pivoted_df # Start with the pivoted_df\n",
    "        for col_to_convert in array_columns_to_convert:\n",
    "            df_after_conversion = df_after_conversion.withColumn(\n",
    "                col_to_convert,\n",
    "                F.when(F.col(col_to_convert).isNull(), F.lit('no'))          # Handle NULLs (from pivot for no data)\n",
    "                .when(F.size(F.col(col_to_convert)) == 0, F.lit('no'))       # Empty array -> 'no'\n",
    "                .when(F.array_contains(F.col(col_to_convert), F.lit('yes')), F.lit('yes')) # Contains 'yes' -> 'yes'\n",
    "                .when(F.array_contains(F.col(col_to_convert), F.lit('no')), F.lit('no'))   # Contains 'no' -> 'no'\n",
    "                .otherwise(F.lit('no')) # Fallback for unexpected array content (e.g., ['other'], ['yes','no'])\n",
    "            )\n",
    "\n",
    "        # 4. Generate a unique name for this DataFrame and store it\n",
    "        df_key = f\"df_pivot_{agg_col_name.lower()}_by_{pivot_col_name.lower()}\"\n",
    "        all_pivoted_dfs[df_key] = df_after_conversion.withColumnRenamed( 'phase4Clean','Phase>=4'\n",
    "        ).withColumnRenamed('phase3Clean','Phase>=3'\n",
    "        ).withColumnRenamed('phase2Clean','Phase>=2'\n",
    "        ).withColumnRenamed('phase1Clean','Phase>=1')\n",
    "\n",
    "\n",
    "# --- Accessing your generated DataFrames ---\n",
    "print(\"\\n--- All generated DataFrames are stored in 'all_pivoted_dfs' dictionary ---\")\n",
    "print(\"Keys available:\", all_pivoted_dfs.keys())\n",
    "############## HYBRID ##############\n",
    "####################################\n",
    "def strip_only(lst):\n",
    "    return [x.removesuffix(\"_only\") for x in lst]  # Python 3.9+\n",
    "    # or: return [x[:-5] if x.endswith(\"_only\") else x for x in lst]\n",
    "\n",
    "\n",
    "##### PROJECTID\n",
    "project_keys=[f\"{k}_only\" for k,v in disdic.items() if v == 'projectId']\n",
    "main=['GTEx_only', 'UKB_PPP_EUR_only']\n",
    "#stimulated=['Alasoo_2018_only','Cytoimmgen_only','Fairfax_2014_only','Kim-Hellmuth_2017_only','Nathan_2022_only','Nedelec_2016_only','Quach_2016_only','Randolph_2021_only','Schmiedel_2018_only']\n",
    "#cellLine=['CAP_only','HipSci_only','iPSCORE_only','Jerber_2021_only','PhLiPS_only','Schwartzentruber_2018_only','TwinsUK_only']\n",
    "\n",
    "derivedCellLine=['TwinsUK_only','PhLiPS_only','CAP_only','GENCORD_only','Sun_2018_only','Nedelec_2016_only']\n",
    "canonicalCellLine=['Alasoo_2018_only','Jerber_2021_only','GEUVADIS_only','iPSCORE_only','Aygun_2021_only','Schwartzentruber_2018_only']\n",
    "stimulated=['Schmiedel_2018_only','Bossini-Castillo_2019_only','Alasoo_2018_only','Cytoimmgen_only','Gilchrist_2021_only','CAP_only','Quach_2016_only','Randolph_2021_only','Sun_2018_only','Nedelec_2016_only','Kim-Hellmuth_2017_only']\n",
    "\n",
    "# Apply\n",
    "main = strip_only(main)\n",
    "canonicalCellLine = strip_only(canonicalCellLine)\n",
    "derivedCellLine = strip_only(derivedCellLine)\n",
    "stimulated = strip_only(stimulated)\n",
    "\n",
    "others=[item for item in strip_only(project_keys) if item not in main]\n",
    "nonStimulated=[item for item in strip_only(project_keys) if item not in stimulated]\n",
    "nonCanonicalCellLine = [item for item in strip_only(project_keys) if item not in canonicalCellLine]\n",
    "nonDerivedCellLine = [item for item in strip_only(project_keys) if item not in derivedCellLine]\n",
    "\n",
    "#otherCellLine=[item for item in strip_only(project_keys) if item not in cellLine]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply\n",
    "main = strip_only(main)\n",
    "canonicalCellLine = strip_only(canonicalCellLine)\n",
    "derivedCellLine = strip_only(derivedCellLine)\n",
    "stimulated = strip_only(stimulated)\n",
    "\n",
    "others=[item for item in strip_only(project_keys) if item not in main]\n",
    "nonStimulated=[item for item in strip_only(project_keys) if item not in stimulated]\n",
    "nonCanonicalCellLine = [item for item in strip_only(project_keys) if item not in canonicalCellLine]\n",
    "nonDerivedCellLine = [item for item in strip_only(project_keys) if item not in derivedCellLine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with df_pivot_nonecellyes_by_projectid\n",
      "There are  8 columns to analyse with phases\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/othersProjectId_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/othersProjectId_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/othersProjectId_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/othersProjectId_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/othersProjectId_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/GTExUKB_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/GTExUKB_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/GTExUKB_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/GTExUKB_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/GTExUKB_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/stimulated_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/stimulated_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/stimulated_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/stimulated_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/stimulated_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonStimulated_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonStimulated_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonStimulated_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonStimulated_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonStimulated_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "df unpersisted\n",
      "working with df_pivot_ndiagonalyes_by_projectid\n",
      "There are  8 columns to analyse with phases\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/othersProjectId_only_predictor_Phase>=4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/othersProjectId_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/othersProjectId_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/othersProjectId_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/othersProjectId_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/GTExUKB_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/GTExUKB_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/GTExUKB_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/GTExUKB_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/GTExUKB_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/stimulated_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/stimulated_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/stimulated_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/stimulated_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/stimulated_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonStimulated_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonStimulated_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonStimulated_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonStimulated_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonStimulated_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "df unpersisted\n",
      "working with df_pivot_hasgenetics_by_projectid\n",
      "There are  8 columns to analyse with phases\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/othersProjectId_only_predictor_Phase>=4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/othersProjectId_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/othersProjectId_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/othersProjectId_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/othersProjectId_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/GTExUKB_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/GTExUKB_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/GTExUKB_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/GTExUKB_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/GTExUKB_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/stimulated_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/stimulated_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/stimulated_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/stimulated_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='stimulated_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/stimulated_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonStimulated_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonStimulated_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonStimulated_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonStimulated_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonStimulated', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonStimulated_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "df unpersisted\n",
      "importing functions\n",
      "imported functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe written \n",
      " Analysis finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _or_yes(df, cols):\n",
    "    \"\"\"Return a Column that is TRUE if any of the given columns == 'yes'.\n",
    "       Ignores columns not present in df. If none present, returns FALSE.\n",
    "    \"\"\"\n",
    "    present = [c for c in cols if c in df.columns]\n",
    "    if not present:\n",
    "        return F.lit(False)\n",
    "    # (col == 'yes') OR (col == 'yes') OR ...\n",
    "    exprs = [F.col(c) == \"yes\" for c in present]\n",
    "    return reduce(lambda a, b: a | b, exprs)\n",
    "def add_project_group_flags(df, main, canonicalCellLine, derivedCellLine, stimulated):\n",
    "    # project keys = every *_only column in this DF\n",
    "    #project_keys = [c for c in df.columns if c.endswith(\"_only\")]\n",
    "    project_keys=[f\"{k}_only\" for k,v in disdic.items() if v == 'projectId']\n",
    "    # Buckets (suffix preserved)\n",
    "    others=[item for item in strip_only(project_keys) if item not in main]\n",
    "    nonStimulated=[item for item in strip_only(project_keys) if item not in stimulated]\n",
    "    nonCanonicalCellLine = [item for item in strip_only(project_keys) if item not in canonicalCellLine]\n",
    "    nonDerivedCellLine = [item for item in strip_only(project_keys) if item not in derivedCellLine]\n",
    "\n",
    "    # Conditions (each independent)\n",
    "    cond_others          = _or_yes(df, others)  \n",
    "    cond_main            = _or_yes(df, main)  \n",
    "    cond_stimulated      = _or_yes(df, stimulated)    \n",
    "    cond_nonStimulated   = _or_yes(df, nonStimulated)  \n",
    "    cond_canonical       = _or_yes(df, canonicalCellLine)     \n",
    "    cond_nonCanonical    = _or_yes(df, nonCanonicalCellLine)  \n",
    "    cond_derived         = _or_yes(df, derivedCellLine)  \n",
    "    cond_nonDerived      = _or_yes(df, nonDerivedCellLine)         \n",
    "\n",
    "    return (\n",
    "        df.withColumn(\"othersProjectId_only\",   F.when(cond_others, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"GTExUKB_only\",           F.when(cond_main, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"stimulated_only\",        F.when(cond_stimulated, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"nonStimulated\",          F.when(cond_nonStimulated, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"canonicalCellLine\",      F.when(cond_canonical, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"nonCanonicalCellLine\",   F.when(cond_nonCanonical, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"derivedCellLine\",        F.when(cond_derived, \"yes\").otherwise(\"no\"))\n",
    "          .withColumn(\"nonDerivedCellLine\",     F.when(cond_nonDerived, \"yes\").otherwise(\"no\"))\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Apply to the dict entry you mentioned ---\n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = add_project_group_flags(\n",
    "    df=all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'],\n",
    "    main=main,\n",
    "    canonicalCellLine=canonicalCellLine,\n",
    "    derivedCellLine=derivedCellLine,\n",
    "    stimulated=stimulated\n",
    ")\n",
    "\n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = add_project_group_flags(\n",
    "    df=all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'],\n",
    "    main=main,\n",
    "    canonicalCellLine=canonicalCellLine,\n",
    "    derivedCellLine=derivedCellLine,\n",
    "    stimulated=stimulated\n",
    ")\n",
    "\n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = add_project_group_flags(\n",
    "    df=all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'],\n",
    "    main=main,\n",
    "    canonicalCellLine=canonicalCellLine,\n",
    "    derivedCellLine=derivedCellLine,\n",
    "    stimulated=stimulated\n",
    ")\n",
    "\n",
    "# If you wanted to apply to every DF in the dict (only if they all share *_only columns):\n",
    "# for k, df in all_pivoted_dfs.items():\n",
    "#     all_pivoted_dfs[k] = add_project_group_flags(df, main, stimulated, cellLine)\n",
    "\n",
    "###append to dictionary\n",
    "\n",
    "disdic.update({'othersProjectId': 'projectId','Stimulated': 'projectId','cellLine': 'projectId', 'othersBiosampleName_only': 'biosampleName', 'otherRightStudyType':'rightStudyType'})\n",
    "\n",
    "###################################\n",
    "###################################\n",
    "result = []\n",
    "result_st = []\n",
    "result_ci = []\n",
    "array2 = []\n",
    "listado = []\n",
    "result_all = []\n",
    "today_date = str(date.today())\n",
    "\n",
    "for key,df in all_pivoted_dfs.items():\n",
    "\n",
    "    print(f'working with {key}')\n",
    "    parts = key.split('_by_') ### take the part of key belonging to column name\n",
    "    column_name = parts[1] ### take the last part which is column name\n",
    "    all_pivoted_dfs[key].persist()\n",
    "    #unique_values = all_pivoted_dfs[key].drop('null').columns[7:]\n",
    "    unique_values = all_pivoted_dfs[key].drop('null').columns[-8:] ### just the interesting columns for us \n",
    "    filtered_unique_values = [x for x in unique_values if x is not None and x != 'null']\n",
    "    print('There are ', len(filtered_unique_values), 'columns to analyse with phases')\n",
    "    rows = comparisons_df_iterative(filtered_unique_values)\n",
    "\n",
    "    # If needed, now process the rest\n",
    "    for row in rows:\n",
    "        print('performing', row)\n",
    "        results = aggregations_original(\n",
    "            all_pivoted_dfs[key], key, listado, *row, today_date\n",
    "        )\n",
    "        result_all.append(results)\n",
    "        print('results appended')\n",
    "    all_pivoted_dfs[key].unpersist()\n",
    "    print('df unpersisted')\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"group\", StringType(), True),\n",
    "        StructField(\"comparison\", StringType(), True),\n",
    "        StructField(\"phase\", StringType(), True),\n",
    "        StructField(\"oddsRatio\", DoubleType(), True),\n",
    "        StructField(\"pValue\", DoubleType(), True),\n",
    "        StructField(\"lowerInterval\", DoubleType(), True),\n",
    "        StructField(\"upperInterval\", DoubleType(), True),\n",
    "        StructField(\"total\", StringType(), True),\n",
    "        StructField(\"values\", ArrayType(ArrayType(IntegerType())), True),\n",
    "        StructField(\"relSuccess\", DoubleType(), True),\n",
    "        StructField(\"rsLower\", DoubleType(), True),\n",
    "        StructField(\"rsUpper\", DoubleType(), True),\n",
    "        StructField(\"path\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "import re\n",
    "\n",
    "# Define the list of patterns to search for\n",
    "patterns = [\n",
    "    \"_only\",\n",
    "    #\"_tissue\",\n",
    "    #\"_isSignalFromRightTissue\",\n",
    "    \"_isRightTissueSignalAgreed\",\n",
    "]\n",
    "# Create a regex pattern to match any of the substrings\n",
    "regex_pattern = \"(\" + \"|\".join(map(re.escape, patterns)) + \")\"\n",
    "\n",
    "# Convert list of lists to DataFrame\n",
    "df = (\n",
    "    spreadSheetFormatter(spark.createDataFrame(result_all, schema=schema))\n",
    "    .withColumn(\n",
    "        \"prefix\",\n",
    "        F.regexp_replace(\n",
    "            F.col(\"comparison\"), regex_pattern + \".*\", \"\"\n",
    "        ),  # Extract part before the pattern\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"suffix\",\n",
    "        F.regexp_extract(\n",
    "            F.col(\"comparison\"), regex_pattern, 0\n",
    "        ),  # Extract the pattern itself\n",
    "    )\n",
    ")\n",
    "\n",
    "### annotate projectId, tissue, qtl type and doe type:\n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr=create_map([F.lit(x) for x in chain(*disdic.items())])\n",
    "\n",
    "df_annot=df.withColumn('annotation',mapping_expr.getItem(F.col('prefix')))\n",
    "\n",
    "df_annot.toPandas().to_csv(\n",
    "    f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_AllPhasesMixtures2.csv\"\n",
    ")\n",
    "\n",
    "print(\"dataframe written \\n Analysis finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+\n",
      "|UKB_PPP_EUR|othersProjectId_only|count|\n",
      "+-----------+--------------------+-----+\n",
      "|         no|                 yes| 4461|\n",
      "|         no|                  no|69699|\n",
      "|        yes|                  no|   22|\n",
      "|        yes|                 yes|    5|\n",
      "+-----------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].groupBy('UKB_PPP_EUR','othersProjectId_only').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------+--------+--------+--------+------+----+-----------+----------+---------+---------------------+--------+---------+---+-----+----------+----------+------+------------+------------+-------+--------+----+--------------+------+-----------+-----------+-----------------+----------+--------------+-----------+------------+------+----+---------+----------+------+----------+------+-------------+--------------+---------------------+--------------+--------+-------+-----------+-----------+----------+-------+----------------+--------------------+------------+---------------+-------------+-----------------+--------------------+---------------+------------------+\n",
      "|       targetId|    diseaseId|Phase>=4|Phase>=3|Phase>=2|Phase>=1|PhaseT|null|Alasoo_2018|Aygun_2021|BLUEPRINT|Bossini-Castillo_2019|BrainSeq|Braineac2|CAP|CEDAR|CommonMind|Cytoimmgen|FUSION|Fairfax_2012|Fairfax_2014|GENCORD|GEUVADIS|GTEx|Gilchrist_2021|HipSci|Jerber_2021|Kasela_2017|Kim-Hellmuth_2017|Lepik_2017|Naranbhai_2015|Nathan_2022|Nedelec_2016|OneK1K|PISA|Peng_2018|Perez_2022|PhLiPS|Quach_2016|ROSMAP|Randolph_2021|Schmiedel_2018|Schwartzentruber_2018|Steinberg_2020|Sun_2018|TwinsUK|UKB_PPP_EUR|Walker_2019|Young_2019|iPSCORE|van_de_Bunt_2015|othersProjectId_only|GTExUKB_only|stimulated_only|nonStimulated|canonicalCellLine|nonCanonicalCellLine|derivedCellLine|nonDerivedCellLine|\n",
      "+---------------+-------------+--------+--------+--------+--------+------+----+-----------+----------+---------+---------------------+--------+---------+---+-----+----------+----------+------+------------+------------+-------+--------+----+--------------+------+-----------+-----------+-----------------+----------+--------------+-----------+------------+------+----+---------+----------+------+----------+------+-------------+--------------+---------------------+--------------+--------+-------+-----------+-----------+----------+-------+----------------+--------------------+------------+---------------+-------------+-----------------+--------------------+---------------+------------------+\n",
      "|ENSG00000120659|  EFO_0000384|      no|      no|      no|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|     yes|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|        yes|         no|        no|     no|              no|                 yes|         yes|             no|          yes|              yes|                 yes|             no|               yes|\n",
      "|ENSG00000143799|MONDO_0004992|     yes|     yes|     yes|     yes|    no|  no|         no|        no|      yes|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|     yes|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|        yes|         no|        no|     no|              no|                 yes|         yes|             no|          yes|              yes|                 yes|             no|               yes|\n",
      "|ENSG00000143799|  EFO_0000756|      no|      no|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|     yes|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|        yes|         no|        no|     no|              no|                 yes|         yes|             no|          yes|              yes|                 yes|             no|               yes|\n",
      "|ENSG00000114013|MONDO_0005301|      no|      no|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|         yes|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|        yes|         no|        no|     no|              no|                 yes|         yes|             no|          yes|               no|                 yes|             no|               yes|\n",
      "|ENSG00000126218|  EFO_0004286|      no|      no|      no|      no|   yes|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|        yes|         no|        no|     no|              no|                 yes|         yes|             no|          yes|               no|                 yes|             no|               yes|\n",
      "+---------------+-------------+--------+--------+--------+--------+------+----+-----------+----------+---------+---------------------+--------+---------+---+-----+----------+----------+------+------------+------------+-------+--------+----+--------------+------+-----------+-----------+-----------------+----------+--------------+-----------+------------+------+----+---------+----------+------+----------+------+-------------+--------------+---------------------+--------------+--------+-------+-----------+-----------+----------+-------+----------------+--------------------+------------+---------------+-------------+-----------------+--------------------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].filter((F.col('UKB_PPP_EUR')=='yes') & (F.col('othersProjectId_only')=='yes')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out]\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "project_list = benchmark.select(\"projectId\").distinct().rdd.map(lambda r: r[0]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quach_2016',\n",
       " 'UKB_PPP_EUR',\n",
       " 'FUSION',\n",
       " 'BLUEPRINT',\n",
       " 'GTEx',\n",
       " 'GEUVADIS',\n",
       " 'BrainSeq',\n",
       " 'TwinsUK',\n",
       " 'Lepik_2017',\n",
       " 'HipSci',\n",
       " 'Schmiedel_2018',\n",
       " 'Fairfax_2014',\n",
       " 'Bossini-Castillo_2019',\n",
       " 'ROSMAP',\n",
       " 'Peng_2018',\n",
       " 'CommonMind',\n",
       " 'Cytoimmgen',\n",
       " 'Alasoo_2018',\n",
       " 'Kim-Hellmuth_2017',\n",
       " 'CAP',\n",
       " 'Nedelec_2016',\n",
       " 'Walker_2019',\n",
       " 'Jerber_2021',\n",
       " 'Kasela_2017',\n",
       " 'Fairfax_2012',\n",
       " 'CEDAR',\n",
       " 'OneK1K',\n",
       " 'Aygun_2021',\n",
       " 'GENCORD',\n",
       " 'PhLiPS',\n",
       " 'Schwartzentruber_2018',\n",
       " 'Young_2019',\n",
       " 'Sun_2018',\n",
       " 'van_de_Bunt_2015',\n",
       " 'PISA',\n",
       " 'Perez_2022',\n",
       " 'Steinberg_2020',\n",
       " 'Gilchrist_2021',\n",
       " 'iPSCORE',\n",
       " 'Braineac2',\n",
       " 'Nathan_2022',\n",
       " 'Naranbhai_2015',\n",
       " 'Randolph_2021',\n",
       " None]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PROJECTID\n",
    "\n",
    "project_keys=[f\"{k}_only\" for k,v in disdic.items() if v == 'projectId']\n",
    "main=['GTEx_only', 'UKB_PPP_EUR_only']\n",
    "#stimulated=['Alasoo_2018_only','Cytoimmgen_only','Fairfax_2014_only','Kim-Hellmuth_2017_only','Nathan_2022_only','Nedelec_2016_only','Quach_2016_only','Randolph_2021_only','Schmiedel_2018_only']\n",
    "#cellLine=['CAP_only','HipSci_only','iPSCORE_only','Jerber_2021_only','PhLiPS_only','Schwartzentruber_2018_only','TwinsUK_only']\n",
    "\n",
    "derivedCellLine=['TwinsUK_only','PhLiPS_only','CAP_only','GENCORD_only','Sun_2018_only','Nedelec_2016_only']\n",
    "canonicalCellLine=['Alasoo_2018_only','Jerber_2021_only','GEUVADIS_only','iPSCORE_only','Aygun_2021_only','Schwartzentruber_2018_only']\n",
    "stimulated=['Schmiedel_2018_only','Bossini-Castillo_2019_only','Alasoo_2018_only','Cytoimmgen_only','Gilchrist_2021_only','CAP_only','Quach_2016_only','Randolph_2021_only','Sun_2018_only','Nedelec_2016_only','Kim-Hellmuth_2017_only']\n",
    "\n",
    "# Apply\n",
    "main = strip_only(main)\n",
    "canonicalCellLine = strip_only(canonicalCellLine)\n",
    "derivedCellLine = strip_only(derivedCellLine)\n",
    "stimulated = strip_only(stimulated)\n",
    "\n",
    "others=[item for item in strip_only(project_keys[7:]) if item not in main]\n",
    "nonStimulated=[item for item in strip_only(project_keys[7:]) if item not in stimulated]\n",
    "nonCanonicalCellLine = [item for item in strip_only(project_keys[7:]) if item not in canonicalCellLine]\n",
    "nonDerivedCellLine = [item for item in strip_only(project_keys[7:]) if item not in derivedCellLine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kasela_2017',\n",
       " 'Schmiedel_2018',\n",
       " 'Fairfax_2012',\n",
       " 'Cytoimmgen',\n",
       " 'Bossini-Castillo_2019',\n",
       " 'CEDAR',\n",
       " 'OneK1K',\n",
       " 'BLUEPRINT',\n",
       " 'ROSMAP',\n",
       " 'CommonMind',\n",
       " 'BrainSeq',\n",
       " 'HipSci',\n",
       " 'Quach_2016',\n",
       " 'Nathan_2022',\n",
       " 'Steinberg_2020',\n",
       " 'CAP',\n",
       " 'TwinsUK',\n",
       " 'iPSCORE',\n",
       " 'GENCORD',\n",
       " 'Peng_2018',\n",
       " 'Nedelec_2016',\n",
       " 'Alasoo_2018',\n",
       " 'Schwartzentruber_2018',\n",
       " 'Aygun_2021',\n",
       " 'Walker_2019',\n",
       " 'GEUVADIS',\n",
       " 'FUSION',\n",
       " 'Lepik_2017',\n",
       " 'van_de_Bunt_2015',\n",
       " 'Perez_2022',\n",
       " 'Fairfax_2014',\n",
       " 'PISA',\n",
       " 'PhLiPS',\n",
       " 'Sun_2018',\n",
       " 'Jerber_2021',\n",
       " 'Kim-Hellmuth_2017',\n",
       " 'Gilchrist_2021',\n",
       " 'Braineac2',\n",
       " 'Young_2019',\n",
       " 'Naranbhai_2015',\n",
       " 'Randolph_2021',\n",
       " None]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "others[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark session created at 2025-09-18 06:58:53.503052\n",
      "Analysis started on 2025-09-18 at  2025-09-18 06:58:53.503052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 06:58:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/09/18 06:58:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully with the following configurations:\n",
      "  spark.driver.memory: 24g\n",
      "  spark.executor.memory: 32g\n",
      "  spark.executor.cores: 4\n",
      "  spark.executor.instances: 12\n",
      "  spark.yarn.executor.memoryOverhead: 8g\n",
      "  spark.sql.shuffle.partitions: 192\n",
      "  spark.default.parallelism: 192\n",
      "Spark UI available at: http://jr-doe-temp1-m.c.open-targets-eu-dev.internal:38947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded files\n",
      "loaded newColoc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded gwasComplete\n",
      "loaded resolvedColloc\n",
      "run temporary direction of effect\n",
      "built drugApproved dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load comparisons_df_iterative function\n",
      "created full_data and lists\n",
      "loaded rightTissue dataset\n",
      "built negativeTD dataset\n",
      "built bench2 dataset\n",
      "looping for variables_study\n",
      "entering the big loops\n",
      "\n",
      "--- Creating DataFrame for Aggregation: 'NoneCellYes' and Pivot: 'projectId' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified ArrayType columns for conversion: ['null', 'Alasoo_2018', 'Aygun_2021', 'BLUEPRINT', 'Bossini-Castillo_2019', 'BrainSeq', 'Braineac2', 'CAP', 'CEDAR', 'CommonMind', 'Cytoimmgen', 'FUSION', 'Fairfax_2012', 'Fairfax_2014', 'GENCORD', 'GEUVADIS', 'GTEx', 'Gilchrist_2021', 'HipSci', 'Jerber_2021', 'Kasela_2017', 'Kim-Hellmuth_2017', 'Lepik_2017', 'Naranbhai_2015', 'Nathan_2022', 'Nedelec_2016', 'OneK1K', 'PISA', 'Peng_2018', 'Perez_2022', 'PhLiPS', 'Quach_2016', 'ROSMAP', 'Randolph_2021', 'Schmiedel_2018', 'Schwartzentruber_2018', 'Steinberg_2020', 'Sun_2018', 'TwinsUK', 'UKB_PPP_EUR', 'Walker_2019', 'Young_2019', 'iPSCORE', 'van_de_Bunt_2015']\n",
      "\n",
      "--- Creating DataFrame for Aggregation: 'NdiagonalYes' and Pivot: 'projectId' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 07:01:57 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified ArrayType columns for conversion: ['null', 'Alasoo_2018', 'Aygun_2021', 'BLUEPRINT', 'Bossini-Castillo_2019', 'BrainSeq', 'Braineac2', 'CAP', 'CEDAR', 'CommonMind', 'Cytoimmgen', 'FUSION', 'Fairfax_2012', 'Fairfax_2014', 'GENCORD', 'GEUVADIS', 'GTEx', 'Gilchrist_2021', 'HipSci', 'Jerber_2021', 'Kasela_2017', 'Kim-Hellmuth_2017', 'Lepik_2017', 'Naranbhai_2015', 'Nathan_2022', 'Nedelec_2016', 'OneK1K', 'PISA', 'Peng_2018', 'Perez_2022', 'PhLiPS', 'Quach_2016', 'ROSMAP', 'Randolph_2021', 'Schmiedel_2018', 'Schwartzentruber_2018', 'Steinberg_2020', 'Sun_2018', 'TwinsUK', 'UKB_PPP_EUR', 'Walker_2019', 'Young_2019', 'iPSCORE', 'van_de_Bunt_2015']\n",
      "\n",
      "--- Creating DataFrame for Aggregation: 'hasGenetics' and Pivot: 'projectId' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 07:02:31 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified ArrayType columns for conversion: ['null', 'Alasoo_2018', 'Aygun_2021', 'BLUEPRINT', 'Bossini-Castillo_2019', 'BrainSeq', 'Braineac2', 'CAP', 'CEDAR', 'CommonMind', 'Cytoimmgen', 'FUSION', 'Fairfax_2012', 'Fairfax_2014', 'GENCORD', 'GEUVADIS', 'GTEx', 'Gilchrist_2021', 'HipSci', 'Jerber_2021', 'Kasela_2017', 'Kim-Hellmuth_2017', 'Lepik_2017', 'Naranbhai_2015', 'Nathan_2022', 'Nedelec_2016', 'OneK1K', 'PISA', 'Peng_2018', 'Perez_2022', 'PhLiPS', 'Quach_2016', 'ROSMAP', 'Randolph_2021', 'Schmiedel_2018', 'Schwartzentruber_2018', 'Steinberg_2020', 'Sun_2018', 'TwinsUK', 'UKB_PPP_EUR', 'Walker_2019', 'Young_2019', 'iPSCORE', 'van_de_Bunt_2015']\n",
      "\n",
      "--- All generated DataFrames are stored in 'all_pivoted_dfs' dictionary ---\n",
      "Keys available: dict_keys(['df_pivot_nonecellyes_by_projectid', 'df_pivot_ndiagonalyes_by_projectid', 'df_pivot_hasgenetics_by_projectid'])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from array import ArrayType\n",
    "from functions import (\n",
    "    relative_success,\n",
    "    spreadSheetFormatter,\n",
    "    discrepancifier,\n",
    "    temporary_directionOfEffect,\n",
    "    buildColocData,\n",
    "    gwasDataset,\n",
    ")\n",
    "# from stoppedTrials import terminated_td\n",
    "from DoEAssessment import directionOfEffect\n",
    "# from membraneTargets import target_membrane\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    DecimalType,\n",
    "    StringType,\n",
    "    FloatType,\n",
    ")\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# --- Build the SparkSession ---\n",
    "# Use the .config() method to set these parameters before calling .getOrCreate()\n",
    "# This ensures Spark requests the correct resources from YARN at the start.\n",
    "driver_memory = \"24g\"                 # plenty for planning & small collects\n",
    "executor_cores = 4                    # sweet spot for GC + Python workers\n",
    "num_executors  = 12                   # 12 * 4 = 48 cores for executors; ~16 cores left for driver/OS\n",
    "executor_memory = \"32g\"               # per executor heap\n",
    "executor_memory_overhead = \"8g\"       # ~20% overhead for PySpark/Arrow/off-heap\n",
    "# Totals: (32+8) * 12 = 480 GB executors + 24 GB driver ≈ 504 GB (adjust down if your hard cap is <500 GB)\n",
    "# If you must stay strictly ≤ 500 GB, use executor_memory=\"30g\", overhead=\"6g\"  → (36 * 12) + 24 = 456 + 24 = 480 GB\n",
    "\n",
    "shuffle_partitions   = 192            # ≈ 2–4× total cores (48) → start with 192\n",
    "default_parallelism  = 192\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyOptimizedPySparkApp\") \\\n",
    "    .config(\"spark.master\", \"yarn\") \\\n",
    "    .config(\"spark.driver.memory\", driver_memory) \\\n",
    "    .config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .config(\"spark.executor.cores\", executor_cores) \\\n",
    "    .config(\"spark.executor.instances\", num_executors) \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions) \\\n",
    "    .config(\"spark.default.parallelism\", default_parallelism) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"SparkSession created successfully with the following configurations:\")\n",
    "print(f\"  spark.driver.memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"  spark.executor.memory: {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"  spark.executor.cores: {spark.conf.get('spark.executor.cores')}\")\n",
    "print(f\"  spark.executor.instances: {spark.conf.get('spark.executor.instances')}\")\n",
    "print(f\"  spark.yarn.executor.memoryOverhead: {spark.conf.get('spark.yarn.executor.memoryOverhead')}\")\n",
    "print(f\"  spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"  spark.default.parallelism: {spark.conf.get('spark.default.parallelism')}\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# --- Your PySpark Code Here ---\n",
    "# Now you can proceed with your data loading and processing.\n",
    "# Example:\n",
    "# df = spark.read.parquet(\"hdfs:///user/your_user/your_large_data.parquet\")\n",
    "# print(f\"Number of rows in DataFrame: {df.count()}\")\n",
    "# df.groupBy(\"some_column\").agg({\"another_column\": \"sum\"}).show()\n",
    "\n",
    "# Remember to stop the SparkSession when you are done\n",
    "# spark.stop()\n",
    "\n",
    "path_n='gs://open-targets-data-releases/25.06/output/'\n",
    "\n",
    "target = spark.read.parquet(f\"{path_n}target/\")\n",
    "\n",
    "diseases = spark.read.parquet(f\"{path_n}disease/\")\n",
    "\n",
    "evidences = spark.read.parquet(f\"{path_n}evidence\")\n",
    "\n",
    "credible = spark.read.parquet(f\"{path_n}credible_set\")\n",
    "\n",
    "new = spark.read.parquet(f\"{path_n}colocalisation_coloc\") \n",
    "\n",
    "index=spark.read.parquet(f\"{path_n}study/\")\n",
    "\n",
    "variantIndex = spark.read.parquet(f\"{path_n}variant\")\n",
    "\n",
    "biosample = spark.read.parquet(f\"{path_n}biosample\")\n",
    "\n",
    "ecaviar=spark.read.parquet(f\"{path_n}colocalisation_ecaviar\")\n",
    "\n",
    "all_coloc=ecaviar.unionByName(new, allowMissingColumns=True)\n",
    "\n",
    "print(\"loaded files\")\n",
    "\n",
    "#### FIRST MODULE: BUILDING COLOC \n",
    "newColoc=buildColocData(all_coloc,credible,index)\n",
    "\n",
    "print(\"loaded newColoc\")\n",
    "\n",
    "### SECOND MODULE: PROCESS EVIDENCES TO AVOID EXCESS OF COLUMNS \n",
    "gwasComplete = gwasDataset(evidences,credible)\n",
    "\n",
    "#### THIRD MODULE: INCLUDE COLOC IN THE \n",
    "resolvedColoc = (\n",
    "    (\n",
    "        newColoc.withColumnRenamed(\"geneId\", \"targetId\")\n",
    "        .join(\n",
    "            gwasComplete.withColumnRenamed(\"studyLocusId\", \"leftStudyLocusId\"),\n",
    "            on=[\"leftStudyLocusId\", \"targetId\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .join(  ### propagated using parent terms\n",
    "            diseases.selectExpr(\n",
    "                \"id as diseaseId\", \"name\", \"parents\", \"therapeuticAreas\"\n",
    "            ),\n",
    "            on=\"diseaseId\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"diseaseId\",\n",
    "            F.explode_outer(F.concat(F.array(F.col(\"diseaseId\")), F.col(\"parents\"))),\n",
    "        )\n",
    "        .drop(\"parents\", \"oldDiseaseId\")\n",
    "    ).withColumn(\n",
    "        \"colocDoE\",\n",
    "        F.when(\n",
    "            F.col(\"rightStudyType\").isin(\n",
    "                [\"eqtl\", \"pqtl\", \"tuqtl\", \"sceqtl\", \"sctuqtl\"]\n",
    "            ),\n",
    "            F.when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"GoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"LoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"LoF_protect\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"GoF_protect\"),\n",
    "            ),\n",
    "        ).when(\n",
    "            F.col(\"rightStudyType\").isin(\n",
    "                [\"sqtl\", \"scsqtl\"]\n",
    "            ),  ### opposite directionality than sqtl\n",
    "            F.when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"LoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"GoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"GoF_protect\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"LoF_protect\"),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    # .persist()\n",
    ")\n",
    "print(\"loaded resolvedColloc\")\n",
    "\n",
    "datasource_filter = [\n",
    "#   \"ot_genetics_portal\",\n",
    "    \"gwas_credible_sets\",\n",
    "    \"gene_burden\",\n",
    "    \"eva\",\n",
    "    \"eva_somatic\",\n",
    "    \"gene2phenotype\",\n",
    "    \"orphanet\",\n",
    "    \"cancer_gene_census\",\n",
    "    \"intogen\",\n",
    "    \"impc\",\n",
    "    \"chembl\",\n",
    "]\n",
    "\n",
    "assessment, evidences, actionType, oncolabel = temporary_directionOfEffect(\n",
    "    path_n, datasource_filter\n",
    ")\n",
    "\n",
    "print(\"run temporary direction of effect\")\n",
    "\n",
    "\n",
    "print(\"built drugApproved dataset\")\n",
    "\n",
    "\n",
    "#### FOURTH MODULE BUILDING CHEMBL ASSOCIATIONS - HERE TAKE CARE WITH FILTERING STEP \n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter((F.col(\"datasourceId\") == \"chembl\"))\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(F.col(\"clinicalPhase\")).over(\n",
    "                Window.partitionBy(\"targetId\", \"diseaseId\")\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\")\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    #.filter(F.col(\"coherencyDiagonal\") == \"coherent\")\n",
    "    .drop(\n",
    "        \"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\"\n",
    "    )\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    "    # .persist()\n",
    ")\n",
    "\n",
    "####2 Define agregation function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def convertTuple(tup):\n",
    "    st = \",\".join(map(str, tup))\n",
    "    return st\n",
    "\n",
    "\n",
    "#####3 run in a function\n",
    "def aggregations_original(\n",
    "    df,\n",
    "    data,\n",
    "    listado,\n",
    "    comparisonColumn,\n",
    "    comparisonType,\n",
    "    predictionColumn,\n",
    "    predictionType,\n",
    "    today_date,\n",
    "):\n",
    "    wComparison = Window.partitionBy(comparisonColumn)\n",
    "    wPrediction = Window.partitionBy(predictionColumn)\n",
    "    wPredictionComparison = Window.partitionBy(comparisonColumn, predictionColumn)\n",
    "    results = []\n",
    "    # uniqIds = df.select(\"targetId\", \"diseaseId\").distinct().count()\n",
    "    out = (\n",
    "        df.withColumn(\"comparisonType\", F.lit(comparisonType))\n",
    "        .withColumn(\"dataset\", F.lit(data))\n",
    "        .withColumn(\"predictionType\", F.lit(predictionType))\n",
    "        # .withColumn(\"total\", F.lit(uniqIds))\n",
    "        .withColumn(\"a\", F.count(\"targetId\").over(wPredictionComparison))\n",
    "        .withColumn(\"comparisonColumn\", F.lit(comparisonColumn))\n",
    "        .withColumn(\"predictionColumnValue\", F.lit(predictionColumn))\n",
    "        .withColumn(\n",
    "            \"predictionTotal\",\n",
    "            F.count(\"targetId\").over(wPrediction),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"comparisonTotal\",\n",
    "            F.count(\"targetId\").over(wComparison),\n",
    "        )\n",
    "        .select(\n",
    "            F.col(predictionColumn).alias(\"prediction\"),\n",
    "            F.col(comparisonColumn).alias(\"comparison\"),\n",
    "            \"dataset\",\n",
    "            \"comparisonColumn\",\n",
    "            \"predictionColumnValue\",\n",
    "            \"comparisonType\",\n",
    "            \"predictionType\",\n",
    "            \"a\",\n",
    "            \"predictionTotal\",\n",
    "            \"comparisonTotal\",\n",
    "        )\n",
    "        .filter(F.col(\"prediction\").isNotNull())\n",
    "        .filter(F.col(\"comparison\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    '''\n",
    "    out.write.mode(\"overwrite\").parquet(\n",
    "        \"gs://ot-team/jroldan/\"\n",
    "        + str(\n",
    "            today_date\n",
    "            + \"_\"\n",
    "            + \"analysis/\"\n",
    "            + data\n",
    "            # + \"_propagated\"\n",
    "            + \"/\"\n",
    "            + comparisonColumn\n",
    "            + \"_\"\n",
    "            + comparisonType\n",
    "            + \"_\"\n",
    "            + predictionColumn\n",
    "            + \".parquet\"\n",
    "        )\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    listado.append(\n",
    "        \"gs://ot-team/jroldan/\"\n",
    "        + str(\n",
    "            today_date\n",
    "            + \"_\"\n",
    "            + \"analysis/\"\n",
    "            + data\n",
    "            # + \"_propagated\"\n",
    "            + \"/\"\n",
    "            + comparisonColumn\n",
    "            + \"_\"\n",
    "            + comparisonType\n",
    "            + \"_\"\n",
    "            + predictionColumn\n",
    "            + \".parquet\"\n",
    "        )\n",
    "    )\n",
    "    path = \"gs://ot-team/jroldan/\" + str(\n",
    "        today_date\n",
    "        + \"_\"\n",
    "        + \"analysis/\"\n",
    "        + data\n",
    "        # + \"_propagated\"\n",
    "        + \"/\"\n",
    "        + comparisonColumn\n",
    "        + \"_\"\n",
    "        + comparisonType\n",
    "        + \"_\"\n",
    "        + predictionColumn\n",
    "        + \".parquet\"\n",
    "    )\n",
    "    print(path)\n",
    "    \n",
    "    ### making analysis\n",
    "    array1 = np.delete(\n",
    "        out.join(full_data, on=[\"prediction\", \"comparison\"], how=\"outer\")\n",
    "        .groupBy(\"comparison\")\n",
    "        .pivot(\"prediction\")\n",
    "        .agg(F.first(\"a\"))\n",
    "        .sort(F.col(\"comparison\").desc())\n",
    "        .select(\"comparison\", \"yes\", \"no\")\n",
    "        .fillna(0)\n",
    "        .toPandas()\n",
    "        .to_numpy(),\n",
    "        [0],\n",
    "        1,\n",
    "    )\n",
    "    total = np.sum(array1)\n",
    "    res_npPhaseX = np.array(array1, dtype=int)\n",
    "    resX = convertTuple(fisher_exact(res_npPhaseX, alternative=\"two-sided\"))\n",
    "    resx_CI = convertTuple(\n",
    "        odds_ratio(res_npPhaseX).confidence_interval(confidence_level=0.95)\n",
    "    )\n",
    "\n",
    "    result_st.append(resX)\n",
    "    result_ci.append(resx_CI)\n",
    "    (rs_result, rs_ci) = relative_success(array1)\n",
    "    results.extend(\n",
    "        [\n",
    "            comparisonType,\n",
    "            comparisonColumn,\n",
    "            predictionColumn,\n",
    "            round(float(resX.split(\",\")[0]), 2),\n",
    "            float(resX.split(\",\")[1]),\n",
    "            round(float(resx_CI.split(\",\")[0]), 2),\n",
    "            round(float(resx_CI.split(\",\")[1]), 2),\n",
    "            str(total),\n",
    "            np.array(res_npPhaseX).tolist(),\n",
    "            round(float(rs_result), 2),\n",
    "            round(float(rs_ci[0]), 2),\n",
    "            round(float(rs_ci[1]), 2),\n",
    "            # studies,\n",
    "            # tissues,\n",
    "            path,\n",
    "        ]\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "#### 3 Loop over different datasets (as they will have different rows and columns)\n",
    "\n",
    "\n",
    "def comparisons_df_iterative(elements):\n",
    "    #toAnalysis = [(key, value) for key, value in disdic.items() if value == projectId]\n",
    "    toAnalysis = [(col, \"predictor\") for col in elements]\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"comparison\", StringType(), True),\n",
    "            StructField(\"comparisonType\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    comparisons = spark.createDataFrame(toAnalysis, schema=schema)\n",
    "    ### include all the columns as predictor\n",
    "\n",
    "    predictions = spark.createDataFrame(\n",
    "        data=[\n",
    "            (\"Phase>=4\", \"clinical\"),\n",
    "            ('Phase>=3','clinical'),\n",
    "            ('Phase>=2','clinical'),\n",
    "            ('Phase>=1','clinical'),\n",
    "            (\"PhaseT\", \"clinical\"),\n",
    "        ]\n",
    "    )\n",
    "    return comparisons.join(predictions, how=\"full\").collect()\n",
    "\n",
    "\n",
    "print(\"load comparisons_df_iterative function\")\n",
    "\n",
    "\n",
    "full_data = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\"yes\", \"yes\"),\n",
    "        (\"yes\", \"no\"),\n",
    "        (\"no\", \"yes\"),\n",
    "        (\"no\", \"no\"),\n",
    "    ],\n",
    "    schema=StructType(\n",
    "        [\n",
    "            StructField(\"prediction\", StringType(), True),\n",
    "            StructField(\"comparison\", StringType(), True),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\"created full_data and lists\")\n",
    "\n",
    "#rightTissue = spark.read.csv(\n",
    "#    'gs://ot-team/jroldan/analysis/20250526_rightTissue.csv',\n",
    "#    header=True,\n",
    "#).drop(\"_c0\")\n",
    "\n",
    "print(\"loaded rightTissue dataset\")\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\")\n",
    "    .count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\"))\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "print(\"built negativeTD dataset\")\n",
    "\n",
    "print(\"built bench2 dataset\")\n",
    "\n",
    "###### cut from here\n",
    "print(\"looping for variables_study\")\n",
    "\n",
    "#### new part with chatgpt -- TEST\n",
    "\n",
    "## QUESTIONS TO ANSWER:\n",
    "# HAVE ECAVIAR >=0.8\n",
    "# HAVE COLOC \n",
    "# HAVE COLOC >= 0.8\n",
    "# HAVE COLOC + ECAVIAR >= 0.01\n",
    "# HAVE COLOC >= 0.8 + ECAVIAR >= 0.01\n",
    "# RIGHT JOING WITH CHEMBL \n",
    "\n",
    "### FIFTH MODULE: BUILDING BENCHMARK OF THE DATASET TO EXTRACT EHE ANALYSIS \n",
    "\n",
    "resolvedColocFiltered = resolvedColoc.filter((F.col('clpp')>=0.01) | (F.col('h4')>=0.8))\n",
    "benchmark = (\n",
    "    (\n",
    "        resolvedColocFiltered.filter( ## .filter(F.col(\"betaGwas\") < 0)\n",
    "        F.col(\"name\") != \"COVID-19\"\n",
    "    )\n",
    "        .join(  ### select just GWAS giving protection\n",
    "            analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\"  ### RIGHT SIDE\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"AgreeDrug\",\n",
    "            F.when(\n",
    "                (F.col(\"drugGoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"GoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"drugLoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"LoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "    )  #### remove COVID-19 associations\n",
    ").join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    "\n",
    "\n",
    "### drug mechanism of action\n",
    "mecact_path = f\"{path_n}drug_mechanism_of_action/\" #  mechanismOfAction == old version\n",
    "mecact = spark.read.parquet(mecact_path)\n",
    "\n",
    "inhibitors = [\n",
    "    \"RNAI INHIBITOR\",\n",
    "    \"NEGATIVE MODULATOR\",\n",
    "    \"NEGATIVE ALLOSTERIC MODULATOR\",\n",
    "    \"ANTAGONIST\",\n",
    "    \"ANTISENSE INHIBITOR\",\n",
    "    \"BLOCKER\",\n",
    "    \"INHIBITOR\",\n",
    "    \"DEGRADER\",\n",
    "    \"INVERSE AGONIST\",\n",
    "    \"ALLOSTERIC ANTAGONIST\",\n",
    "    \"DISRUPTING AGENT\",\n",
    "]\n",
    "\n",
    "activators = [\n",
    "    \"PARTIAL AGONIST\",\n",
    "    \"ACTIVATOR\",\n",
    "    \"POSITIVE ALLOSTERIC MODULATOR\",\n",
    "    \"POSITIVE MODULATOR\",\n",
    "    \"AGONIST\",\n",
    "    \"SEQUESTERING AGENT\",  ## lost at 31.01.2025\n",
    "    \"STABILISER\",\n",
    "    # \"EXOGENOUS GENE\", ## added 24.06.2025\n",
    "    # \"EXOGENOUS PROTEIN\" ## added 24.06.2025\n",
    "]\n",
    "\n",
    "\n",
    "actionType = (\n",
    "        mecact.select(\n",
    "            F.explode_outer(\"chemblIds\").alias(\"drugId\"),\n",
    "            \"actionType\",\n",
    "            \"mechanismOfAction\",\n",
    "            \"targets\",\n",
    "        )\n",
    "        .select(\n",
    "            F.explode_outer(\"targets\").alias(\"targetId\"),\n",
    "            \"drugId\",\n",
    "            \"actionType\",\n",
    "            \"mechanismOfAction\",\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"drugId\")\n",
    "        .agg(F.collect_set(\"actionType\").alias(\"actionType2\"))\n",
    "    ).withColumn('nMoA', F.size(F.col('actionType2')))\n",
    "\n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter((F.col(\"datasourceId\") == \"chembl\")).join(actionType, on=['targetId','drugId'], how='left')\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(F.col(\"clinicalPhase\")).over(\n",
    "                Window.partitionBy(\"targetId\", \"diseaseId\")\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\",'actionType2')\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    #.filter(F.col(\"coherencyDiagonal\") == \"coherent\")\n",
    "    .drop(\n",
    "        \"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\"\n",
    "    )\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    ")\n",
    "\n",
    "benchmark = (\n",
    "    (\n",
    "        resolvedColocFiltered.filter( ## .filter(F.col(\"betaGwas\") < 0)\n",
    "        F.col(\"name\") != \"COVID-19\"\n",
    "    )\n",
    "        .join(  ### select just GWAS giving protection\n",
    "            analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\"  ### RIGHT SIDE\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"AgreeDrug\",\n",
    "            F.when(\n",
    "                (F.col(\"drugGoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"GoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"drugLoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"LoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "    )  #### remove COVID-19 associations\n",
    ").join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\")\n",
    "    .count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\"))\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "### create disdic dictionary\n",
    "disdic={}\n",
    "\n",
    "# --- Configuration for your iterative pivoting ---\n",
    "group_by_columns = ['targetId', 'diseaseId','phase4Clean','phase3Clean','phase2Clean','phase1Clean','PhaseT']\n",
    "#columns_to_pivot_on = ['actionType2', 'biosampleName', 'projectId', 'rightStudyType','colocalisationMethod']\n",
    "columns_to_pivot_on = ['projectId']\n",
    "columns_to_aggregate = ['NoneCellYes', 'NdiagonalYes','hasGenetics'] # The values you want to collect in the pivoted cells\n",
    "all_pivoted_dfs = {}\n",
    "\n",
    "doe_columns=[\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "diagonal_lof=['LoF_protect','GoF_risk']\n",
    "diagonal_gof=['LoF_risk','GoF_protect']\n",
    "\n",
    "conditions = [\n",
    "    F.when(F.col(c) == F.col(\"maxDoE\"), F.lit(c)).otherwise(F.lit(None)) for c in doe_columns\n",
    "    ]\n",
    "print('entering the big loops')\n",
    "# --- Nested Loops for Dynamic Pivoting ---\n",
    "for agg_col_name in columns_to_aggregate:\n",
    "    for pivot_col_name in columns_to_pivot_on:\n",
    "        print(f\"\\n--- Creating DataFrame for Aggregation: '{agg_col_name}' and Pivot: '{pivot_col_name}' ---\")\n",
    "        current_col_pvalue_order_window = Window.partitionBy(\"targetId\", \"diseaseId\", \"maxClinPhase\", pivot_col_name).orderBy(F.col('colocalisationMethod').asc(), F.col(\"qtlPValueExponent\").asc())\n",
    "        test2=discrepancifier(benchmark.withColumn('actionType2', F.concat_ws(\",\", F.col(\"actionType2\"))).withColumn('qtlColocDoE',F.first('colocDoE').over(current_col_pvalue_order_window)).groupBy(\n",
    "        \"targetId\", \"diseaseId\", \"maxClinPhase\", \"drugLoF_protect\", \"drugGoF_protect\",pivot_col_name)\n",
    "        .pivot(\"colocDoE\")\n",
    "        .count()\n",
    "        .withColumnRenamed('drugLoF_protect', 'LoF_protect_ch')\n",
    "        .withColumnRenamed('drugGoF_protect', 'GoF_protect_ch')).withColumn( ## .filter(F.col('coherencyDiagonal')!='noEvid')\n",
    "    \"arrayN\", F.array(*[F.col(c) for c in doe_columns])\n",
    "    ).withColumn(\n",
    "        \"maxDoE\", F.array_max(F.col(\"arrayN\"))\n",
    "    ).withColumn(\"maxDoE_names\", F.array(*conditions)\n",
    "    ).withColumn(\"maxDoE_names\", F.expr(\"filter(maxDoE_names, x -> x is not null)\")\n",
    "    ).withColumn(\n",
    "        \"NoneCellYes\",\n",
    "        F.when((F.col(\"LoF_protect_ch\").isNotNull() & (F.col('GoF_protect_ch').isNull())) & (F.array_contains(F.col(\"maxDoE_names\"), F.lit(\"LoF_protect\")))==True, F.lit('yes'))\n",
    "        .when((F.col(\"GoF_protect_ch\").isNotNull() & (F.col('LoF_protect_ch').isNull())) & (F.array_contains(F.col(\"maxDoE_names\"), F.lit(\"GoF_protect\")))==True, F.lit('yes')\n",
    "            ).otherwise(F.lit('no'))  # If the value is null, return null # Otherwise, check if name is in array\n",
    "    ).withColumn(\n",
    "        \"NdiagonalYes\",\n",
    "        F.when((F.col(\"LoF_protect_ch\").isNotNull() & (F.col('GoF_protect_ch').isNull())) & \n",
    "            (F.size(F.array_intersect(F.col(\"maxDoE_names\"), F.array([F.lit(x) for x in diagonal_lof]))) > 0),\n",
    "            F.lit(\"yes\")\n",
    "        ).when((F.col(\"GoF_protect_ch\").isNotNull() & (F.col('LoF_protect_ch').isNull())) & \n",
    "            (F.size(F.array_intersect(F.col(\"maxDoE_names\"), F.array([F.lit(x) for x in diagonal_gof]))) > 0),\n",
    "            F.lit(\"yes\")\n",
    "        ).otherwise(F.lit('no'))\n",
    "    ).withColumn(\n",
    "        \"drugCoherency\",\n",
    "        F.when(\n",
    "            (F.col(\"LoF_protect_ch\").isNotNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNull()), F.lit(\"coherent\")\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"LoF_protect_ch\").isNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNotNull()), F.lit(\"coherent\")\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"LoF_protect_ch\").isNotNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNotNull()), F.lit(\"dispar\")\n",
    "        )\n",
    "        .otherwise(F.lit(\"other\")),\n",
    "    ).join(negativeTD, on=[\"targetId\", \"diseaseId\"], how=\"left\").withColumn(\n",
    "        \"PhaseT\",\n",
    "        F.when(F.col(\"stopReason\") == \"Negative\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase4Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") == 4) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase3Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 3) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase2Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 2) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase1Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 1) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"hasGenetics\",\n",
    "        F.when(F.col(\"coherencyDiagonal\") != \"noEvid\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    )\n",
    "        # 1. Get distinct values for the pivot column (essential for pivot())\n",
    "        # This brings a small amount of data to the driver, but is necessary for the pivot schema.\n",
    "        #distinct_pivot_values = [row[0] for row in test2.select(pivot_col_name).distinct().collect()]\n",
    "        # print(f\"Distinct values for '{pivot_col_name}': {distinct_pivot_values}\")\n",
    "\n",
    "        # 2. Perform the groupBy, pivot, and aggregate operations\n",
    "        # The .pivot() function requires the list of distinct values for better performance\n",
    "        # and correct schema inference.\n",
    "        pivoted_df = (\n",
    "            test2.groupBy(*group_by_columns)\n",
    "            .pivot(pivot_col_name) # Provide distinct values distinct_pivot_values\n",
    "            .agg(F.collect_set(F.col(agg_col_name))) # Collect all values into a set\n",
    "            .fillna(0) # Fill cells that have no data with an empty list instead of null\n",
    "        )\n",
    "        # 3. Add items to dictionary to map the columns:\n",
    "        # filter out None and 'null':\n",
    "        datasetColumns=pivoted_df.columns\n",
    "        filtered = [x for x in datasetColumns if x is not None and x != 'null']\n",
    "        # using list comprehension\n",
    "        for item in filtered:\n",
    "            disdic[item] = pivot_col_name\n",
    "\n",
    "        # 3. Add the 'data' literal column dynamically\n",
    "        # This column indicates which aggregation column was used.\n",
    "        #pivoted_df = pivoted_df.withColumn('data', F.lit(f'Drug_{agg_col_name}'))\n",
    "\n",
    "        array_columns_to_convert = [\n",
    "            field.name for field in pivoted_df.schema.fields\n",
    "            if isinstance(field.dataType, ArrayType)\n",
    "        ]\n",
    "        print(f\"Identified ArrayType columns for conversion: {array_columns_to_convert}\")\n",
    "\n",
    "        # 4. Apply the conversion logic to each identified array column\n",
    "        df_after_conversion = pivoted_df # Start with the pivoted_df\n",
    "        for col_to_convert in array_columns_to_convert:\n",
    "            df_after_conversion = df_after_conversion.withColumn(\n",
    "                col_to_convert,\n",
    "                F.when(F.col(col_to_convert).isNull(), F.lit('no'))          # Handle NULLs (from pivot for no data)\n",
    "                .when(F.size(F.col(col_to_convert)) == 0, F.lit('no'))       # Empty array -> 'no'\n",
    "                .when(F.array_contains(F.col(col_to_convert), F.lit('yes')), F.lit('yes')) # Contains 'yes' -> 'yes'\n",
    "                .when(F.array_contains(F.col(col_to_convert), F.lit('no')), F.lit('no'))   # Contains 'no' -> 'no'\n",
    "                .otherwise(F.lit('no')) # Fallback for unexpected array content (e.g., ['other'], ['yes','no'])\n",
    "            )\n",
    "\n",
    "        # 4. Generate a unique name for this DataFrame and store it\n",
    "        df_key = f\"df_pivot_{agg_col_name.lower()}_by_{pivot_col_name.lower()}\"\n",
    "        all_pivoted_dfs[df_key] = df_after_conversion.withColumnRenamed( 'phase4Clean','Phase>=4'\n",
    "        ).withColumnRenamed('phase3Clean','Phase>=3'\n",
    "        ).withColumnRenamed('phase2Clean','Phase>=2'\n",
    "        ).withColumnRenamed('phase1Clean','Phase>=1')\n",
    "\n",
    "\n",
    "# --- Accessing your generated DataFrames ---\n",
    "print(\"\\n--- All generated DataFrames are stored in 'all_pivoted_dfs' dictionary ---\")\n",
    "print(\"Keys available:\", all_pivoted_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------+--------+--------+--------+------+----+-----------+----------+---------+---------------------+--------+---------+---+-----+----------+----------+------+------------+------------+-------+--------+----+--------------+------+-----------+-----------+-----------------+----------+--------------+-----------+------------+------+----+---------+----------+------+----------+------+-------------+--------------+---------------------+--------------+--------+-------+-----------+-----------+----------+-------+----------------+--------------------+----------------+--------------+-----------------+--------------------+---------------+------------------+-------+\n",
      "|       targetId|     diseaseId|Phase>=4|Phase>=3|Phase>=2|Phase>=1|PhaseT|null|Alasoo_2018|Aygun_2021|BLUEPRINT|Bossini-Castillo_2019|BrainSeq|Braineac2|CAP|CEDAR|CommonMind|Cytoimmgen|FUSION|Fairfax_2012|Fairfax_2014|GENCORD|GEUVADIS|GTEx|Gilchrist_2021|HipSci|Jerber_2021|Kasela_2017|Kim-Hellmuth_2017|Lepik_2017|Naranbhai_2015|Nathan_2022|Nedelec_2016|OneK1K|PISA|Peng_2018|Perez_2022|PhLiPS|Quach_2016|ROSMAP|Randolph_2021|Schmiedel_2018|Schwartzentruber_2018|Steinberg_2020|Sun_2018|TwinsUK|UKB_PPP_EUR|Walker_2019|Young_2019|iPSCORE|van_de_Bunt_2015|othersProjectId_only|estimulated_only|nonEstimulated|canonicalCellLine|nonCanonicalCellLine|derivedCellLine|nonDerivedCellLine|GTExUKB|\n",
      "+---------------+--------------+--------+--------+--------+--------+------+----+-----------+----------+---------+---------------------+--------+---------+---+-----+----------+----------+------+------------+------------+-------+--------+----+--------------+------+-----------+-----------+-----------------+----------+--------------+-----------+------------+------+----+---------+----------+------+----------+------+-------------+--------------+---------------------+--------------+--------+-------+-----------+-----------+----------+-------+----------------+--------------------+----------------+--------------+-----------------+--------------------+---------------+------------------+-------+\n",
      "|ENSG00000004468|   EFO_0000183|      no|      no|      no|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000004779|    HP_0001249|      no|      no|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000004779| MONDO_0018906|      no|      no|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000005844| MONDO_0005147|      no|      no|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000007314|   EFO_1000249|      no|     yes|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000007314|   EFO_1001866|      no|      no|      no|      no|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000010671|   EFO_1000749|      no|      no|      no|      no|   yes|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000011677|   EFO_1000644|      no|     yes|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000012779|   EFO_0004616|      no|      no|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000012779|   EFO_0006911|      no|      no|      no|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000014138|   EFO_1001945|      no|     yes|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000014138| MONDO_0000159|      no|      no|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000022355|   EFO_0009708|      no|      no|      no|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000022355|    HP_0003419|      no|     yes|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000023228|   EFO_1001818|      no|      no|      no|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000037280|   EFO_0003869|      no|     yes|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000043591| MONDO_0001134|      no|     yes|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000043591| MONDO_0007254|      no|     yes|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000043591|Orphanet_79292|      no|      no|      no|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "|ENSG00000048392| MONDO_0008315|      no|      no|     yes|     yes|    no|  no|         no|        no|       no|                   no|      no|       no| no|   no|        no|        no|    no|          no|          no|     no|      no|  no|            no|    no|         no|         no|               no|        no|            no|         no|          no|    no|  no|       no|        no|    no|        no|    no|           no|            no|                   no|            no|      no|     no|         no|         no|        no|     no|              no|                  no|              no|            no|               no|                  no|             no|                no|     no|\n",
      "+---------------+--------------+--------+--------+--------+--------+------+----+-----------+----------+---------+---------------------+--------+---------+---+-----+----------+----------+------+------------+------------+-------+--------+----+--------------+------+-----------+-----------+-----------------+----------+--------------+-----------+------------+------+----+---------+----------+------+----------+------+-------------+--------------+---------------------+--------------+--------+-------+-----------+-----------+----------+-------+----------------+--------------------+----------------+--------------+-----------------+--------------------+---------------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with df_pivot_nonecellyes_by_projectid\n",
      "There are  8 columns to analyse with phases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/othersProjectId_only_predictor_Phase>=4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_84 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_27 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_43 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_96 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_11 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_43 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_37 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_133 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_122 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_11 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_71 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_65 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_95 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_183 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_97 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_39 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_133 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_18 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_67 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_67 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_135 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_39 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_42 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_67 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_162 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_120 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_95 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_27 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_162 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_11 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_15 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_171 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_141 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_169 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_59 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_128 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_84 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_64 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_87 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_104 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_71 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_74 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_143 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_23 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_120 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_141 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_173 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_97 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_22 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_3 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_37 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_133 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_40 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_63 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_159 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_38 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_149 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_135 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_31 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_158 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_133 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_180 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_3 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_95 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_107 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_173 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_90 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_112 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_7 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_95 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_107 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_101 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_110 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_135 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_26 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_66 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_63 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_43 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_30 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_33 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_72 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_105 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_39 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_169 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_38 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_105 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_63 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_63 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_39 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_87 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_23 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_183 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_42 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_104 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_104 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_180 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_159 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_123 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_104 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_170 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_140 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_101 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_15 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_171 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_69 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_7 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_64 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_85 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_116 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_183 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_38 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_97 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_63 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_7 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_7 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_39 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_66 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_171 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_159 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_3 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_53 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_3 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_107 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_169 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_107 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_66 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_47 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_149 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_110 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_33 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_104 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_104 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_42 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_65 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_170 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_61 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_140 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_113 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_133 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_120 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_23 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_94 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_19 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_110 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_31 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_19 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_37 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_105 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_165 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_149 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_31 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_65 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_41 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_27 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_27 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_140 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_37 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_11 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_183 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_128 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_122 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_180 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_43 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_23 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_66 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_42 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_117 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_15 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_105 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_97 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_122 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_143 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_10 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_98 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_107 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_71 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_71 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_170 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_40 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_151 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_162 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_183 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_7 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_140 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_120 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_15 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_66 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_171 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_71 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_170 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_61 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_88 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_92 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_122 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_141 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_14 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_149 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_31 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_98 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_110 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_15 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_42 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_46 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_42 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_101 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_43 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_101 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_135 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_151 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_162 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_66 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_173 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_143 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_156 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_110 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_50 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_143 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_142 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_159 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_95 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_141 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_128 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_78 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_171 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_163 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_151 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_27 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_67 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_101 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_162 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_33 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_58 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_159 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_33 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_87 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_57 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_71 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_138 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_37 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_60 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_149 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_61 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_38 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_6 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_170 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_84 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_128 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_23 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_43 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_40 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_105 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_70 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_169 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_151 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_180 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_151 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_122 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_65 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_37 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_64 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_84 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_38 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_40 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_33 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_19 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_3 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_54 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_173 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_67 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_87 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_84 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_61 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_141 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_143 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_173 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_11 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_169 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_31 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_98 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_140 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_153 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_128 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_64 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_105 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_67 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_40 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_64 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_180 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_120 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_39 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_38_2 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_87 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_595_19 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_97 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_107 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_98 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_19 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_65 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_260_135 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_155_38 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_98 !\n",
      "25/09/18 07:12:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_71_61 !\n",
      "25/09/18 07:12:18 WARN YarnAllocator: Container from a bad node: container_1758173756047_0002_01_000005 on host: jr-doe-temp1-m.c.open-targets-eu-dev.internal. Exit status: 143. Diagnostics: [2025-09-18 07:12:18.566]Container killed on request. Exit code is 143\n",
      "[2025-09-18 07:12:18.566]Container exited with a non-zero exit code 143. \n",
      "[2025-09-18 07:12:18.567]Killed by external signal\n",
      ".\n",
      "25/09/18 07:12:18 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1758173756047_0002_01_000005 on host: jr-doe-temp1-m.c.open-targets-eu-dev.internal. Exit status: 143. Diagnostics: [2025-09-18 07:12:18.566]Container killed on request. Exit code is 143\n",
      "[2025-09-18 07:12:18.566]Container exited with a non-zero exit code 143. \n",
      "[2025-09-18 07:12:18.567]Killed by external signal\n",
      ".\n",
      "25/09/18 07:12:18 ERROR YarnScheduler: Lost executor 4 on jr-doe-temp1-m.c.open-targets-eu-dev.internal: Container from a bad node: container_1758173756047_0002_01_000005 on host: jr-doe-temp1-m.c.open-targets-eu-dev.internal. Exit status: 143. Diagnostics: [2025-09-18 07:12:18.566]Container killed on request. Exit code is 143\n",
      "[2025-09-18 07:12:18.566]Container exited with a non-zero exit code 143. \n",
      "[2025-09-18 07:12:18.567]Killed by external signal\n",
      ".\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/othersProjectId_only_predictor_Phase>=3.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/othersProjectId_only_predictor_Phase>=2.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/othersProjectId_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/othersProjectId_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/estimulated_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/estimulated_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/estimulated_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/estimulated_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/estimulated_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonEstimulated_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonEstimulated_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonEstimulated_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonEstimulated_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonEstimulated_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/canonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonCanonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/derivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/nonDerivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/GTExUKB_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/GTExUKB_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/GTExUKB_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/GTExUKB_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_nonecellyes_by_projectid/GTExUKB_predictor_PhaseT.parquet\n",
      "results appended\n",
      "df unpersisted\n",
      "working with df_pivot_ndiagonalyes_by_projectid\n",
      "There are  8 columns to analyse with phases\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/othersProjectId_only_predictor_Phase>=4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/othersProjectId_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/othersProjectId_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/othersProjectId_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/othersProjectId_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/estimulated_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/estimulated_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/estimulated_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/estimulated_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/estimulated_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonEstimulated_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonEstimulated_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonEstimulated_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonEstimulated_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonEstimulated_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/canonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonCanonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/derivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/nonDerivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/GTExUKB_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/GTExUKB_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/GTExUKB_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/GTExUKB_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_ndiagonalyes_by_projectid/GTExUKB_predictor_PhaseT.parquet\n",
      "results appended\n",
      "df unpersisted\n",
      "working with df_pivot_hasgenetics_by_projectid\n",
      "There are  8 columns to analyse with phases\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/othersProjectId_only_predictor_Phase>=4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/othersProjectId_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/othersProjectId_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/othersProjectId_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='othersProjectId_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/othersProjectId_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/estimulated_only_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/estimulated_only_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/estimulated_only_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/estimulated_only_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='estimulated_only', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/estimulated_only_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonEstimulated_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonEstimulated_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonEstimulated_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonEstimulated_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonEstimulated', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonEstimulated_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='canonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/canonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonCanonicalCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonCanonicalCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='derivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/derivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='nonDerivedCellLine', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/nonDerivedCellLine_predictor_PhaseT.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=4', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/GTExUKB_predictor_Phase>=4.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=3', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/GTExUKB_predictor_Phase>=3.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=2', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/GTExUKB_predictor_Phase>=2.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='Phase>=1', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/GTExUKB_predictor_Phase>=1.parquet\n",
      "results appended\n",
      "performing Row(comparison='GTExUKB', comparisonType='predictor', _1='PhaseT', _2='clinical')\n",
      "gs://ot-team/jroldan/2025-09-18_analysis/df_pivot_hasgenetics_by_projectid/GTExUKB_predictor_PhaseT.parquet\n",
      "results appended\n",
      "df unpersisted\n",
      "importing functions\n",
      "imported functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe written \n",
      " Analysis finished\n"
     ]
    }
   ],
   "source": [
    "##### PROJECTID\n",
    "project_keys = (\n",
    "    benchmark\n",
    "    .select(\"projectId\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .map(lambda r: r[0])\n",
    "    .filter(lambda x: x is not None)  # <- remove NULLs\n",
    "    .collect()\n",
    ")\n",
    "#project_keys=[f\"{k}_only\" for k,v in disdic.items() if v == 'projectId']\n",
    "main=['GTEx_only', 'UKB_PPP_EUR_only']\n",
    "#stimulated=['Alasoo_2018_only','Cytoimmgen_only','Fairfax_2014_only','Kim-Hellmuth_2017_only','Nathan_2022_only','Nedelec_2016_only','Quach_2016_only','Randolph_2021_only','Schmiedel_2018_only']\n",
    "#cellLine=['CAP_only','HipSci_only','iPSCORE_only','Jerber_2021_only','PhLiPS_only','Schwartzentruber_2018_only','TwinsUK_only']\n",
    "derivedCellLine=['TwinsUK_only','PhLiPS_only','CAP_only','GENCORD_only','Sun_2018_only','Nedelec_2016_only']\n",
    "canonicalCellLine=['Alasoo_2018_only','Jerber_2021_only','GEUVADIS_only','iPSCORE_only','Aygun_2021_only','Schwartzentruber_2018_only']\n",
    "stimulated=['Schmiedel_2018_only','Bossini-Castillo_2019_only','Alasoo_2018_only','Cytoimmgen_only','Gilchrist_2021_only','CAP_only','Quach_2016_only','Randolph_2021_only','Sun_2018_only','Nedelec_2016_only','Kim-Hellmuth_2017_only']\n",
    "\n",
    "def strip_only(lst):\n",
    "    return [x.removesuffix(\"_only\") for x in lst]  # Python 3.9+\n",
    "    # or: return [x[:-5] if x.endswith(\"_only\") else x for x in lst]\n",
    "\n",
    "# Apply\n",
    "main = strip_only(main)\n",
    "canonicalCellLine = strip_only(canonicalCellLine)\n",
    "derivedCellLine = strip_only(derivedCellLine)\n",
    "stimulated = strip_only(stimulated)\n",
    "\n",
    "others=[item for item in project_keys if item not in main]\n",
    "nonStimulated=[item for item in project_keys if item not in stimulated]\n",
    "nonCanonicalCellLine = [item for item in project_keys if item not in canonicalCellLine]\n",
    "nonDerivedCellLine = [item for item in project_keys if item not in derivedCellLine]\n",
    "\n",
    "\n",
    "# First condition: any \"yes\" in list1\n",
    "# others\n",
    "condition1 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), others[1:], F.col(others[0]) == \"yes\")\n",
    "# estimulated\n",
    "condition2 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), stimulated[1:], F.col(stimulated[0]) == \"yes\")\n",
    "## non estimulated:\n",
    "condition3 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), nonStimulated[1:], F.col(nonStimulated[0]) == \"yes\")\n",
    "# canonical cellLine\n",
    "condition4 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), canonicalCellLine[1:], F.col(canonicalCellLine[0]) == \"yes\")\n",
    "# non canonical cellline\n",
    "condition5 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), nonCanonicalCellLine[1:], F.col(nonCanonicalCellLine[0]) == \"yes\")\n",
    "# derived cell line \n",
    "condition6 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), derivedCellLine[1:], F.col(derivedCellLine[0]) == \"yes\")\n",
    "# non derived cellline\n",
    "condition7 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), nonDerivedCellLine[1:], F.col(nonDerivedCellLine[0]) == \"yes\")\n",
    "# mainprojects\n",
    "condition8 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), main[1:], F.col(main[0]) == \"yes\")\n",
    "\n",
    "\n",
    "# Add both columns\n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"othersProjectId_only\", F.when(condition1, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"estimulated_only\", F.when(condition2, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"nonEstimulated\", F.when(condition3, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"canonicalCellLine\", F.when(condition4, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"nonCanonicalCellLine\", F.when(condition5, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"derivedCellLine\", F.when(condition6, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"nonDerivedCellLine\", F.when(condition7, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"GTExUKB\", F.when(condition8, \"yes\").otherwise(\"no\")) \n",
    "\n",
    "# Add both columns\n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"othersProjectId_only\", F.when(condition1, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"estimulated_only\", F.when(condition2, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"nonEstimulated\", F.when(condition3, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"canonicalCellLine\", F.when(condition4, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"nonCanonicalCellLine\", F.when(condition5, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"derivedCellLine\", F.when(condition6, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"nonDerivedCellLine\", F.when(condition7, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"GTExUKB\", F.when(condition8, \"yes\").otherwise(\"no\")) \n",
    "\n",
    "# Add both columns\n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"othersProjectId_only\", F.when(condition1, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"estimulated_only\", F.when(condition2, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"nonEstimulated\", F.when(condition3, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"canonicalCellLine\", F.when(condition4, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"nonCanonicalCellLine\", F.when(condition5, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"derivedCellLine\", F.when(condition6, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"nonDerivedCellLine\", F.when(condition7, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"GTExUKB\", F.when(condition8, \"yes\").otherwise(\"no\")) \n",
    "\n",
    "\n",
    "# If you wanted to apply to every DF in the dict (only if they all share *_only columns):\n",
    "# for k, df in all_pivoted_dfs.items():\n",
    "#     all_pivoted_dfs[k] = add_project_group_flags(df, main, stimulated, cellLine)\n",
    "\n",
    "###append to dictionary\n",
    "\n",
    "disdic.update({'othersProjectId': 'projectId','Stimulated': 'projectId','cellLine': 'projectId', 'othersBiosampleName_only': 'biosampleName', 'otherRightStudyType':'rightStudyType'})\n",
    "\n",
    "###################################\n",
    "###################################\n",
    "result = []\n",
    "result_st = []\n",
    "result_ci = []\n",
    "array2 = []\n",
    "listado = []\n",
    "result_all = []\n",
    "today_date = str(date.today())\n",
    "\n",
    "for key,df in all_pivoted_dfs.items():\n",
    "\n",
    "    print(f'working with {key}')\n",
    "    parts = key.split('_by_') ### take the part of key belonging to column name\n",
    "    column_name = parts[1] ### take the last part which is column name\n",
    "    all_pivoted_dfs[key].persist()\n",
    "    #unique_values = all_pivoted_dfs[key].drop('null').columns[7:]\n",
    "    unique_values = all_pivoted_dfs[key].drop('null').columns[-8:] ### just the interesting columns for us \n",
    "    filtered_unique_values = [x for x in unique_values if x is not None and x != 'null']\n",
    "    print('There are ', len(filtered_unique_values), 'columns to analyse with phases')\n",
    "    rows = comparisons_df_iterative(filtered_unique_values)\n",
    "\n",
    "    # If needed, now process the rest\n",
    "    for row in rows:\n",
    "        print('performing', row)\n",
    "        results = aggregations_original(\n",
    "            all_pivoted_dfs[key], key, listado, *row, today_date\n",
    "        )\n",
    "        result_all.append(results)\n",
    "        print('results appended')\n",
    "    all_pivoted_dfs[key].unpersist()\n",
    "    print('df unpersisted')\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"group\", StringType(), True),\n",
    "        StructField(\"comparison\", StringType(), True),\n",
    "        StructField(\"phase\", StringType(), True),\n",
    "        StructField(\"oddsRatio\", DoubleType(), True),\n",
    "        StructField(\"pValue\", DoubleType(), True),\n",
    "        StructField(\"lowerInterval\", DoubleType(), True),\n",
    "        StructField(\"upperInterval\", DoubleType(), True),\n",
    "        StructField(\"total\", StringType(), True),\n",
    "        StructField(\"values\", ArrayType(ArrayType(IntegerType())), True),\n",
    "        StructField(\"relSuccess\", DoubleType(), True),\n",
    "        StructField(\"rsLower\", DoubleType(), True),\n",
    "        StructField(\"rsUpper\", DoubleType(), True),\n",
    "        StructField(\"path\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "import re\n",
    "\n",
    "# Define the list of patterns to search for\n",
    "patterns = [\n",
    "    \"_only\",\n",
    "    #\"_tissue\",\n",
    "    #\"_isSignalFromRightTissue\",\n",
    "    \"_isRightTissueSignalAgreed\",\n",
    "]\n",
    "# Create a regex pattern to match any of the substrings\n",
    "regex_pattern = \"(\" + \"|\".join(map(re.escape, patterns)) + \")\"\n",
    "\n",
    "# Convert list of lists to DataFrame\n",
    "df = (\n",
    "    spreadSheetFormatter(spark.createDataFrame(result_all, schema=schema))\n",
    "    .withColumn(\n",
    "        \"prefix\",\n",
    "        F.regexp_replace(\n",
    "            F.col(\"comparison\"), regex_pattern + \".*\", \"\"\n",
    "        ),  # Extract part before the pattern\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"suffix\",\n",
    "        F.regexp_extract(\n",
    "            F.col(\"comparison\"), regex_pattern, 0\n",
    "        ),  # Extract the pattern itself\n",
    "    )\n",
    ")\n",
    "\n",
    "### annotate projectId, tissue, qtl type and doe type:\n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr=create_map([F.lit(x) for x in chain(*disdic.items())])\n",
    "\n",
    "df_annot=df.withColumn('annotation',mapping_expr.getItem(F.col('prefix')))\n",
    "\n",
    "df_annot.toPandas().to_csv(\n",
    "    f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_AllPhasesMixtures3.csv\"\n",
    ")\n",
    "\n",
    "print(\"dataframe written \\n Analysis finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nonStimulated',\n",
       " 'canonicalCellLine',\n",
       " 'nonCanonicalCellLine',\n",
       " 'derivedCellLine',\n",
       " 'nonDerivedCellLine',\n",
       " 'estimulated_only',\n",
       " 'nonEstimulated',\n",
       " 'GTExUKB']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].columns[-8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I put the code here to notice that is the one working for studiesJoint, is the same as above, but altogether in a chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from array import ArrayType\n",
    "from functions import (\n",
    "    relative_success,\n",
    "    spreadSheetFormatter,\n",
    "    discrepancifier,\n",
    "    temporary_directionOfEffect,\n",
    "    buildColocData,\n",
    "    gwasDataset,\n",
    ")\n",
    "# from stoppedTrials import terminated_td\n",
    "from DoEAssessment import directionOfEffect\n",
    "# from membraneTargets import target_membrane\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    DecimalType,\n",
    "    StringType,\n",
    "    FloatType,\n",
    ")\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# --- Build the SparkSession ---\n",
    "# Use the .config() method to set these parameters before calling .getOrCreate()\n",
    "# This ensures Spark requests the correct resources from YARN at the start.\n",
    "driver_memory = \"24g\"                 # plenty for planning & small collects\n",
    "executor_cores = 4                    # sweet spot for GC + Python workers\n",
    "num_executors  = 12                   # 12 * 4 = 48 cores for executors; ~16 cores left for driver/OS\n",
    "executor_memory = \"32g\"               # per executor heap\n",
    "executor_memory_overhead = \"8g\"       # ~20% overhead for PySpark/Arrow/off-heap\n",
    "# Totals: (32+8) * 12 = 480 GB executors + 24 GB driver ≈ 504 GB (adjust down if your hard cap is <500 GB)\n",
    "# If you must stay strictly ≤ 500 GB, use executor_memory=\"30g\", overhead=\"6g\"  → (36 * 12) + 24 = 456 + 24 = 480 GB\n",
    "\n",
    "shuffle_partitions   = 192            # ≈ 2–4× total cores (48) → start with 192\n",
    "default_parallelism  = 192\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyOptimizedPySparkApp\") \\\n",
    "    .config(\"spark.master\", \"yarn\") \\\n",
    "    .config(\"spark.driver.memory\", driver_memory) \\\n",
    "    .config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .config(\"spark.executor.cores\", executor_cores) \\\n",
    "    .config(\"spark.executor.instances\", num_executors) \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", executor_memory_overhead) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", shuffle_partitions) \\\n",
    "    .config(\"spark.default.parallelism\", default_parallelism) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"SparkSession created successfully with the following configurations:\")\n",
    "print(f\"  spark.driver.memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"  spark.executor.memory: {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"  spark.executor.cores: {spark.conf.get('spark.executor.cores')}\")\n",
    "print(f\"  spark.executor.instances: {spark.conf.get('spark.executor.instances')}\")\n",
    "print(f\"  spark.yarn.executor.memoryOverhead: {spark.conf.get('spark.yarn.executor.memoryOverhead')}\")\n",
    "print(f\"  spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"  spark.default.parallelism: {spark.conf.get('spark.default.parallelism')}\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# --- Your PySpark Code Here ---\n",
    "# Now you can proceed with your data loading and processing.\n",
    "# Example:\n",
    "# df = spark.read.parquet(\"hdfs:///user/your_user/your_large_data.parquet\")\n",
    "# print(f\"Number of rows in DataFrame: {df.count()}\")\n",
    "# df.groupBy(\"some_column\").agg({\"another_column\": \"sum\"}).show()\n",
    "\n",
    "# Remember to stop the SparkSession when you are done\n",
    "# spark.stop()\n",
    "\n",
    "path_n='gs://open-targets-data-releases/25.06/output/'\n",
    "\n",
    "target = spark.read.parquet(f\"{path_n}target/\")\n",
    "\n",
    "diseases = spark.read.parquet(f\"{path_n}disease/\")\n",
    "\n",
    "evidences = spark.read.parquet(f\"{path_n}evidence\")\n",
    "\n",
    "credible = spark.read.parquet(f\"{path_n}credible_set\")\n",
    "\n",
    "new = spark.read.parquet(f\"{path_n}colocalisation_coloc\") \n",
    "\n",
    "index=spark.read.parquet(f\"{path_n}study/\")\n",
    "\n",
    "variantIndex = spark.read.parquet(f\"{path_n}variant\")\n",
    "\n",
    "biosample = spark.read.parquet(f\"{path_n}biosample\")\n",
    "\n",
    "ecaviar=spark.read.parquet(f\"{path_n}colocalisation_ecaviar\")\n",
    "\n",
    "all_coloc=ecaviar.unionByName(new, allowMissingColumns=True)\n",
    "\n",
    "print(\"loaded files\")\n",
    "\n",
    "#### FIRST MODULE: BUILDING COLOC \n",
    "newColoc=buildColocData(all_coloc,credible,index)\n",
    "\n",
    "print(\"loaded newColoc\")\n",
    "\n",
    "### SECOND MODULE: PROCESS EVIDENCES TO AVOID EXCESS OF COLUMNS \n",
    "gwasComplete = gwasDataset(evidences,credible)\n",
    "\n",
    "#### THIRD MODULE: INCLUDE COLOC IN THE \n",
    "resolvedColoc = (\n",
    "    (\n",
    "        newColoc.withColumnRenamed(\"geneId\", \"targetId\")\n",
    "        .join(\n",
    "            gwasComplete.withColumnRenamed(\"studyLocusId\", \"leftStudyLocusId\"),\n",
    "            on=[\"leftStudyLocusId\", \"targetId\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .join(  ### propagated using parent terms\n",
    "            diseases.selectExpr(\n",
    "                \"id as diseaseId\", \"name\", \"parents\", \"therapeuticAreas\"\n",
    "            ),\n",
    "            on=\"diseaseId\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"diseaseId\",\n",
    "            F.explode_outer(F.concat(F.array(F.col(\"diseaseId\")), F.col(\"parents\"))),\n",
    "        )\n",
    "        .drop(\"parents\", \"oldDiseaseId\")\n",
    "    ).withColumn(\n",
    "        \"colocDoE\",\n",
    "        F.when(\n",
    "            F.col(\"rightStudyType\").isin(\n",
    "                [\"eqtl\", \"pqtl\", \"tuqtl\", \"sceqtl\", \"sctuqtl\"]\n",
    "            ),\n",
    "            F.when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"GoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"LoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"LoF_protect\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"GoF_protect\"),\n",
    "            ),\n",
    "        ).when(\n",
    "            F.col(\"rightStudyType\").isin(\n",
    "                [\"sqtl\", \"scsqtl\"]\n",
    "            ),  ### opposite directionality than sqtl\n",
    "            F.when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"LoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") > 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"GoF_risk\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") > 0),\n",
    "                F.lit(\"GoF_protect\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"betaGwas\") < 0) & (F.col(\"betaRatioSignAverage\") < 0),\n",
    "                F.lit(\"LoF_protect\"),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    # .persist()\n",
    ")\n",
    "print(\"loaded resolvedColloc\")\n",
    "\n",
    "datasource_filter = [\n",
    "#   \"ot_genetics_portal\",\n",
    "    \"gwas_credible_sets\",\n",
    "    \"gene_burden\",\n",
    "    \"eva\",\n",
    "    \"eva_somatic\",\n",
    "    \"gene2phenotype\",\n",
    "    \"orphanet\",\n",
    "    \"cancer_gene_census\",\n",
    "    \"intogen\",\n",
    "    \"impc\",\n",
    "    \"chembl\",\n",
    "]\n",
    "\n",
    "assessment, evidences, actionType, oncolabel = temporary_directionOfEffect(\n",
    "    path_n, datasource_filter\n",
    ")\n",
    "\n",
    "print(\"run temporary direction of effect\")\n",
    "\n",
    "\n",
    "print(\"built drugApproved dataset\")\n",
    "\n",
    "\n",
    "#### FOURTH MODULE BUILDING CHEMBL ASSOCIATIONS - HERE TAKE CARE WITH FILTERING STEP \n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter((F.col(\"datasourceId\") == \"chembl\"))\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(F.col(\"clinicalPhase\")).over(\n",
    "                Window.partitionBy(\"targetId\", \"diseaseId\")\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\")\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    #.filter(F.col(\"coherencyDiagonal\") == \"coherent\")\n",
    "    .drop(\n",
    "        \"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\"\n",
    "    )\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    "    # .persist()\n",
    ")\n",
    "\n",
    "####2 Define agregation function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import odds_ratio\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def convertTuple(tup):\n",
    "    st = \",\".join(map(str, tup))\n",
    "    return st\n",
    "\n",
    "\n",
    "#####3 run in a function\n",
    "def aggregations_original(\n",
    "    df,\n",
    "    data,\n",
    "    listado,\n",
    "    comparisonColumn,\n",
    "    comparisonType,\n",
    "    predictionColumn,\n",
    "    predictionType,\n",
    "    today_date,\n",
    "):\n",
    "    wComparison = Window.partitionBy(comparisonColumn)\n",
    "    wPrediction = Window.partitionBy(predictionColumn)\n",
    "    wPredictionComparison = Window.partitionBy(comparisonColumn, predictionColumn)\n",
    "    results = []\n",
    "    # uniqIds = df.select(\"targetId\", \"diseaseId\").distinct().count()\n",
    "    out = (\n",
    "        df.withColumn(\"comparisonType\", F.lit(comparisonType))\n",
    "        .withColumn(\"dataset\", F.lit(data))\n",
    "        .withColumn(\"predictionType\", F.lit(predictionType))\n",
    "        # .withColumn(\"total\", F.lit(uniqIds))\n",
    "        .withColumn(\"a\", F.count(\"targetId\").over(wPredictionComparison))\n",
    "        .withColumn(\"comparisonColumn\", F.lit(comparisonColumn))\n",
    "        .withColumn(\"predictionColumnValue\", F.lit(predictionColumn))\n",
    "        .withColumn(\n",
    "            \"predictionTotal\",\n",
    "            F.count(\"targetId\").over(wPrediction),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"comparisonTotal\",\n",
    "            F.count(\"targetId\").over(wComparison),\n",
    "        )\n",
    "        .select(\n",
    "            F.col(predictionColumn).alias(\"prediction\"),\n",
    "            F.col(comparisonColumn).alias(\"comparison\"),\n",
    "            \"dataset\",\n",
    "            \"comparisonColumn\",\n",
    "            \"predictionColumnValue\",\n",
    "            \"comparisonType\",\n",
    "            \"predictionType\",\n",
    "            \"a\",\n",
    "            \"predictionTotal\",\n",
    "            \"comparisonTotal\",\n",
    "        )\n",
    "        .filter(F.col(\"prediction\").isNotNull())\n",
    "        .filter(F.col(\"comparison\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    '''\n",
    "    out.write.mode(\"overwrite\").parquet(\n",
    "        \"gs://ot-team/jroldan/\"\n",
    "        + str(\n",
    "            today_date\n",
    "            + \"_\"\n",
    "            + \"analysis/\"\n",
    "            + data\n",
    "            # + \"_propagated\"\n",
    "            + \"/\"\n",
    "            + comparisonColumn\n",
    "            + \"_\"\n",
    "            + comparisonType\n",
    "            + \"_\"\n",
    "            + predictionColumn\n",
    "            + \".parquet\"\n",
    "        )\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    listado.append(\n",
    "        \"gs://ot-team/jroldan/\"\n",
    "        + str(\n",
    "            today_date\n",
    "            + \"_\"\n",
    "            + \"analysis/\"\n",
    "            + data\n",
    "            # + \"_propagated\"\n",
    "            + \"/\"\n",
    "            + comparisonColumn\n",
    "            + \"_\"\n",
    "            + comparisonType\n",
    "            + \"_\"\n",
    "            + predictionColumn\n",
    "            + \".parquet\"\n",
    "        )\n",
    "    )\n",
    "    path = \"gs://ot-team/jroldan/\" + str(\n",
    "        today_date\n",
    "        + \"_\"\n",
    "        + \"analysis/\"\n",
    "        + data\n",
    "        # + \"_propagated\"\n",
    "        + \"/\"\n",
    "        + comparisonColumn\n",
    "        + \"_\"\n",
    "        + comparisonType\n",
    "        + \"_\"\n",
    "        + predictionColumn\n",
    "        + \".parquet\"\n",
    "    )\n",
    "    print(path)\n",
    "    \n",
    "    ### making analysis\n",
    "    array1 = np.delete(\n",
    "        out.join(full_data, on=[\"prediction\", \"comparison\"], how=\"outer\")\n",
    "        .groupBy(\"comparison\")\n",
    "        .pivot(\"prediction\")\n",
    "        .agg(F.first(\"a\"))\n",
    "        .sort(F.col(\"comparison\").desc())\n",
    "        .select(\"comparison\", \"yes\", \"no\")\n",
    "        .fillna(0)\n",
    "        .toPandas()\n",
    "        .to_numpy(),\n",
    "        [0],\n",
    "        1,\n",
    "    )\n",
    "    total = np.sum(array1)\n",
    "    res_npPhaseX = np.array(array1, dtype=int)\n",
    "    resX = convertTuple(fisher_exact(res_npPhaseX, alternative=\"two-sided\"))\n",
    "    resx_CI = convertTuple(\n",
    "        odds_ratio(res_npPhaseX).confidence_interval(confidence_level=0.95)\n",
    "    )\n",
    "\n",
    "    result_st.append(resX)\n",
    "    result_ci.append(resx_CI)\n",
    "    (rs_result, rs_ci) = relative_success(array1)\n",
    "    results.extend(\n",
    "        [\n",
    "            comparisonType,\n",
    "            comparisonColumn,\n",
    "            predictionColumn,\n",
    "            round(float(resX.split(\",\")[0]), 2),\n",
    "            float(resX.split(\",\")[1]),\n",
    "            round(float(resx_CI.split(\",\")[0]), 2),\n",
    "            round(float(resx_CI.split(\",\")[1]), 2),\n",
    "            str(total),\n",
    "            np.array(res_npPhaseX).tolist(),\n",
    "            round(float(rs_result), 2),\n",
    "            round(float(rs_ci[0]), 2),\n",
    "            round(float(rs_ci[1]), 2),\n",
    "            # studies,\n",
    "            # tissues,\n",
    "            path,\n",
    "        ]\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "#### 3 Loop over different datasets (as they will have different rows and columns)\n",
    "\n",
    "\n",
    "def comparisons_df_iterative(elements):\n",
    "    #toAnalysis = [(key, value) for key, value in disdic.items() if value == projectId]\n",
    "    toAnalysis = [(col, \"predictor\") for col in elements]\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"comparison\", StringType(), True),\n",
    "            StructField(\"comparisonType\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    comparisons = spark.createDataFrame(toAnalysis, schema=schema)\n",
    "    ### include all the columns as predictor\n",
    "\n",
    "    predictions = spark.createDataFrame(\n",
    "        data=[\n",
    "            (\"Phase>=4\", \"clinical\"),\n",
    "            ('Phase>=3','clinical'),\n",
    "            ('Phase>=2','clinical'),\n",
    "            ('Phase>=1','clinical'),\n",
    "            (\"PhaseT\", \"clinical\"),\n",
    "        ]\n",
    "    )\n",
    "    return comparisons.join(predictions, how=\"full\").collect()\n",
    "\n",
    "\n",
    "print(\"load comparisons_df_iterative function\")\n",
    "\n",
    "\n",
    "full_data = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\"yes\", \"yes\"),\n",
    "        (\"yes\", \"no\"),\n",
    "        (\"no\", \"yes\"),\n",
    "        (\"no\", \"no\"),\n",
    "    ],\n",
    "    schema=StructType(\n",
    "        [\n",
    "            StructField(\"prediction\", StringType(), True),\n",
    "            StructField(\"comparison\", StringType(), True),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\"created full_data and lists\")\n",
    "\n",
    "#rightTissue = spark.read.csv(\n",
    "#    'gs://ot-team/jroldan/analysis/20250526_rightTissue.csv',\n",
    "#    header=True,\n",
    "#).drop(\"_c0\")\n",
    "\n",
    "print(\"loaded rightTissue dataset\")\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\")\n",
    "    .count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\"))\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "print(\"built negativeTD dataset\")\n",
    "\n",
    "print(\"built bench2 dataset\")\n",
    "\n",
    "###### cut from here\n",
    "print(\"looping for variables_study\")\n",
    "\n",
    "#### new part with chatgpt -- TEST\n",
    "\n",
    "## QUESTIONS TO ANSWER:\n",
    "# HAVE ECAVIAR >=0.8\n",
    "# HAVE COLOC \n",
    "# HAVE COLOC >= 0.8\n",
    "# HAVE COLOC + ECAVIAR >= 0.01\n",
    "# HAVE COLOC >= 0.8 + ECAVIAR >= 0.01\n",
    "# RIGHT JOING WITH CHEMBL \n",
    "\n",
    "### FIFTH MODULE: BUILDING BENCHMARK OF THE DATASET TO EXTRACT EHE ANALYSIS \n",
    "\n",
    "resolvedColocFiltered = resolvedColoc.filter((F.col('clpp')>=0.01) | (F.col('h4')>=0.8))\n",
    "benchmark = (\n",
    "    (\n",
    "        resolvedColocFiltered.filter( ## .filter(F.col(\"betaGwas\") < 0)\n",
    "        F.col(\"name\") != \"COVID-19\"\n",
    "    )\n",
    "        .join(  ### select just GWAS giving protection\n",
    "            analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\"  ### RIGHT SIDE\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"AgreeDrug\",\n",
    "            F.when(\n",
    "                (F.col(\"drugGoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"GoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"drugLoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"LoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "    )  #### remove COVID-19 associations\n",
    ").join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    "\n",
    "\n",
    "### drug mechanism of action\n",
    "mecact_path = f\"{path_n}drug_mechanism_of_action/\" #  mechanismOfAction == old version\n",
    "mecact = spark.read.parquet(mecact_path)\n",
    "\n",
    "inhibitors = [\n",
    "    \"RNAI INHIBITOR\",\n",
    "    \"NEGATIVE MODULATOR\",\n",
    "    \"NEGATIVE ALLOSTERIC MODULATOR\",\n",
    "    \"ANTAGONIST\",\n",
    "    \"ANTISENSE INHIBITOR\",\n",
    "    \"BLOCKER\",\n",
    "    \"INHIBITOR\",\n",
    "    \"DEGRADER\",\n",
    "    \"INVERSE AGONIST\",\n",
    "    \"ALLOSTERIC ANTAGONIST\",\n",
    "    \"DISRUPTING AGENT\",\n",
    "]\n",
    "\n",
    "activators = [\n",
    "    \"PARTIAL AGONIST\",\n",
    "    \"ACTIVATOR\",\n",
    "    \"POSITIVE ALLOSTERIC MODULATOR\",\n",
    "    \"POSITIVE MODULATOR\",\n",
    "    \"AGONIST\",\n",
    "    \"SEQUESTERING AGENT\",  ## lost at 31.01.2025\n",
    "    \"STABILISER\",\n",
    "    # \"EXOGENOUS GENE\", ## added 24.06.2025\n",
    "    # \"EXOGENOUS PROTEIN\" ## added 24.06.2025\n",
    "]\n",
    "\n",
    "\n",
    "actionType = (\n",
    "        mecact.select(\n",
    "            F.explode_outer(\"chemblIds\").alias(\"drugId\"),\n",
    "            \"actionType\",\n",
    "            \"mechanismOfAction\",\n",
    "            \"targets\",\n",
    "        )\n",
    "        .select(\n",
    "            F.explode_outer(\"targets\").alias(\"targetId\"),\n",
    "            \"drugId\",\n",
    "            \"actionType\",\n",
    "            \"mechanismOfAction\",\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"drugId\")\n",
    "        .agg(F.collect_set(\"actionType\").alias(\"actionType2\"))\n",
    "    ).withColumn('nMoA', F.size(F.col('actionType2')))\n",
    "\n",
    "analysis_chembl_indication = (\n",
    "    discrepancifier(\n",
    "        assessment.filter((F.col(\"datasourceId\") == \"chembl\")).join(actionType, on=['targetId','drugId'], how='left')\n",
    "        .withColumn(\n",
    "            \"maxClinPhase\",\n",
    "            F.max(F.col(\"clinicalPhase\")).over(\n",
    "                Window.partitionBy(\"targetId\", \"diseaseId\")\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"targetId\", \"diseaseId\", \"maxClinPhase\",'actionType2')\n",
    "        .pivot(\"homogenized\")\n",
    "        .agg(F.count(\"targetId\"))\n",
    "    )\n",
    "    #.filter(F.col(\"coherencyDiagonal\") == \"coherent\")\n",
    "    .drop(\n",
    "        \"coherencyDiagonal\", \"coherencyOneCell\", \"noEvaluable\", \"GoF_risk\", \"LoF_risk\"\n",
    "    )\n",
    "    .withColumnRenamed(\"GoF_protect\", \"drugGoF_protect\")\n",
    "    .withColumnRenamed(\"LoF_protect\", \"drugLoF_protect\")\n",
    ")\n",
    "\n",
    "benchmark = (\n",
    "    (\n",
    "        resolvedColocFiltered.filter( ## .filter(F.col(\"betaGwas\") < 0)\n",
    "        F.col(\"name\") != \"COVID-19\"\n",
    "    )\n",
    "        .join(  ### select just GWAS giving protection\n",
    "            analysis_chembl_indication, on=[\"targetId\", \"diseaseId\"], how=\"right\"  ### RIGHT SIDE\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"AgreeDrug\",\n",
    "            F.when(\n",
    "                (F.col(\"drugGoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"GoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .when(\n",
    "                (F.col(\"drugLoF_protect\").isNotNull())\n",
    "                & (F.col(\"colocDoE\") == \"LoF_protect\"),\n",
    "                F.lit(\"yes\"),\n",
    "            )\n",
    "            .otherwise(F.lit(\"no\")),\n",
    "        )\n",
    "    )  #### remove COVID-19 associations\n",
    ").join(biosample.select(\"biosampleId\", \"biosampleName\"), on=\"biosampleId\", how=\"left\")\n",
    "\n",
    "negativeTD = (\n",
    "    evidences.filter(F.col(\"datasourceId\") == \"chembl\")\n",
    "    .select(\"targetId\", \"diseaseId\", \"studyStopReason\", \"studyStopReasonCategories\")\n",
    "    .filter(F.array_contains(F.col(\"studyStopReasonCategories\"), \"Negative\"))\n",
    "    .groupBy(\"targetId\", \"diseaseId\")\n",
    "    .count()\n",
    "    .withColumn(\"stopReason\", F.lit(\"Negative\"))\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "### create disdic dictionary\n",
    "disdic={}\n",
    "\n",
    "# --- Configuration for your iterative pivoting ---\n",
    "group_by_columns = ['targetId', 'diseaseId','phase4Clean','phase3Clean','phase2Clean','phase1Clean','PhaseT']\n",
    "#columns_to_pivot_on = ['actionType2', 'biosampleName', 'projectId', 'rightStudyType','colocalisationMethod']\n",
    "columns_to_pivot_on = ['projectId']\n",
    "columns_to_aggregate = ['NoneCellYes', 'NdiagonalYes','hasGenetics'] # The values you want to collect in the pivoted cells\n",
    "all_pivoted_dfs = {}\n",
    "\n",
    "doe_columns=[\"LoF_protect\", \"GoF_risk\", \"LoF_risk\", \"GoF_protect\"]\n",
    "diagonal_lof=['LoF_protect','GoF_risk']\n",
    "diagonal_gof=['LoF_risk','GoF_protect']\n",
    "\n",
    "conditions = [\n",
    "    F.when(F.col(c) == F.col(\"maxDoE\"), F.lit(c)).otherwise(F.lit(None)) for c in doe_columns\n",
    "    ]\n",
    "print('entering the big loops')\n",
    "# --- Nested Loops for Dynamic Pivoting ---\n",
    "for agg_col_name in columns_to_aggregate:\n",
    "    for pivot_col_name in columns_to_pivot_on:\n",
    "        print(f\"\\n--- Creating DataFrame for Aggregation: '{agg_col_name}' and Pivot: '{pivot_col_name}' ---\")\n",
    "        current_col_pvalue_order_window = Window.partitionBy(\"targetId\", \"diseaseId\", \"maxClinPhase\", pivot_col_name).orderBy(F.col('colocalisationMethod').asc(), F.col(\"qtlPValueExponent\").asc())\n",
    "        test2=discrepancifier(benchmark.withColumn('actionType2', F.concat_ws(\",\", F.col(\"actionType2\"))).withColumn('qtlColocDoE',F.first('colocDoE').over(current_col_pvalue_order_window)).groupBy(\n",
    "        \"targetId\", \"diseaseId\", \"maxClinPhase\", \"drugLoF_protect\", \"drugGoF_protect\",pivot_col_name)\n",
    "        .pivot(\"colocDoE\")\n",
    "        .count()\n",
    "        .withColumnRenamed('drugLoF_protect', 'LoF_protect_ch')\n",
    "        .withColumnRenamed('drugGoF_protect', 'GoF_protect_ch')).withColumn( ## .filter(F.col('coherencyDiagonal')!='noEvid')\n",
    "    \"arrayN\", F.array(*[F.col(c) for c in doe_columns])\n",
    "    ).withColumn(\n",
    "        \"maxDoE\", F.array_max(F.col(\"arrayN\"))\n",
    "    ).withColumn(\"maxDoE_names\", F.array(*conditions)\n",
    "    ).withColumn(\"maxDoE_names\", F.expr(\"filter(maxDoE_names, x -> x is not null)\")\n",
    "    ).withColumn(\n",
    "        \"NoneCellYes\",\n",
    "        F.when((F.col(\"LoF_protect_ch\").isNotNull() & (F.col('GoF_protect_ch').isNull())) & (F.array_contains(F.col(\"maxDoE_names\"), F.lit(\"LoF_protect\")))==True, F.lit('yes'))\n",
    "        .when((F.col(\"GoF_protect_ch\").isNotNull() & (F.col('LoF_protect_ch').isNull())) & (F.array_contains(F.col(\"maxDoE_names\"), F.lit(\"GoF_protect\")))==True, F.lit('yes')\n",
    "            ).otherwise(F.lit('no'))  # If the value is null, return null # Otherwise, check if name is in array\n",
    "    ).withColumn(\n",
    "        \"NdiagonalYes\",\n",
    "        F.when((F.col(\"LoF_protect_ch\").isNotNull() & (F.col('GoF_protect_ch').isNull())) & \n",
    "            (F.size(F.array_intersect(F.col(\"maxDoE_names\"), F.array([F.lit(x) for x in diagonal_lof]))) > 0),\n",
    "            F.lit(\"yes\")\n",
    "        ).when((F.col(\"GoF_protect_ch\").isNotNull() & (F.col('LoF_protect_ch').isNull())) & \n",
    "            (F.size(F.array_intersect(F.col(\"maxDoE_names\"), F.array([F.lit(x) for x in diagonal_gof]))) > 0),\n",
    "            F.lit(\"yes\")\n",
    "        ).otherwise(F.lit('no'))\n",
    "    ).withColumn(\n",
    "        \"drugCoherency\",\n",
    "        F.when(\n",
    "            (F.col(\"LoF_protect_ch\").isNotNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNull()), F.lit(\"coherent\")\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"LoF_protect_ch\").isNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNotNull()), F.lit(\"coherent\")\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"LoF_protect_ch\").isNotNull())\n",
    "            & (F.col(\"GoF_protect_ch\").isNotNull()), F.lit(\"dispar\")\n",
    "        )\n",
    "        .otherwise(F.lit(\"other\")),\n",
    "    ).join(negativeTD, on=[\"targetId\", \"diseaseId\"], how=\"left\").withColumn(\n",
    "        \"PhaseT\",\n",
    "        F.when(F.col(\"stopReason\") == \"Negative\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase4Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") == 4) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase3Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 3) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase2Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 2) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"phase1Clean\",\n",
    "        F.when(\n",
    "            (F.col(\"maxClinPhase\") >= 1) & (F.col(\"PhaseT\") == \"no\"), F.lit(\"yes\")\n",
    "        ).otherwise(F.lit(\"no\")),\n",
    "    ).withColumn(\n",
    "        \"hasGenetics\",\n",
    "        F.when(F.col(\"coherencyDiagonal\") != \"noEvid\", F.lit(\"yes\")).otherwise(F.lit(\"no\")),\n",
    "    )\n",
    "        # 1. Get distinct values for the pivot column (essential for pivot())\n",
    "        # This brings a small amount of data to the driver, but is necessary for the pivot schema.\n",
    "        #distinct_pivot_values = [row[0] for row in test2.select(pivot_col_name).distinct().collect()]\n",
    "        # print(f\"Distinct values for '{pivot_col_name}': {distinct_pivot_values}\")\n",
    "\n",
    "        # 2. Perform the groupBy, pivot, and aggregate operations\n",
    "        # The .pivot() function requires the list of distinct values for better performance\n",
    "        # and correct schema inference.\n",
    "        pivoted_df = (\n",
    "            test2.groupBy(*group_by_columns)\n",
    "            .pivot(pivot_col_name) # Provide distinct values distinct_pivot_values\n",
    "            .agg(F.collect_set(F.col(agg_col_name))) # Collect all values into a set\n",
    "            .fillna(0) # Fill cells that have no data with an empty list instead of null\n",
    "        )\n",
    "        # 3. Add items to dictionary to map the columns:\n",
    "        # filter out None and 'null':\n",
    "        datasetColumns=pivoted_df.columns\n",
    "        filtered = [x for x in datasetColumns if x is not None and x != 'null']\n",
    "        # using list comprehension\n",
    "        for item in filtered:\n",
    "            disdic[item] = pivot_col_name\n",
    "\n",
    "        # 3. Add the 'data' literal column dynamically\n",
    "        # This column indicates which aggregation column was used.\n",
    "        #pivoted_df = pivoted_df.withColumn('data', F.lit(f'Drug_{agg_col_name}'))\n",
    "\n",
    "        array_columns_to_convert = [\n",
    "            field.name for field in pivoted_df.schema.fields\n",
    "            if isinstance(field.dataType, ArrayType)\n",
    "        ]\n",
    "        print(f\"Identified ArrayType columns for conversion: {array_columns_to_convert}\")\n",
    "\n",
    "        # 4. Apply the conversion logic to each identified array column\n",
    "        df_after_conversion = pivoted_df # Start with the pivoted_df\n",
    "        for col_to_convert in array_columns_to_convert:\n",
    "            df_after_conversion = df_after_conversion.withColumn(\n",
    "                col_to_convert,\n",
    "                F.when(F.col(col_to_convert).isNull(), F.lit('no'))          # Handle NULLs (from pivot for no data)\n",
    "                .when(F.size(F.col(col_to_convert)) == 0, F.lit('no'))       # Empty array -> 'no'\n",
    "                .when(F.array_contains(F.col(col_to_convert), F.lit('yes')), F.lit('yes')) # Contains 'yes' -> 'yes'\n",
    "                .when(F.array_contains(F.col(col_to_convert), F.lit('no')), F.lit('no'))   # Contains 'no' -> 'no'\n",
    "                .otherwise(F.lit('no')) # Fallback for unexpected array content (e.g., ['other'], ['yes','no'])\n",
    "            )\n",
    "\n",
    "        # 4. Generate a unique name for this DataFrame and store it\n",
    "        df_key = f\"df_pivot_{agg_col_name.lower()}_by_{pivot_col_name.lower()}\"\n",
    "        all_pivoted_dfs[df_key] = df_after_conversion.withColumnRenamed( 'phase4Clean','Phase>=4'\n",
    "        ).withColumnRenamed('phase3Clean','Phase>=3'\n",
    "        ).withColumnRenamed('phase2Clean','Phase>=2'\n",
    "        ).withColumnRenamed('phase1Clean','Phase>=1')\n",
    "\n",
    "\n",
    "# --- Accessing your generated DataFrames ---\n",
    "print(\"\\n--- All generated DataFrames are stored in 'all_pivoted_dfs' dictionary ---\")\n",
    "print(\"Keys available:\", all_pivoted_dfs.keys())\n",
    "##### PROJECTID\n",
    "project_keys = (\n",
    "    benchmark\n",
    "    .select(\"projectId\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .map(lambda r: r[0])\n",
    "    .filter(lambda x: x is not None)  # <- remove NULLs\n",
    "    .collect()\n",
    ")\n",
    "#project_keys=[f\"{k}_only\" for k,v in disdic.items() if v == 'projectId']\n",
    "main=['GTEx_only', 'UKB_PPP_EUR_only']\n",
    "#stimulated=['Alasoo_2018_only','Cytoimmgen_only','Fairfax_2014_only','Kim-Hellmuth_2017_only','Nathan_2022_only','Nedelec_2016_only','Quach_2016_only','Randolph_2021_only','Schmiedel_2018_only']\n",
    "#cellLine=['CAP_only','HipSci_only','iPSCORE_only','Jerber_2021_only','PhLiPS_only','Schwartzentruber_2018_only','TwinsUK_only']\n",
    "derivedCellLine=['TwinsUK_only','PhLiPS_only','CAP_only','GENCORD_only','Sun_2018_only','Nedelec_2016_only']\n",
    "canonicalCellLine=['Alasoo_2018_only','Jerber_2021_only','GEUVADIS_only','iPSCORE_only','Aygun_2021_only','Schwartzentruber_2018_only']\n",
    "stimulated=['Schmiedel_2018_only','Bossini-Castillo_2019_only','Alasoo_2018_only','Cytoimmgen_only','Gilchrist_2021_only','CAP_only','Quach_2016_only','Randolph_2021_only','Sun_2018_only','Nedelec_2016_only','Kim-Hellmuth_2017_only']\n",
    "\n",
    "def strip_only(lst):\n",
    "    return [x.removesuffix(\"_only\") for x in lst]  # Python 3.9+\n",
    "    # or: return [x[:-5] if x.endswith(\"_only\") else x for x in lst]\n",
    "\n",
    "# Apply\n",
    "main = strip_only(main)\n",
    "canonicalCellLine = strip_only(canonicalCellLine)\n",
    "derivedCellLine = strip_only(derivedCellLine)\n",
    "stimulated = strip_only(stimulated)\n",
    "\n",
    "others=[item for item in project_keys if item not in main]\n",
    "nonStimulated=[item for item in project_keys if item not in stimulated]\n",
    "nonCanonicalCellLine = [item for item in project_keys if item not in canonicalCellLine]\n",
    "nonDerivedCellLine = [item for item in project_keys if item not in derivedCellLine]\n",
    "\n",
    "\n",
    "# First condition: any \"yes\" in list1\n",
    "# others\n",
    "condition1 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), others[1:], F.col(others[0]) == \"yes\")\n",
    "# estimulated\n",
    "condition2 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), stimulated[1:], F.col(stimulated[0]) == \"yes\")\n",
    "## non estimulated:\n",
    "condition3 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), nonStimulated[1:], F.col(nonStimulated[0]) == \"yes\")\n",
    "# canonical cellLine\n",
    "condition4 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), canonicalCellLine[1:], F.col(canonicalCellLine[0]) == \"yes\")\n",
    "# non canonical cellline\n",
    "condition5 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), nonCanonicalCellLine[1:], F.col(nonCanonicalCellLine[0]) == \"yes\")\n",
    "# derived cell line \n",
    "condition6 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), derivedCellLine[1:], F.col(derivedCellLine[0]) == \"yes\")\n",
    "# non derived cellline\n",
    "condition7 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), nonDerivedCellLine[1:], F.col(nonDerivedCellLine[0]) == \"yes\")\n",
    "# mainprojects\n",
    "condition8 = reduce(lambda acc, col: acc | (F.col(col) == \"yes\"), main[1:], F.col(main[0]) == \"yes\")\n",
    "\n",
    "\n",
    "# Add both columns\n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"othersProjectId_only\", F.when(condition1, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"estimulated_only\", F.when(condition2, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"nonEstimulated\", F.when(condition3, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"canonicalCellLine\", F.when(condition4, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"nonCanonicalCellLine\", F.when(condition5, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"derivedCellLine\", F.when(condition6, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"nonDerivedCellLine\", F.when(condition7, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'] = all_pivoted_dfs['df_pivot_nonecellyes_by_projectid'].withColumn(\"GTExUKB\", F.when(condition8, \"yes\").otherwise(\"no\")) \n",
    "\n",
    "# Add both columns\n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"othersProjectId_only\", F.when(condition1, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"estimulated_only\", F.when(condition2, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"nonEstimulated\", F.when(condition3, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"canonicalCellLine\", F.when(condition4, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"nonCanonicalCellLine\", F.when(condition5, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"derivedCellLine\", F.when(condition6, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"nonDerivedCellLine\", F.when(condition7, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'] = all_pivoted_dfs['df_pivot_ndiagonalyes_by_projectid'].withColumn(\"GTExUKB\", F.when(condition8, \"yes\").otherwise(\"no\")) \n",
    "\n",
    "# Add both columns\n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"othersProjectId_only\", F.when(condition1, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"estimulated_only\", F.when(condition2, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"nonEstimulated\", F.when(condition3, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"canonicalCellLine\", F.when(condition4, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"nonCanonicalCellLine\", F.when(condition5, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"derivedCellLine\", F.when(condition6, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"nonDerivedCellLine\", F.when(condition7, \"yes\").otherwise(\"no\")) \n",
    "all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'] = all_pivoted_dfs['df_pivot_hasgenetics_by_projectid'].withColumn(\"GTExUKB\", F.when(condition8, \"yes\").otherwise(\"no\")) \n",
    "\n",
    "\n",
    "# If you wanted to apply to every DF in the dict (only if they all share *_only columns):\n",
    "# for k, df in all_pivoted_dfs.items():\n",
    "#     all_pivoted_dfs[k] = add_project_group_flags(df, main, stimulated, cellLine)\n",
    "\n",
    "###append to dictionary\n",
    "\n",
    "disdic.update({'othersProjectId': 'projectId','Stimulated': 'projectId','cellLine': 'projectId', 'othersBiosampleName_only': 'biosampleName', 'otherRightStudyType':'rightStudyType'})\n",
    "\n",
    "###################################\n",
    "###################################\n",
    "result = []\n",
    "result_st = []\n",
    "result_ci = []\n",
    "array2 = []\n",
    "listado = []\n",
    "result_all = []\n",
    "today_date = str(date.today())\n",
    "\n",
    "for key,df in all_pivoted_dfs.items():\n",
    "\n",
    "    print(f'working with {key}')\n",
    "    parts = key.split('_by_') ### take the part of key belonging to column name\n",
    "    column_name = parts[1] ### take the last part which is column name\n",
    "    all_pivoted_dfs[key].persist()\n",
    "    #unique_values = all_pivoted_dfs[key].drop('null').columns[7:]\n",
    "    unique_values = all_pivoted_dfs[key].drop('null').columns[-8:] ### just the interesting columns for us \n",
    "    filtered_unique_values = [x for x in unique_values if x is not None and x != 'null']\n",
    "    print('There are ', len(filtered_unique_values), 'columns to analyse with phases')\n",
    "    rows = comparisons_df_iterative(filtered_unique_values)\n",
    "\n",
    "    # If needed, now process the rest\n",
    "    for row in rows:\n",
    "        print('performing', row)\n",
    "        results = aggregations_original(\n",
    "            all_pivoted_dfs[key], key, listado, *row, today_date\n",
    "        )\n",
    "        result_all.append(results)\n",
    "        print('results appended')\n",
    "    all_pivoted_dfs[key].unpersist()\n",
    "    print('df unpersisted')\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"group\", StringType(), True),\n",
    "        StructField(\"comparison\", StringType(), True),\n",
    "        StructField(\"phase\", StringType(), True),\n",
    "        StructField(\"oddsRatio\", DoubleType(), True),\n",
    "        StructField(\"pValue\", DoubleType(), True),\n",
    "        StructField(\"lowerInterval\", DoubleType(), True),\n",
    "        StructField(\"upperInterval\", DoubleType(), True),\n",
    "        StructField(\"total\", StringType(), True),\n",
    "        StructField(\"values\", ArrayType(ArrayType(IntegerType())), True),\n",
    "        StructField(\"relSuccess\", DoubleType(), True),\n",
    "        StructField(\"rsLower\", DoubleType(), True),\n",
    "        StructField(\"rsUpper\", DoubleType(), True),\n",
    "        StructField(\"path\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "import re\n",
    "\n",
    "# Define the list of patterns to search for\n",
    "patterns = [\n",
    "    \"_only\",\n",
    "    #\"_tissue\",\n",
    "    #\"_isSignalFromRightTissue\",\n",
    "    \"_isRightTissueSignalAgreed\",\n",
    "]\n",
    "# Create a regex pattern to match any of the substrings\n",
    "regex_pattern = \"(\" + \"|\".join(map(re.escape, patterns)) + \")\"\n",
    "\n",
    "# Convert list of lists to DataFrame\n",
    "df = (\n",
    "    spreadSheetFormatter(spark.createDataFrame(result_all, schema=schema))\n",
    "    .withColumn(\n",
    "        \"prefix\",\n",
    "        F.regexp_replace(\n",
    "            F.col(\"comparison\"), regex_pattern + \".*\", \"\"\n",
    "        ),  # Extract part before the pattern\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"suffix\",\n",
    "        F.regexp_extract(\n",
    "            F.col(\"comparison\"), regex_pattern, 0\n",
    "        ),  # Extract the pattern itself\n",
    "    )\n",
    ")\n",
    "\n",
    "### annotate projectId, tissue, qtl type and doe type:\n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr=create_map([F.lit(x) for x in chain(*disdic.items())])\n",
    "\n",
    "df_annot=df.withColumn('annotation',mapping_expr.getItem(F.col('prefix')))\n",
    "\n",
    "df_annot.toPandas().to_csv(\n",
    "    f\"gs://ot-team/jroldan/analysis/{today_date}_credibleSetColocDoEanalysis_filteredColocAndCaviarWithOthers4phasesTrue_AllPhasesMixtures3.csv\"\n",
    ")\n",
    "\n",
    "print(\"dataframe written \\n Analysis finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
